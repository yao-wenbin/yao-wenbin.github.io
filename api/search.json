[{"id":"4b2a67d332f9fb2b23f7506e99d7e81b","title":"Java中属性初始化的差异","content":"在Java中，你可以直接在声明属性时初始化、也可以在构造函数中初始化、甚至可以在static代码块中初始化，那么三者有什么差异呢？\n\n初始化的时机不同static代码块中的代码会在类第一次加载时就进行，不需要实例的存在。而构造函数和属性中初始化的变量在对象创建时才进行。所以static代码块适用于被static关键字修饰的属性初始化。\n\n错误的处理如果在属性中初始化时发生错误很难处理异常。但是在static和构造函数中可以通过try-catch的语句来捕获并处理异常，适用于更复杂的场景。\n\n\n所以如果变量需要在多个对象之间共享，或者需要在对象不存在时就进行初始化，应该使用static代码块进行初始化。\n","slug":"Java中属性初始化的差异","date":"2023-05-05T13:14:39.000Z","categories_index":"","tags_index":"Java","author_index":"姚文彬"},{"id":"ba1c6774f6090755efbcf2d6c3343eae","title":"Performance Schema入门指南:你的MySQL性能监测杀器","content":"简介Performance提供了有关MySQL服务器内部运行的底层指标。在高负载下数据库调优是一个循环迭代的过程，每次更改以调整数据库的性能时，都需要了解更改是否有什么影响。而Performance Schama就是一个能够存储回答这个问题所需要的数据的数据库。\n工作机制插桩（instrument）插桩是指在MySQL代码中插入探测代码，以获取我们想要了解的信息。 比如要收集关于select语句的情况，我们就需要启用statement&#x2F;sql&#x2F;select插桩。\n在performance_schema中，setup_instruments包含了所有支持的插桩列表。每个插桩的名称都有斜杆分割的部件组成。比如statment&#x2F;sql&#x2F;select，从左到右依次表示从通用到特定的子系统。\n消费者表（consumer）消费者表就是记录某个插桩从插入到当前运行时的数据。比如某条SQL的执行总数、是否使用索引、话费的时间等信息。\nMySQL 8.0.25版本中包含了110个表，基于用途可以分为以下几类：\n\n当前和历史数据\n汇总和摘要\n实例表（Instance）\n设置表（Setup）\n其他表\n\n资源消耗Performance Schema收集的数据都保存在内存中，而每个插桩指令的调用都会额外的产生宏调用，将数据存放到performance_schema中，这就意味着插桩越多CPU的使用频率越高。但对CPU使用率实际影响取决于特定的插桩。比如与statement相关的插件，在查询过程中只会被调用一次；而wait类的插桩调用频率就高的多，比如在扫描一个一百万行的InnoDB表，引擎需要设置和释放一百万行锁，那么CPU的使用率就会显著增加。所以启动statment、内存和元数据锁类型的插桩，你不会注意到对CPU负载的任何增加。\n注意事项\n插桩目标必须得到MySQL组件的支持。 假设想要使用内存插桩，那么存储引擎应该是支持内存插桩的。\n只在启用后才收集数据。 在禁用所有插桩的情况下启动服务器，然后想要监测内存使用情况。那么全局缓冲区（比如InnoDB缓冲池）的确切数量就无法得知，因为启用内存插桩前就已经分配了该缓冲区。\n它很难释放内存。禁用了特定的插桩或消费者表也不会释放内存，除非重启服务器。\n\nsys Schema自5.7版本依赖，MySQL发行版就包括一个和performacne_schema配套的sys_schema。 它是基于performance_schema上的视图和存储过程组成。 他的目的是让performance_schema的体验更加流畅。\n启动或禁用performance_schema要启用或禁用，只需要修改MySQL实例中的performance_schema即可，这是一个只读变量，所以要么在配置文件中修改，要么在MySQL服务器启动时通过命令参数更改\n插桩可以通过setup_instruments来查看插桩的状态\nmysql> SELECT * FROM performance_schema.setup_instruments WHERE NAME = 'statement/sql/select'\\G\n*************************** 1. row ***************************\n         NAME: statement/sql/select\n      ENABLED: YES\n        TIMED: YES\n   PROPERTIES:\n   VOLATILITY: 0\nDOCUMENTATION: NULL\n1 row in set (0.00 sec)\nENABLED值为YES，表示我们已经开启了statement/sql/select这个插桩了。\n对于插桩的禁用或启用，有三种方式\n\nUPDATE语句\n\n通过标准的SQL语句我们可以直接操作setup_instruments表\nmysql> UPDATE performance_schema.setup_instruments SET ENABLED = 'YES' WHERE NAME = 'statement/sql/select';\n\nQuery OK, 0 rows affected (0.00 sec)\nRows matched: 1  Changed: 0  Warnings: 0\n\n\nsys存储过程\n\n通过sys提供的存储过程，我们可以用更短的语句进行操作\nmysql> CALL sys.ps_setup_enable_instrument('statement/sql/select');\n\n+-----------------------+\n| summary               |\n+-----------------------+\n| Enabled 0 instruments |\n+-----------------------+\n1 row in set (0.02 sec)\n\nQuery OK, 0 rows affected (0.02 sec)\n\n\n\n\n\n\n\n\n\n\n上面两种方式都会随则MySQL实例的重新启动而失效\n\n启动选项\n\n这是唯一一个不会随着MySQL的重启而失效的方式。 该选项还支持通配符。\nperformance-schema-instrument='statement/sql/select=ON'\n\n消费者表与插桩一样，同样通过setup_consumers来查看消费者表的状态同时支持三种方式启动或禁用\n\n直接使用SQL操作setup_consumers表\n调用sys_schema的ps_setup_enable_consumer和ps_setup_disable_consumer来启动和禁用\n使用performance-schema-consumer启动参数\n\n使用场景检查SQL语句Performance Schema将SQL语句指标都存储在了events_statments_current、events_statments_history、events_statments_long三张具有相同的结构的表中。\n我们可以直接使用perfomance_schema进行查询：\nmysql> SELECT * FROM performance_schema.events_statements_history\\G;\n*************************** 1. row ***************************\n              THREAD_ID: 1580\n               EVENT_ID: 133\n           END_EVENT_ID: 133\n             EVENT_NAME: statement/sql/select\n                 SOURCE: init_net_server_extension.cc:97\n            TIMER_START: 989005876364000000\n              TIMER_END: 989005877298000000\n             TIMER_WAIT: 934000000\n              LOCK_TIME: 12000000\n               SQL_TEXT: SELECT * FROM events_statments_current\n                 DIGEST: 287a21eaee8bafb4d09ae546dbc232495489a4a8fd60e50eb535e1eb1cd384f4\n            DIGEST_TEXT: SELECT * FROM `events_statments_current`\n         CURRENT_SCHEMA: performance_schema\n            OBJECT_TYPE: NULL\n          OBJECT_SCHEMA: NULL\n            OBJECT_NAME: NULL\n  OBJECT_INSTANCE_BEGIN: NULL\n            MYSQL_ERRNO: 1146\n      RETURNED_SQLSTATE: 42S02\n           MESSAGE_TEXT: Table 'performance_schema.events_statments_current' doesn't exist\n                 ERRORS: 1\n               WARNINGS: 0\n          ROWS_AFFECTED: 0\n              ROWS_SENT: 0\n          ROWS_EXAMINED: 0\nCREATED_TMP_DISK_TABLES: 0\n     CREATED_TMP_TABLES: 0\n       SELECT_FULL_JOIN: 0\n SELECT_FULL_RANGE_JOIN: 0\n           SELECT_RANGE: 0\n     SELECT_RANGE_CHECK: 0\n            SELECT_SCAN: 0\n      SORT_MERGE_PASSES: 0\n             SORT_RANGE: 0\n              SORT_ROWS: 0\n              SORT_SCAN: 0\n          NO_INDEX_USED: 0\n     NO_GOOD_INDEX_USED: 0\n       NESTING_EVENT_ID: NULL\n     NESTING_EVENT_TYPE: NULL\n    NESTING_EVENT_LEVEL: 0\n           STATEMENT_ID: 14059\n               CPU_TIME: 0\n       EXECUTION_ENGINE: PRIMARY\n里面提供了非常多的有用的信息，包括：SQL语句是什么（SQL_TEXT）、所查询的schema是什么（CURRENT_SCHEMA）、是否包含错误或警告（ERRORS&#x2F;WARNINGS）、是否影响列（ROWS_AFFECTED）、是否使用了全表扫描（SELECT_SCAN）、是否创建了磁盘临时表（CREATED_TMP_DISK_TABLES）、是否没有使用索引(NO_INDEX_USED)、是否没有使用最合适的索引（NO_GOOD_INDEX_USED）\n查询无索引SQL当我们希望找到某个shema中所有没有使用合适索引的查询，可以运行一下命令：\nmysql> SELECT THREAD_ID, SQL_TEXT, ROWS_SENT, ROWS_EXAMINED, CREATED_TMP_TABLES,\nNO_INDEX_USED, NO_GOOD_INDEX_USED\nFROM performance_schema.events_statements_history_long\nWHERE CURRENT_SCHEMA = 'sakila' AND (NO_INDEX_USED > 0 OR NO_GOOD_INDEX_USED > 0);\n\n查询执行时间超过5秒的查询MySQL中为了更加精准的时间，TIMER_WAIT的时间单位为纳秒（1s &#x3D; 10^9 ns）\nmysql> SELECT THREAD_ID, SQL_TEXT, ROWS_SENT, ROWS_EXAMINED, CREATED_TMP_TABLES,\nNO_INDEX_USED, NO_GOOD_INDEX_USED\nFROM performance_schema.events_statements_history_long\nWHERE TIMER_WAIT > 5000000000 \n\n使用sys_schema前面我们说到sys_schema是基于performance_schema之上为我们提供方便使用的视图和存储过程的。我们可以通过视图查询来查询优化的语句，比如：\nmysql> SELECT query, total_latency, no_index_used_count, rows_sent,rows_examined FROM sys.statements_with_full_table_scans where db = 'charon'\\G;\n\n*************************** 1. row ***************************\n              query: SELECT * FROM `sql_explain_statistics`\n      total_latency: 97.00 us\nno_index_used_count: 1\n          rows_sent: 1\n      rows_examined: 1\n\n以下是可以用于查询需要优化的SQL语句的视图\n\n\n\n视图\n描述\n\n\n\nstatements_analysis\n具有聚合信息的语句视图\n\n\nstatments_with_errors_or_warnings\n所有引起错误或警告的语句视图\n\n\nstatements_with_full_table_scans\n所有执行了全表扫描的语句视图\n\n\nstatements_with_runtimes_in_95th_percentile\n所有平均执行时间在前95%的语句视图\n\n\nstatements_with_sorting\n所有执行了排序的语句视图\n\n\nstatements_with_temp_tables\n所有使用了临时表的语句视图\n\n\n","slug":"Performance-Schema入门指南-你的MySQL性能监测杀器","date":"2022-11-28T14:23:44.000Z","categories_index":"","tags_index":"MySQL,监控","author_index":"姚文彬"},{"id":"4468f948c4ca4e742d657728958a113e","title":"你真的了解你的MySQL吗(MySQL基准测试)","content":"如果你没有真正的对服务器上的MySQL进行基准测试，就无法了解其真实情况到底是如何。\n\n\n\n\n\n\n\n\n\n基准测试是数据库工程师必备的技能之一，否则你如何知道自己真的在优化数据库？\n为什么需要基测？基测可以观察系统在不同压力下的行为：\n\n验证基于系统的一些假设是否符合实际情况。\n测试当前的运行情况，如果连这都不知道你如何进行优化？\n模拟比当前系统更高的负载，找出系统的平静。\n测试不同的硬件、软件操作系统配置。\n证明新采购的设备是否正确配置。\n\n基测工具针对MySQL的基准测试工具就有很多了，但是我们推荐使用sysbench（较为简单）和Percona的TPCC-MySQL（面向复杂的场景）。\nSysbech测试案例接下来的本章将着重讲解如何使用sysbench测试MySQL实例。\n下载\nDebian&#x2F;Ubuntu\ncurl -s https://packagecloud.io/install/repositories/akopytov/sysbench/script.deb.sh | sudo bash\nsudo apt -y install sysbench\n\nRHEL&#x2F;Centos\ncurl -s https://packagecloud.io/install/repositories/akopytov/sysbench/script.rpm.sh | sudo bash\nsudo yum -y install sysbench\n\nmacOS\n# Add --with-postgresql if you need PostgreSQL support\nbrew install sysbench\n\n查看脚本sysbench支持更加复杂的测试，同时自身也内嵌了一些关于OLTP系统的测试，我们可以使用find / -name oltp*.lua来查找目录，然后跳转至相应目录查看全部脚本\n\n\n\n\n\n\n\n\n\nOLTP(Online Transaction Processing) 一般指我们\n$ find / -name oltp*.lua  #查找sysbench自带的数据写入脚本的路径,后面执行命令需要用到\n\n/usr/share/sysbench/oltp_delete.lua\n/usr/share/sysbench/oltp_update_non_index.lua\n/usr/share/sysbench/oltp_read_write.lua\n/usr/share/sysbench/oltp_update_index.lua\n/usr/share/sysbench/oltp_read_only.lua\n/usr/share/sysbench/oltp_common.lua\n/usr/share/sysbench/oltp_write_only.lua\n/usr/share/sysbench/oltp_point_select.lua\n/usr/share/sysbench/oltp_insert.lua\n/usr/share/sysbench/tests/include/oltp_legacy/oltp_simple.lua\n/usr/share/sysbench/tests/include/oltp_legacy/oltp.lua\n\n$ ll /usr/share/sysbench/\ntotal 64\n-rwxr-xr-x 1 root root  1452 Apr 25  2020 bulk_insert.lua\n-rw-r--r-- 1 root root 14369 Apr 25  2020 oltp_common.lua\n-rwxr-xr-x 1 root root  1290 Apr 25  2020 oltp_delete.lua\n-rwxr-xr-x 1 root root  2415 Apr 25  2020 oltp_insert.lua\n-rwxr-xr-x 1 root root  1265 Apr 25  2020 oltp_point_select.lua\n-rwxr-xr-x 1 root root  1649 Apr 25  2020 oltp_read_only.lua\n-rwxr-xr-x 1 root root  1824 Apr 25  2020 oltp_read_write.lua\n-rwxr-xr-x 1 root root  1118 Apr 25  2020 oltp_update_index.lua\n-rwxr-xr-x 1 root root  1127 Apr 25  2020 oltp_update_non_index.lua\n-rwxr-xr-x 1 root root  1440 Apr 25  2020 oltp_write_only.lua\n-rwxr-xr-x 1 root root  1919 Apr 25  2020 select_random_points.lua\n-rwxr-xr-x 1 root root  2118 Apr 25  2020 select_random_ranges.lua\ndrwxr-xr-x 4 root root  4096 Nov  4 16:02 tests\n\n\n\n小试牛刀接下来我们将sysbench正式作用于我们的服务器， 笔者准备了一台2C4G的云服务器、MySQL5.7、默认MySQL配置。首先我们需要进入MySQL创建一个测试数据库名为sysbench_test：\n$ mysql -uroot -p\n\nmysql> create database sysbench_test\n\nmysql> quit\n\n\n对于数据库的测试，我们一般会使用到sysbench的oltp脚本， 而其中分为读、写、读写混合等多种场景。\noltp测试步骤基本上分为：准备数据(prepare) - 执行测试(run) - 清理数据(cleanup) 三个步骤。而一般我们只需要用相同的参数运行不同的命令即可。\n准备数据：\nsysbench oltp_read_write --mysql-host=127.0.0.1 --mysql-port=3306 --mysql-db=sysbench_test --mysql-user=root --mysql-password=xxxxx --table_size=100 --tables=10 --threads=20 --report-interval=10 --time=120 prepare\n\n# oltp_read_write 指定sysbench内嵌的测试脚本\n### 准备时参数\n# --mysql-host/db/user/passowrd/port 测试数据库地址/名称/用户名/密码/端口\n# --tables 指定生成的表数量　\n# --table_size 指定每张表表的数据量\n\n### 运行时桉树\n# --thread 指定测试时的线程数\n# --report-interval 指定运行时日志打印间隔\n# --time 指定测试时长\n\n\n执行测试：\n$ sysbench oltp_read_write --mysql-host=127.0.0.1 --mysql-port=3306 --mysql-db=sysbench_test --mysql-user=root --mysql-password=xxx --table_size=100 --tables=10 --threads=20 --report-interval=10 --time=120 run\nsysbench 1.0.20 (using bundled LuaJIT 2.1.0-beta2)\n\nRunning the test with following options:\nNumber of threads: 20\nReport intermediate results every 10 second(s)\nInitializing random number generator from current time\n\n\nInitializing worker threads...\n\nThreads started!\n\n[ 10s ] thds: 20 tps: 298.33 qps: 6897.07 (r/w/o: 4973.02/1270.50/653.55) lat (ms,95%): 161.51 err/s: 54.99 reconn/s: 0.00\n[ 20s ] thds: 20 tps: 325.41 qps: 7474.29 (r/w/o: 5380.34/1384.24/709.72) lat (ms,95%): 147.61 err/s: 58.80 reconn/s: 0.00\n[ 30s ] thds: 20 tps: 289.30 qps: 6699.44 (r/w/o: 4828.16/1236.99/634.29) lat (ms,95%): 164.45 err/s: 55.70 reconn/s: 0.00\n[ 40s ] thds: 20 tps: 326.50 qps: 7444.30 (r/w/o: 5354.60/1380.80/708.90) lat (ms,95%): 150.29 err/s: 55.90 reconn/s: 0.00\n[ 50s ] thds: 20 tps: 326.40 qps: 7480.10 (r/w/o: 5383.30/1385.90/710.90) lat (ms,95%): 150.29 err/s: 58.10 reconn/s: 0.00\n[ 60s ] thds: 20 tps: 338.10 qps: 7669.44 (r/w/o: 5508.03/1429.91/731.50) lat (ms,95%): 147.61 err/s: 55.30 reconn/s: 0.00\n[ 70s ] thds: 20 tps: 320.50 qps: 7305.44 (r/w/o: 5251.46/1358.39/695.59) lat (ms,95%): 153.02 err/s: 54.70 reconn/s: 0.00\n[ 80s ] thds: 20 tps: 361.80 qps: 8212.92 (r/w/o: 5901.02/1528.60/783.30) lat (ms,95%): 137.35 err/s: 59.60 reconn/s: 0.00\n[ 90s ] thds: 20 tps: 345.80 qps: 7903.20 (r/w/o: 5684.10/1467.20/751.90) lat (ms,95%): 139.85 err/s: 60.30 reconn/s: 0.00\n[ 100s ] thds: 20 tps: 351.00 qps: 8032.68 (r/w/o: 5779.16/1489.81/763.71) lat (ms,95%): 134.90 err/s: 61.70 reconn/s: 0.00\n[ 110s ] thds: 20 tps: 348.10 qps: 7964.29 (r/w/o: 5730.19/1476.70/757.40) lat (ms,95%): 137.35 err/s: 61.20 reconn/s: 0.00\n[ 120s ] thds: 20 tps: 343.40 qps: 7899.49 (r/w/o: 5690.09/1459.50/749.90) lat (ms,95%): 134.90 err/s: 63.10 reconn/s: 0.00\nSQL statistics:\n    queries performed:\n        read:                            654696\n        write:                           168754\n        other:                           86531\n        total:                           909981\n    transactions:                        39767  (331.22 per sec.)\n    queries:                             909981 (7579.29 per sec.)\n    ignored errors:                      6997   (58.28 per sec.)\n    reconnects:                          0      (0.00 per sec.)\n\nGeneral statistics:\n    total time:                          120.0600s\n    total number of events:              39767\n\nLatency (ms):\n         min:                                    4.99\n         avg:                                   60.36\n         max:                                  407.99\n         95th percentile:                      147.61\n         sum:                              2400503.17\n\nThreads fairness:\n    events (avg/stddev):           1988.3500/25.31\n    execution time (avg/stddev):   120.0252/0.02\n\n\n重要的参数指标为SQL statistics下的**queries(即QPS)**：达到了7579 per sec.也就是说当前MySQL在10张表100条数据的情况下面对20个线程的读写吞吐量达到了7579QPS。当然这只是在小数据小压力的情况下 ，并不能直接作为线上环境的参考。\n那么接下来让我们清理数据准备下一轮测试\n加大压力刚刚只是尝试一下sysbench的功能，这次我们开始使用较大的压力对MySQL进行读测试（一般对MySQL的读写测试应该分开单独的进行以更好的评估MySQL哪些方面需要提升）。\n\n测试操作：读\n线程数：100\n表大小：100w\n表数量：10\n测试时长：5分钟\n\n对应执行命令：\nsysbench oltp_read_only --mysql-host=127.0.0.1 --mysql-port=3306 --mysql-db=sysbench_test --mysql-user=root --mysql-password=[yourpassowrd] --table_size=1000000 --tables=10 --threads=100 --report-interval=10 --time=300 [command]\n\n测试结果：可以看到我们的MySQL在100个并发访问线程下对于10张表，每张表100w数据进行5分钟的读写测试。能够达到1w的读性能， 还是非常不错的。\n测试写性能现在让我们清理一下测试数据，最后测试一轮写性能\n\n测试操作：写\n线程数：100\n表大小：100w\n表数量：10\n测试时长：3分钟\n\n测试命令：\nsysbench oltp_write_only --mysql-host=127.0.0.1 --mysql-port=3306 --mysql-db=sysbench_test --mysql-user=root --mysql-password=[yourpassowrd] --table_size=1000000 --tables=10 --threads=100 --report-interval=10 --time=300 [command]\n\n\n测试结果：可以看到测试机器上的这个MySQL能够达到9k接近1w的写QPS。\n总结基本测试是我们了解MySQL所要掌握的必备技能之一，而sysbench是一款多线程的性能测试工具。 使用它我们就能够对MySQL的读写QPS有一个非常好的了解。\n","slug":"你真的了解你的MySQL吗-MySQL基准测试","date":"2022-11-12T13:31:33.000Z","categories_index":"","tags_index":"MySQL","author_index":"姚文彬"},{"id":"ad2c272cfe3adee335f5f82ded13d5f6","title":"RocketMQ事务消息","content":"什么是事务首先我们先回顾一下事务的主要作用，是要保证多个操作的原子性，多个事务操作要么一起成功，要么一起失败。\n分布式事务使用场景使用场景模拟：用户支付订单后，同时涉及到多个下游：物流发货、积分变更、购物车状态清空。在这个场景中的事务操作有四种：\n\n订单系统的状态状态更新。\n物流系统的订单物流记录新增。\n积分系统的用户积分更新。\n购物车系统的用户购物车状态更新。\n\n传统解决方案为了实现这样的事务操作，传统的解决方案就是基于XA协议（二阶段和三阶段提交）的分布式事务，将四个操作分装为四个独立的大事务来解决。这样能够帮助业务处理结果的正确性，但是无法由于资源锁定的粒度大，并发性能低。\nRocketMQ的事务消息而RocketMQ中的事务消息，为我们提供了最终一致性的解决方案：处理流程如下图所示：\n()[&#x2F;Pasted image 20230424141848.png]\n\n实现过程类似二阶段提交，通过Half Message半消息来帮助我们在Server中作为提交第一阶段作为事务的开始。\n如果服务端未收到发送着提交的第二阶段的二次确认结果，那么会主动的向消息生产者集群的任一消费者发起消息回查。\n\n使用限制：根据RocketMQ官方文档中给出的的事务消息的使用限制如下：\n\n仅支持MessageType消息为Tranasction的Topic主题使用。\nApache RocketMQ不保证消息消费结果与上游事务的一致性，所以需要下游业务分支做好消费重试自行保证消息被正确处理。\n中间状态可见性，RocketMQ的事务消息为最终一致性，所以消息在提交到消费完成之前，上游和下游看到的消息的状态可能会出现不一致性。\n事务超时时间，RocketMQ事务生命周期默认存在超时机制，若执行超时时间内服务端无法确认提交还是回滚，那么最终会回滚消息。\n\n实践：\n引入RocketMQ官方的springboot集成包\n&lt;dependency>  \n    &lt;groupId>org.apache.rocketmq&lt;/groupId>  \n    &lt;artifactId>rocketmq-spring-boot-starter&lt;/artifactId>  \n    &lt;version>2.2.3&lt;/version>  \n&lt;/dependency>\n\n使用application.yml配置nameserv地址：\nrocketmq:  \n  name-server: 127.0.0.1:9876  \n  producer:  \n    group: producer-group\n\n创建Application\n@SpringBootApplication  \npublic class ProducerApplication implements CommandLineRunner &#123;\n\n\t@Resource  \n\tOrderPaidEventProducer paidEventProducer;\n\t\n\t@Override  \n\tpublic void run(String... args) &#123;  \t  \n\t    // Transaction Message  \n\t    OrderPaidMessage paidMessage = new OrderPaidMessage(order);  \n\t    paidEventProducer.send(paidMessage);  \n\t&#125;\n\t\n\tpublic static void main(String[] args) &#123;  \n\t    SpringApplication.run(ProducerApplication.class, args);  \n\t&#125;\n&#125;\n\n创建OrderPadiEventProducer订单支付事务消息发送者\n/**  \n * @Author yaowenbin\n * @Date 2023/4/24 \n */\n@Service  \n@RequiredArgsConstructor  \npublic class OrderPaidEventProducer &#123;  \n  \n    private final Logger logger = LoggerFactory.getLogger(OrderPaidEventProducer.class);  \n  \n    private final RocketMQTemplate template;  \n  \n  \n    public void send(OrderPaidMessage message) &#123;  \n        TransactionSendResult result = template.sendMessageInTransaction(\"application-transaction\",  \n                MessageBuilder.withPayload(message).build(),  \n                null  \n        );  \n  \n        logger.info(\"Transaction Send Result: &#123;&#125;\", result);  \n    &#125;  \n%\n    @RocketMQTransactionListener  \n    static class TransactionListenerImpl implements RocketMQLocalTransactionListener &#123;  \n  \n        @Override  \n        public RocketMQLocalTransactionState executeLocalTransaction(Message message, Object o) &#123;  \n            return null;  \n        &#125;  \n  \n        @Override  \n        public RocketMQLocalTransactionState checkLocalTransaction(Message message) &#123;  \n            OrderPaidMessage payload = (OrderPaidMessage) message.getPayload();  \n            return checkOrderStatus(payload.getOrderId()) ? RocketMQLocalTransactionState.COMMIT : RocketMQLocalTransactionState.ROLLBACK;  \n        &#125;  \n  \n        public boolean checkOrderStatus(Long id) &#123;  \n            return id != null;  \n        &#125;  \n    &#125;  \n  \n&#125;\n\n","slug":"rocketmq/事务消息","date":"2023-04-24T06:08:43.782Z","categories_index":"","tags_index":"Java,RocketMQ,分布式事务","author_index":"姚文彬"},{"id":"13c371c2ff633aa9e8eadac4915ee855","title":"FastJson安全漏洞分析","content":"FastJson如何进行序列化的？FastJson和Jackson的序列化方式都是通过反射获取对象的getter方法来获得属性值的（Gson是通过直接反射属性）。\nFastJson的序列化问题当一个类实现了一个接口的时候，在FastJson中进行序列化时，会将这个类的实际类型抹去，只保存该类所实现的接口类型。而这会导致反序列化时无法拿到原始类型。\n在类嵌套类的序列化中，FastJson并不能够为我们将子类给成功反序列化：\n// 序列化\nStore store = new Store();\nstore.setName(\"Hollis\");\nApple apple = new Apple();\napple.setPrice(new BigDecimal(0.5));\nstore.setFruit(apple);\nString jsonString = JSON.toJSONString(store);\nSystem.out.println(\"toJSONString : \" + jsonString);\n// result:\ntoJSONString : &#123;\"fruit\":&#123;\"price\":0.5&#125;,\"name\":\"Hollis\"&#125;\n\n// 反序列化\nStore newStore = JSON.parseObject(jsonString, Store.class);\nSystem.out.println(\"parseObject : \" + newStore);\nApple newApple = (Apple)newStore.getFruit();\nSystem.out.println(\"getFruit : \" + newApple);\n\n// result:\ntoJSONString : &#123;\"fruit\":&#123;\"price\":0.5&#125;,\"name\":\"Hollis\"&#125;\nparseObject : Store&#123;name='Hollis', fruit=&#123;&#125;&#125;\nException in thread \"main\" java.lang.ClassCastException: com.hollis.lab.fastjson.test.$Proxy0 cannot be cast to com.hollis.lab.fastjson.test.Apple\nat com.hollis.lab.fastjson.test.FastJsonTest.main(FastJsonTest.java:26)\n\nAutoType而为了解决这个问题，在FastJson中引入了一个叫AutoType的机制，能够在序列化时把原始类型记录下来。通过SerializerFeature.WriteClassName\nString jsonString = JSON.toJSONString(store,SerializerFeature.WriteClassName);\n\n// reuslt\n&#123;\n    \"@type\":\"com.hollis.lab.fastjson.test.Store\",\n    \"fruit\":&#123;\n        \"@type\":\"com.hollis.lab.fastjson.test.Apple\",\n        \"price\":0.5\n    &#125;,\n    \"name\":\"Hollis\"\n&#125;\n\n可以看到JSON字符串中多出来了@type类型，标注了类的原始类型。在反序列化时，FastJson看到@type的类型就会从Java类库中查找到对应的类。\nRMI而这样本身是没有问题的。而刚好在sun官方提供的类库中有一个JdbcRowSetImpl类，其datasourceName支持rmi传入。没错和log4j2一样的RMI源的问题，当解析这个uri的时候，会去指定的rmi地址中调用方法。所以如果黑客在这个rmi地址中注入了想要执行的指令，那么就会造成严重的后果了。\n如何避免\n升级到最新版本的FastJson\n打开SafeMode，FastJson会自动禁用AutoType。\n\nFastJson的优势虽然FastJson的AutoType导致了安全漏洞，由于其自己定义了序列化工具类，并且使用asm技术避免反射、使用缓存、并且做了很多算法优化等方式，大大提升了序列化及反序列化的效率。\n","slug":"FastJson安全漏洞","date":"2023-03-30T01:27:51.735Z","categories_index":"","tags_index":"Java,序列化","author_index":"姚文彬"},{"id":"ecdf1c9aa66b051d88622cd0bc08ef91","title":"JDK里程碑:JDK8到JDK17的重要特性汇总","content":"\n\n\n\n\n\n\n\n\n从BenchMark可以看到，仅需要从JDK8到JDK17就能够获得64%的改进 — JavaOne开幕式\n本文章的灵感来源于Java One开幕式上JDK开发者的这句话。JDK向前兼容的重要特性意味着，我们不需要改动任何代码只需要将运行源代码的JDK从8替换为JDK17就能够获得大量的提升（极地的成本）。 \n这篇文章讲汇总JDK8到JDK17的关键特性，理解为什么从JDK8到JDK17有着重要意义。\nJDK底层&#x2F;API语法：JDK9\n\n接口方法可以使用private修饰\n支持http2.0和websocketAPI\n模块化编程（对于依赖于Maven的开发者来说使用率和其产生的作用不高）\nString类底层数组从char[]变更为了byte[]。（byte能够在同样支持拉丁字符的情况下只占用1个字节，而char需要2个）\n支持NumberFormat API，能够对数字进行格式化或者压缩\n\nJDK10\n\n推出var关键字支持局部变量类型推断，类似JS推断出值的真实类型\n\nJDK11\n\n集合API的增强\n对Stream、Optional、集合API增强\n\nJDK13\n\nSocket底层优化，引入NIO\nswitch表达式添加yield关键字，用于返回结果，作用类似return\n引入”””三双引号语法（文本块），内部不需要使用换行的转义字符。\n\nJDK14\n\ninstanceof关键字优化，代码块中会直接给对象赋值\n引入record类，会自动生成构造器、getter、setter等方法，类似Lombok。\nNPE优化，可以打印具体哪个方法跑出了NPE，方便单行代码多个函数调用时的异常排查。（开发者福音）\n\nJDK15\n\n隐藏类Hiden Class\n密封类 Sealed Class，密封类能够限定继承或者实现的子类。\n对应java.lang.Class问类中新增了isSealed和getPermittedSubClasses()两个方法用于判断密封类和密封类所允许的拓展Class列表\n\nJDK16正式的将record、instanceof特性引入到JDK版本中\nJDK17正式引入密封类sealed class\n\n统一日志一步刷新\n\nJVM优化：JDK9\n\n设置为G1默认垃圾回收器\n\nJDK10\n\n并行FullGC，优化G1的延迟（JDK9的Full GC只有单线程，而10之后会采用Young和Mixed相同数量的线程进行FullGC）\n线程局部管控，支持在不执行全局VM安全点的情况下对线程执行回调方法，停止单个线程，而不需要停止所有线程。\n\nJDK11\n\n推出ZGC（随后经过了多次的优化，是JDK17中综合性能最好的垃圾收集器，根据JavaOne的介绍其目标是能够支持亚秒级别回收TB界别的堆，还没达到但已经非常强了）\n\nJDK12\n\n推出Shenandoan垃圾收集器（12， 仅支持Linux，）\n拓展swtich表达式，支持返回值（12\n优化G1收集器，将G1的垃圾分为强制回收和可选部分，提高GC效率\n\nJDK13\n\nZGC优化，将标记长时间的空闲对内存空间返回给OS，保证堆大小不会小于配置的最小堆内存\n\nJDK15\n\nZGC性能优化\n\n新语法&#x2F;API实际使用val局部变量类型推断标识符var不是关键字，而只是一个保留的类型名称，仅适用于局部变量，能够通过值来推断出值的数据类型过是什么。示例\nvar str = \"ABC\"; //根据推断为 字符串类型\nvar l = 10L;//根据10L 推断long 类型\nvar flag = true;//根据 true推断 boolean 类型\nvar flag1 = 1;//这里会推断boolean类型。0表示false 非0表示true\nvar list = new ArrayList&lt;String>();  // 推断 ArrayList&lt;String>\nvar stream = list.stream();          // 推断 Stream&lt;String>\n反编译class文件：\nString str = \"ABC\";\nlong l = 10L;\nboolean flag = true;\nint flag1 = true;\nArrayList&lt;String> list = new ArrayList();\nStream&lt;String> stream = list.stream();\n\nswitch表达式\n现在支持使用switch赋值\n引入了lambda表达式，\n引入了yield语法，能够返回值，而不是使用break；\nString result = switch (day) &#123;\n    case \"M\", \"W\", \"F\" -> \"MWF\";\n    case \"T\", \"TH\", \"S\" -> \"TTS\";\n    default -> &#123;\n        if(day.isEmpty())\n            yield \"Please insert a valid day.\";\n        else\n            yield \"Looks like a Sunday.\";\n&#125;\n\ninstanceof模式匹配JDK14之前\nif (obj instanceof Article) &#123;\n  Article a= (Article) obj;\n  System.out.println(a.getAuthor());\n&#125;\nJDK14之后\nif (obj instanceof Article a) &#123;\n  System.out.println(a.getAuthor());\n&#125;\n\n\n\nNPE错误信息更详细JDK14之前\nException in thread \"main\" java.lang.NullPointerException at NullPointerExample.main(NullPointerExample.java:5)\nJDK14之后\nException in thread \"main\" \njava.lang.NullPointerException: Cannot invoke \"Blog.getAuthor()\" because the return value of \"Article.getBlog()\" is null at NullPointerExample.main(NullPointerExample.java:4)\n\nrecord记录类record Author()&#123;&#125;\n//or\nrecord Author (String name, String topic) &#123;&#125;\n编译器会自动生成构造函数、g&#x2F;setter、equals&#x2F;hashCode、toString等方法\nsealed 密封类// 创建一个密封的Hello接口，只允许Hello2类对其进行实现\npublic interface sealed class Hello permits Hello2&#123;&#125;\n\nNumberFormat增加了对压缩数字的支持String number = NumberFormat.getCompactNumberInstance(Locale.US, \n\t\tNumberFormat.Style.SHORT).format(1000);\n// 其中number = “1k“\n\nString类API的增强添加了strip()，isBlack()、isEmpty()、lines()、repeat()等多个方法\n\nstrip()看作是trim的增强，能够去掉unicode空格，对应的还有stripLeading()和stripTailing()\nisBlack()和isEmpty()两者是String类中内置的方法，但依然建议使用各个工具包下的StringUtils。因为使用内置方法时，如果对象为空会产生NPE；而工具类下的StringUtils则不会。\nlines()会将单个多行字符才成多个单行字符\nrepeat()能够构建一个或多个相同字符串组合的字符串\n\n","slug":"JDK里程碑-JDK8到JDK17的重要特性汇总","date":"2022-11-27T03:59:14.000Z","categories_index":"","tags_index":"JDK,JVM","author_index":"姚文彬"},{"id":"c7562fc289c09e37ab4a4b417948e80b","title":"OkHttp：更加优雅的客户端OkHttps","content":"上文中，我们介绍了OkHttp的一些常见的用法，以及对其API进行了一些便于调用的封装。而笔者在深入学习的过程中发现了一个基于Lambda表达式、链式调用进行封装的OkHttps，使其的调用方式更加的优雅、简介。\n简介https://ok.zhxu.cn/v4/introduction.htmlOkHttps 是 2020 年开源的对 OkHttp3 轻量封装的框架，它独创的异步预处理器，特色的标签，灵活的上传下载进度监听与过程控制功能，在轻松解决很多原本另人头疼问题的同时，设计上也力求纯粹与优雅。\nMaven中要想使用OkHttps，我们需要引入一个OkHttps核心包和一个OkHttps-xxxx(序列化框架)的序列化包，比如FastJson2，我们可以通过     cn.zhxu     okhttps     4.0.0     cn.zhxu     okhttps-fastjson2     4.0.0\n如果是Jackson的话，我们只需要将fastjson2换成     cn.zhxu     okhttps-jackson     4.0.0即可。\n基本使用\n首先，我们需要向使用OkHttp那样，构建一个HTTP请求， 为了使用方便，我们更愿意指定一个BaseUrl和MsgConverter：HTTP http &#x3D; HTTP.builder()        .baseUrl(“http://api.example.com&quot;)        &#x2F;&#x2F; GsonMsgConverter源自okhttps-gons包中，使用时更换为实际导入的序列化包        .addMsgConvertor(new GsonMsgConvertor())        .build();\n随后，我们可以通过链式调用的方式来开始一个同步请求了：List users &#x3D; http.sync(“&#x2F;users”) &#x2F;&#x2F; http://api.example.com/users        .get()                         &#x2F;&#x2F; GET请求        .getBody()                     &#x2F;&#x2F; 获取响应报文体        .toList(User.class);           &#x2F;&#x2F; 得到目标数据或者是一个异步请求：http.async(“&#x2F;users&#x2F;1”)                &#x2F;&#x2F;  http://api.example.com/users/1        .setOnResponse((HttpResult res) -&gt; {            &#x2F;&#x2F; 得到目标数据            User user &#x3D; res.getBody().toBean(User.class);        })        .get();                       &#x2F;&#x2F; GET请求如果你想要一个WebSocket通讯的话，可以这样：http.webSocket(“&#x2F;chat”)        .setOnOpen((WebSocket ws, HttpResult res) -&gt; {            ws.send(“向服务器问好”);        })        .setOnMessage((WebSocket ws，Message msg) -&gt; {            &#x2F;&#x2F; 从服务器接收消息（自动反序列化）            Chat chat &#x3D; msg.toBean(Chat.class);            &#x2F;&#x2F; 相同的消息发送给服务器（自动序列化 Chat 对象）            ws.send(chat);        })        .listen();                     &#x2F;&#x2F; 启动监听\n请求三部曲对于任何请求，OkHttps都可以将其看作为三个步骤：\n\n确定请求方式（同步、异步） \n构建请求任务（添加请求参数、设置回调函数） \n调用请求方法（get、post、delete、put）而对于这种模板化的开发方式，作为IDEA的开发者我们可以通过Live Template来提高我们的开发效率：\n\n注入配置OkHttps还支持通过SPI的方式注入自定义配置类\n第一步：新建一个配置类实现cn.zhxu.okhttps.Config接口package com.example.okhttps;\nimport cn.zhxu.okhttps.Config;import cn.zhxu.okhttps.HTTP;\npublic class OkHttpsConfig implements Config {\n@Override\npublic void with(HTTP.Builder builder) &#123;\n    // 在这里对 HTTP.Builder 做一些自定义的配置\n    builder.baseUrl(&quot;https://api.domo.com&quot;);\n    // 如果项目中添加了 okhttps-fastjson 或 okhttps-gson 或 okhttps-jackson 依赖\n    // OkHttps 会自动注入它们提供的 MsgConvertor \n    // 所以这里就不需要再配置 MsgConvertor 了 (内部实现自动注入的原理也是 SPI)\n    // 但如果没有添加这些依赖，那还需要自定义一个 MsgConvertor\n    builder.addMsgConvertor(new MyMsgConvertor());\n&#125;\n\n}第二步：在项目的&#x2F;src&#x2F;main&#x2F;目录下新建resources&#x2F;META-INF&#x2F;services&#x2F;cn.zhxu.okhttps.Config文件，文件内容是上一个配置类的全类名\n为什么要使用SPI注入配置而不是直接给OkHttps设置一个静态变量实例呢？在一般情况下这样是没问题的，但是某些JVM上（特别是Android）会在某些情况下回收静态变量，而OkHttps的HTTP实例一旦被回收之后，配置都会丢失，导致严重的问题。\n对于SpringBoot项目，如果还想要导入application.yml的配置的话，我们可以现在Spring的Bean中加载配置，然后提供静态访问方法，再去配置类中访问这个Bean。@Componentpublic class ConfigBean {\n@Value(&quot;$&#123;okhttps.baseUrl&#125;&quot;)    // 加载配置\nprivate String baseUrl;\n\nprivate static ConfigBean instance;\n\npublic ConfigBean() &#123;\n    instance = this;            // 将该 Bean 单例化\n&#125;\n\npublic static ConfigBean getInstance() &#123;\n    return instance;            // 提供静态访问方法\n&#125;\n\n// 省略 Getter Seatter\n\n}public class OkHttpsConfig implements Config {\n@Override\npublic void with(HTTP.Builder builder) &#123;\n    // 获取 ConfigBean\n    ConfigBean confg = ConfigBean.getInstance();\n    // 使用配置\n    builder.baseUrl(confg.getBaseUrl());\n    // 省略其它配置\n&#125;\n\n}\n","slug":"OkHttp：更加优雅的客户端OkHttps","date":"2022-11-21T00:55:52.000Z","categories_index":"","tags_index":"工具,OkHttp","author_index":"姚文彬"},{"id":"db73c7a78b5189912e109ef7e8566686","title":"减少你的代码量：基于AOP实现字典翻译","content":"什么是字典？开发者更加倾向于使用短小的字段类型比如 tinyint、char(1)等来存储一些类型字段，以让每个数据都能够尽可能少的占用空间。 而用户当然不买单，希望能够看到每个字段都真正含义（比如 status &#x3D; 1时，其真正含义是状态进行中， status &#x3D; 2时，其真正含义是状态完成）而数据从1到进行中的数据变换过程我们称之为字典翻译。\n实现方案而字典是任何后台管理系统比不可少的系统功能模块之一。 而根据架构选型的不同，其内容翻译的主要实现方案有两种：\n\n基于后端查询时自动进行翻译（本文主要讨论内容）\n\n在进行查询、列表时后端能够自动对字段进行翻译，把对于status字段，能够有一个statusName（存储翻译结果的名称，随你定）来说明status的含义，并展示给用户。\n\n基于前端展示时的自动翻译\n\n在后端返回结果后，通过status字段再次去调用后端的API或者是本地的字典缓存，来获取status字段的真实含义，然后展示给用户。\n而本文的剩下内容将讨论如何基于Spring AOP来实现方案1（后端自动翻译）功能。\n使用方式\n添加字段用于填充翻译后的文本\n添加@Dict注解，并指定翻译的字典类型\n在API的返回方法中添加@DictTranslation注解class Entity&#123;\n    ...\n\t// 未翻译的字典值\n    private String type;\n\n    @Dict(EVENT_TYPE)\n    private String typeName;\n    ...\n&#125;\n\nclass Controller&#123;\n    ...\n    \n    @GetMapping(\"/detail\")\n    @DictTranslation\n    public ApiResult getById(@NotBlank @RequestParam(\"id\") String id) &#123;\n        return ApiResult.ok().data(service.getInfo(id));\n    &#125;\n    \n    ...\n&#125;\n\n\n\n\n\n\n\n\n\n\n实现结果\n\n实现步骤\n添加注解@Dict用于指定字典的元数据（字典类型和字典值）\n@Retention(RetentionPolicy.RUNTIME)\n@Target(ElementType.FIELD)\npublic @interface Dict &#123;\n\n    /**\n     * 字典类型：t_sys_dict_type中的dict_type字段.\n     */\n    String value();\n\n    String codeField() default \"\";\n&#125;\n\n\n添加注解@DictTranslation用于手动指定哪些方法会通过AOP实现自动翻译\n@Retention(RetentionPolicy.RUNTIME)\n@Target(ElementType.METHOD)\npublic @interface DictTranslation &#123;\n&#125;\n\n\n实现注解AOP切面\n/**\n * @Author yaowenbin\n * @Date 2022/11/18\n */\n@Aspect\n@Component\n@Slf4j\npublic class DictAspect &#123;\n\n    @Resource\n    private SysDictDataService dictDataService;\n\n    @Pointcut(\"@annotation(dictTranslation)\")\n    public void pointcut(DictTranslation dictTranslation) &#123;\n\n    &#125;\n\n    @Around(\"pointcut(dictTranslation)\")\n    public Object doAround(ProceedingJoinPoint pjp, DictTranslation dictTranslation) throws Throwable &#123;\n        Object proceed = pjp.proceed();\n\n        if (! (proceed instanceof ApiResult) ) &#123;\n            return proceed;\n        &#125;\n\n        ApiResult result = (ApiResult) proceed;\n        // 翻译单个对象\n        notNull(result.get(ApiResult.FIELD_DATA), val -> &#123;\n            translationDict(result.get(\"data\"));\n        &#125;);\n    \t// 翻译列表对象\n        notNull(result.get(ApiResult.FIELD_PAGE), val -> &#123;\n            PageVo pageVo = (PageVo) result.get(\"page\");\n            List dataList = pageVo.getList();\n            for (Object o : dataList) &#123;\n                translationDict(o);\n            &#125;\n        &#125;);\n\n        return proceed;\n    &#125;\n\n    public void translationDict(Object data) &#123;\n        Field[] fields = data.getClass().getDeclaredFields();\n\n        for (Field field : fields) &#123;\n            if (field.isAnnotationPresent(Dict.class)) &#123;\n                Dict dict = field.getAnnotation(Dict.class);\n\n                String codeFieldName;\n                if (dict.codeField().equals(\"\")) &#123;\n                    String fieldName = field.getName();\n                    codeFieldName = fieldName.substring(0, fieldName.length() - 4);\n                &#125; else &#123;\n                    codeFieldName = dict.codeField();\n                &#125;\n\n                char[] chars = codeFieldName.toCharArray();\n                chars[0] = toUpperCase(chars[0]);\n                codeFieldName = String.valueOf(chars);\n                String getterName = \"get\" + codeFieldName;\n\n                Method codeGetter;\n                try &#123;\n                    codeGetter = data.getClass().getMethod(getterName);\n                &#125; catch (NoSuchMethodException | SecurityException e) &#123;\n                    log.warn(\"翻译失败, &#123;&#125;未找到&#123;&#125;()方法，无法翻译该字段\", data.getClass(), getterName);\n                    continue;\n                &#125;\n                String code;\n                try &#123;\n                    code = (String) codeGetter.invoke(data);\n                &#125; catch (IllegalAccessException | InvocationTargetException e) &#123;\n                    log.warn(\"翻译失败, &#123;&#125;调用&#123;&#125;()异常\", data.getClass(), getterName);\n                    continue;\n                &#125;\n\n                DropdownVo dropdown = dictDataService.getDataByType(dict.value(), code);\n                if (dropdown == null) &#123;\n                    log.warn(\"翻译失败, &#123;&#125;类中的&#123;&#125;未找到数据库中&#123;&#125;对应字典数据\", getClass(), dict, code);\n\n                    continue;\n                &#125;\n                ReflectUtil.setFieldValue(data, field.getName(), dropdown.getLabel());\n\n            &#125;\n        &#125;\n    &#125;\n\n    public &lt;V> void notNull(V obj, Consumer&lt;V> consumer) &#123;\n        if (obj != null) &#123;\n            consumer.accept(obj);\n        &#125;\n    &#125;\n    private char toUpperCase(char c) &#123;\n        if (97 &lt;= c &amp;&amp; c&lt;= 122) &#123;\n            c ^= 32;\n        &#125;\n        return c;\n    &#125;\n\n&#125;\n\n\n","slug":"减少你的代码量：基于AOP实现字典翻译","date":"2022-11-18T09:40:43.000Z","categories_index":"","tags_index":"SpringAOP,减少你的代码量","author_index":"姚文彬"},{"id":"132bc548a30620e50cfa7c41af4da051","title":"OkHttp使用指南:封装一个好用的Http客户端","content":"OkHttp是一个高效的HTTP请求客户端对于Android和Java应用。其有着许多高级的功能，比如线程池、GZip压缩和响应缓存。 同时不仅能够发送同步的请求，也能够支持异步调用。\n&lt;dependency>\n    &lt;groupId>com.squareup.okhttp3&lt;/groupId>\n    &lt;artifactId>okhttp&lt;/artifactId>\n    &lt;version>4.9.1&lt;/version>\n&lt;/dependency>\n\n同步请求我们需要基于URL地址来构建一个Reqeust对象，然后调用newCall()来生成一个Call对象，然后调用execute()方法来同步发送请求并获取相应结果\n@Test\npublic void whenGetRequest_thenCorrect() throws IOException &#123;\n    Request request = new Request.Builder()\n      .url(BASE_URL + \"/date\")\n      .build();\n\n    Call call = client.newCall(request);\n    Response response = call.execute();\n\n    assertThat(response.code(), equalTo(200));\n&#125;\n\n\n异步请求@Test\npublic void whenAsynchronousGetRequest_thenCorrect() &#123;\n    Request request = new Request.Builder()\n      .url(BASE_URL + \"/date\")\n      .build();\n\n    Call call = client.newCall(request);\n    call.enqueue(new Callback() &#123;\n        public void onResponse(Call call, Response response) \n          throws IOException &#123;\n            // ...\n        &#125;\n        \n        public void onFailure(Call call, IOException e) &#123;\n            fail();\n        &#125;\n    &#125;);\n&#125;\n\n添加请求参数@Test\npublic void whenGetRequestWithQueryParameter_thenCorrect() \n  throws IOException &#123;\n    \n    HttpUrl.Builder urlBuilder \n      = HttpUrl.parse(BASE_URL + \"/users\").newBuilder();\n    urlBuilder.addQueryParameter(\"name\", \"zhangsan\");\n\n    String url = urlBuilder.build().toString();\n\n    Request request = new Request.Builder()\n      .url(url)\n      .build();\n    Call call = client.newCall(request);\n    Response response = call.execute();\n\n    assertThat(response.code(), equalTo(200));\n&#125;\n\n添加请求体表单请求通过FormBody我们能够构造出表单请求（application&#x2F;x-www-form-urlencoded），使用方式如下：\n@Test\npublic void whenPostRequestWithBody_thenCorrect() \n  throws IOException &#123;\n    RequestBody formBody = new FormBody.Builder()\n      .add(\"username\", \"test\")\n      .add(\"password\", \"test\")\n      .build();\n\n    Request request = new Request.Builder()\n      .url(BASE_URL + \"/users\")\n      .post(formBody)\n      .build();\n\n    Call call = client.newCall(request);\n    Response response = call.execute();\n    \n    assertThat(response.code(), equalTo(200));\n&#125;\n\n\nJSON请求为了适应目前RESTAfulAPI风格的JSON请求格式，OkHttp提供了String类型的json支持，我们可以通过一些json序列化库来帮助我们将一些实体类型转化为json后发送请求：\nprivate MediaType JSON_TYPE = MediaType.get(\"application/json; charset=utf-8\");\n\n@Test\npublic void whenPostRequestWithJSON_thenCorrect() \n  throws IOException &#123;\n    User user = new User().userId(1L).username(\"yaowenbin\");\n    String json = JSON.toJSONString(user);\n    RequestBody requestBody = RequestBody.create(JSON_TYPE, json);\n\n    Request request = new Request.Builder()\n      .url(BASE_URL + \"/users\")\n      .post(formBody)\n      .build();\n\n    Call call = client.newCall(request);\n    Response response = call.execute();\n    \n    assertThat(response.code(), equalTo(200));\n&#125;\n\n\nPUT&#x2F;DELETE请求PUT&#x2F;DELETE和POST请求也类似，只不过把Request调用的post方法转化成了对应的put()和delete()罢了\n@Test\npublic void whenPuttRequestWithBody_thenCorrect() \n        throws IOException &#123;\n    ...\n    Request request = new Request.Builder()\n        .put(requestBody)\n        .url(BASE_URL + \"/users\")\n        .build();\n\t...\n&#125;\n\n@Test\npublic void whenPuttRequestWithBody_thenCorrect() \n        throws IOException &#123;\n    ...\n    Request request = new Request.Builder()\n        .delete(requestBody)\n        .url(BASE_URL + \"/users\")\n        .build();\n\t...\n&#125;\n\n\n请求头单次请求头我们只需要在Request的Builder中使用addHeader即可在请求中添加请求头了：\n@Test\npublic void whenSetHeader_thenCorrect() throws IOException &#123;\n    Request request = new Request.Builder()\n      .url(SAMPLE_URL)\n      .addHeader(\"Content-Type\", \"application/json\")\n      .build();\n\n    Call call = client.newCall(request);\n    Response response = call.execute();\n    response.close();\n&#125;\n\n全局请求头如果我们希望每一个发送的请求都能够携带上全局的请求参数，比如说登录Token时，OkHttp使用了Interceptor来帮助我们达到这样的效果：\n@Test\npublic void whenSetDefaultHeader_thenCorrect() \n  throws IOException &#123;\n    \n    OkHttpClient client = new OkHttpClient.Builder()\n      .addInterceptor(\n        new DefaultContentTypeInterceptor(\"application/json\"))\n      .build();\n\n    Request request = new Request.Builder()\n      .url(SAMPLE_URL)\n      .build();\n\n    Call call = client.newCall(request);\n    Response response = call.execute();\n    response.close();\n&#125;\n\n封装一个更加易用API\n\n\n\n\n\n\n\n\n对于序列化工具你想要使用FastJSON2还是Jackson还是Gson都是没有问题的\nimport com.alibaba.fastjson.JSONObject;\nimport okhttp3.*;\n\nimport java.io.IOException;\n\n/** \n * @author : yaowenbin\n */\npublic class HttpUtil &#123;\n    static OkHttpClient HTTP_CLIENT = new OkHttpClient.Builder()\n            .build();\n\n    public static final MediaType JSON\n            = MediaType.get(\"application/json; charset=utf-8\");\n\n    public static String get(String url) &#123;\n        Request request = new Request.Builder()\n                .get()\n                .url(url)\n                .build();\n\n        return synchronizedCall(request);\n    &#125;\n\n    public static &lt;T> T get(String url, Class&lt;T> clz) &#123;\n        return parse(get(url), clz);\n    &#125;\n\n    public static String post(String url, String json) &#123;\n        RequestBody requestBody = RequestBody.create(JSON, json);\n        Request request = new Request.Builder()\n                .post(requestBody)\n                .url(url)\n                .build();\n\n        return synchronizedCall(request);\n    &#125;\n    public static &lt;T> T post(String url, String json, Class&lt;T> clz) &#123;\n        return parse(post(url, json), clz);\n    &#125;\n\n    public static String put(String url, String json) &#123;\n        RequestBody body = RequestBody.create(JSON, json);\n\n        Request request = new Request.Builder()\n                .url(url)\n                .put(body)\n                .build();\n\n        return synchronizedCall(request);\n    &#125;\n\n    public static String put(String url) &#123;\n        return put(url, \"\");\n    &#125;\n\n\n    public static &lt;T> T put(String url, String json, Class&lt;T> clz) &#123;\n        return parse(put(url, json), clz);\n    &#125;\n\n\n    public static String delete(String url, String json) &#123;\n        RequestBody body = RequestBody.create(JSON, json);\n\n        Request request = new Request.Builder()\n                .url(url)\n                .delete(body)\n                .build();\n        return synchronizedCall(request);\n    &#125;\n\n    public static String delete(String url) &#123;\n        return delete(url, \"\");\n    &#125;\n\n    public static &lt;T> T delete(String url, Class&lt;T> clz) &#123;\n        return parse(\n                delete(url, \"\"),\n                clz\n        );\n    &#125;\n\n    public static &lt;T> T delete(String url, String json, Class&lt;T> clz) &#123;\n        return parse(delete(url, json), clz);\n    &#125;\n\n    private static String synchronizedCall(Request request) &#123;\n        try ( Response response = HTTP_CLIENT.newCall(request).execute() )&#123;\n            return response.body().toString();\n        &#125; catch (IOException e) &#123;\n            throw new RuntimeException(e);\n        &#125;\n    &#125;\n\n    private static&lt;T> T synchronizedCall(Request request, Class&lt;T> clz) &#123;\n        return parse(synchronizedCall(request), clz);\n    &#125;\n\n    public static &lt;T> T parse(String json, Class&lt;T> clz) &#123;\n        return JSONObject.parseObject(json, clz);\n    &#125;\n&#125;\n\n","slug":"OkHttp使用指南-封装一个好用的Http客户端","date":"2022-11-17T14:45:41.000Z","categories_index":"","tags_index":"工具,OkHttp","author_index":"姚文彬"},{"id":"c0da45d30bd1cfd1cf13bf28f1302d4d","title":"Redis BitMap学习:实现海量二值数据统计","content":"为什么要使用bitmapBitmap 的底层数据结构用的是 String 类型的 SDS 数据结构来保存位数组，把每个字节数组的 8 个 bit 位利用起来，每个 bit 位 表示一个元素的二值状态（不是 0 就是 1）。可以将 Bitmap 看成是一个 bit 为单位的数组，所以能够极大程度的节省空间。数组的每个单元只能存储 0 或者 1，数组的下标在 Bitmap 中叫做 offset 偏移量。\nbitmap如何使用bitmap在Redis2.2.0版本后就被引入了，所以我们可以直接使用。其常用操作指令有：SETBIT &lt;key&gt; &lt;offset&gt; &lt;[0,1]&gt;、GETBIT &lt;key&gt; &lt;offset&gt;、BITCOUNT &lt;key&gt;、BITPOS KEY、BITOP operation destkey key [key ...]。\n使用场景：用户打卡假设我们使用一个bitmap来统计所有的用户打卡状态，现在用户1001要打卡。那么通过SETBIT sign_status 1001 1来表示用户1001打卡；\n判断用户是否打卡通过GETBIT sign_status 1001来查看该用户是否打卡。\n查看用户每个月打卡情况如果要按照时间维度来进行统计的话，我们一般会把key设置成sign:&#123;uid&#125;:&#123;yyyyMM&#125;的格式。而对于每个key最多只会占用31个bit也就是4个字节， 非常的节省了。\n\n用户1001在2015年5月16日打卡\nSETBIT sign:1001:202105 16 1\n\n判断用户在2015年5月的打卡情况\nBITCOUNT sign:1001:202105\n\n查看用户首次打卡日期通过BITPOS，我们可以快速的得出一个BITMAP中第一个值为的offset\nBITPOS sign:1001:202105 1\n\n\n统计某个时间段的所有用户登陆次数如果在记录了一亿个用户连续7天打卡数据，如何统计出这7天连续打卡的用户总数呢？Redis提供了BITOP &lt;operation&gt; &lt;destinationKey&gt; &lt;key1&gt; [&lt;key2&gt; ...]这个指令，能够对多个BITMAP进行AND、OR、NOT、XOR的位计算。为了应付这种业务需求，我们会采用sign:&#123;yyyyMMdd&#125; &#123;uid&#125;的设计方式来实现需求。 而想要统计2022年11月01日到03日的用户连续打卡次数只需要通过指令：\nBITOP and sign:20221101to20221103 sign:20221101 sign:20221102 sign:20221103\n即可实现。\n小结当我们遇到的统计场景只需要统计数据的二值状态，比如用户是否存在、 ip 是否是黑名单、以及签到打卡统计等场景就可以考虑使用 Bitmap。只需要一个 bit 位就能表示 0 和 1。在统计海量数据的时候将大大减少内存占用。\n","slug":"Redis-BitMap学习-实现海量二值数据统计","date":"2022-11-15T01:08:44.000Z","categories_index":"","tags_index":"Reids","author_index":"姚文彬"},{"id":"738b9e0126fd231ff00bb0e09eaaa743","title":"如何使用Swagger3与OpenAPI规范","content":"swagger 是一个 api 文档维护组织，后来成为了 Open API 标准的主要定义者，现在最新的版本为17年发布的 Swagger3（Open Api3）。 国内绝大部分人还在用过时的swagger2（17年停止维护并更名为swagger3） swagger2的包名为 io.swagger，而swagger3的包名为 io.swagger.core.v3。 \n17年就停更了东西，目前Swagger2依然占大多数市场，我也不知道为啥\n相比于Swagger2，Swagger3的配置非常的简洁。只需要引入一个Starer的包，并自己定义一个配置文件即可。\n实现步骤（项目中已经实现）：\n\n引入依赖\n&lt;!--Swagger3-->\n&lt;dependency>\n  &lt;groupId>io.springfox&lt;/groupId>\n  &lt;artifactId>springfox-boot-starter&lt;/artifactId>\n  &lt;version>3.0.0&lt;/version>\n&lt;/dependency>\n\n定义配置文件并使用spring.factories自动装配\nimport io.swagger.annotations.ApiOperation;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\nimport springfox.documentation.builders.ApiInfoBuilder;\nimport springfox.documentation.builders.PathSelectors;\nimport springfox.documentation.builders.RequestHandlerSelectors;\nimport springfox.documentation.service.ApiInfo;\nimport springfox.documentation.service.Contact;\nimport springfox.documentation.spi.DocumentationType;\nimport springfox.documentation.spring.web.plugins.Docket;\n\n@Configuration\npublic class Swagger3Config &#123;\n    @Bean\n    public Docket createRestApi() &#123;\n        return new Docket(DocumentationType.OAS_30)\n                .apiInfo(apiInfo())\n                .select()\n                .apis(RequestHandlerSelectors.withMethodAnnotation(ApiOperation.class))\n                .paths(PathSelectors.any())\n                .build();\n    &#125;\n    private ApiInfo apiInfo() &#123;\n        return new ApiInfoBuilder()\n                .title(\"Swagger3接口文档\")\n                .description(\"房易租-租房管理系统\")\n                .contact(new Contact(\"房易租\", \"http://www.baidu.com\",\"ywb992134@163.com\"))\n                .version(\"1.0\")\n                .build();\n    &#125;\n&#125;\n\norg.springframework.boot.autoconfigure.EnableAutoConfiguration = \\\n  com.fangyz.swagger.config.Swagger3Config\n\n\n在对应的Controller类上添加@Api和方法上添加@ApiOperation即可\n\n\n需要说明的是：Swagger会造成严重的代码侵入，因为每个Controller的接口都需要@ApiOperation，但是这也是最方便生成API文档的方法了。所以对于没有时间写API文档的小项目来说，Swagger还是占有非常重要的地位的。\n为什么Swagger3不需要在启动类上@OpenApi（Swagger3中的启动类注解）？因为在Swagger3依赖包中，我们可以找到一个spring.factories，，熟悉Spring Boot的同学都知道这个是一个Spring Boot 特有的SPI文件，能够自动的发现并注册Starter组件的配置。里面有这样的配置：\n# Auto Configure\norg.springframework.boot.autoconfigure.EnableAutoConfiguration=\\\nspringfox.boot.starter.autoconfigure.OpenApiAutoConfiguration\n其OpenApiAutoConfiguration具体的实现源码就是\n@Configuration\n@EnableConfigurationProperties(SpringfoxConfigurationProperties.class)\n// 下面这个是关键默认值就是true，也就是通过这个为我们完成了自动配置\n@ConditionalOnProperty(value = \"springfox.documentation.enabled\", havingValue = \"true\", matchIfMissing = true)\n@Import(&#123;\n    OpenApiDocumentationConfiguration.class,\n    SpringDataRestConfiguration.class,\n    BeanValidatorPluginsConfiguration.class,\n    Swagger2DocumentationConfiguration.class,\n    SwaggerUiWebFluxConfiguration.class,\n    SwaggerUiWebMvcConfiguration.class\n&#125;)\n@AutoConfigureAfter(&#123; WebMvcAutoConfiguration.class, JacksonAutoConfiguration.class,\n    HttpMessageConvertersAutoConfiguration.class, RepositoryRestMvcAutoConfiguration.class &#125;)\npublic class OpenApiAutoConfiguration &#123;\n \n&#125;\n在@EnableOpenApi中我们也可以发现其就是为我们导入了OpenApiAutoConfiguration这个类\n@Retention(RetentionPolicy.RUNTIME)\n@Target(&#123;ElementType.TYPE&#125;)\n@Documented\n@Import(&#123;OpenApiDocumentationConfiguration.class&#125;)\npublic @interface EnableOpenApi &#123;\n&#125;\nSwagger2中的那个Enable巴拉巴拉的也是这样，不过Swagger2并不会为我们自动配置，需要我们手动开启而已\n","slug":"如何使用Swagger3与OpenAPI规范","date":"2022-11-13T08:09:01.000Z","categories_index":"","tags_index":"OpenAPI","author_index":"姚文彬"},{"id":"09b0502743a194205f4a0a60c94f4ead","title":"MySQL深分页问题及解决方案","content":"在MySQL中的需要分页操作我们会使用limit关键字加上偏移量来实现。但实际上limit 10000， 10分页关键字的实际原理是，查询出10000 + 10行数据后，舍弃前面的10000行数据，然后取最后的10行数据。\n而要想优化这种分页查询，我们会尽可能的使用覆盖索引扫描 + 延迟关联来解决，具体的方案就是：\nSELECT * FROM `audit` INNER JOIN \n\t(SELECT id FROM `audit` ORDER BY `create_time` LIMIT 10000, 10) AS lim \nUSING(id);\n这样做的原理就是让执行器在二级索引中树找到10行数据，然后返回到聚集索引树取出所有记录后返回。\n如果是普通的LIMIT语句的话，那么其MySQL执行流程是这样的：\n\n会在二级索引树中去找出过滤出10010行数据。\n然后根据这10010行数据去聚集索引树中找到这10010行数据的取出（回表）。\n舍弃前10000万数据返回，取后10行数据返回（舍弃）。\n\n","slug":"MySQL深分页问题及解决方案","date":"2022-11-12T13:28:38.000Z","categories_index":"","tags_index":"MySQL","author_index":"姚文彬"},{"id":"f09369e68b8b45b4d7396ee196bab5db","title":"MySQL索引到底什么场景会失效？","content":"MySQL在一些场景下会出现设置了索引也会失效的情况。\n1. 数据量过小当MySQL的数据量只有几行十几行的时候，这时候MySQL可能会认为全表扫描的效率甚至高于使用索引扫描，所以产生索引失效。\n数据量过小的情况无论是否使用索引速度都很快，所以无需优化。\n2. 当表过大在MySQL之前的版本中，如果一个索引的返回数据行范围超过了全表的30%，那么MySQL同样会认为全表扫描的效率可能更好，导致了不走索引。 而目前这个基于30的百分比变成了更加复杂的判断条件，会基于表的大小、数据行数和IO块大小来判断。\n这时候的解决方案有两个\n\n如果是MySQL误判的话，使用analy table来重新对表进行分析。\n如果是索引的返回数据量真的超过了MySQL的阈值，但我们依然想要走索引的话可以使用force index来强制某条SQL语句使用索引。\n\n错误结论：使用了or语句且左右是不相同的列MySQL推出了索引合并的概念，来提高or语句时左右是不同列时的查询。  当查询的where语句下包含了or，并且or连接两个不同的索引列的时候， 我们使用explain能够观察到。 其会使用一个名为index_merget的索引类型。\n\n\n\n\n\n\n\n\n\nMySQL索引合并的失效场景：多表连接、较为复杂的OR和AND查询和全文索引。\nselect * from `user` where id = 1 or name = 2;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nunion会对结果进行去重，导致额外的开销。如果能够确保查询结果不会有重复则使用union all性能更高。\n参考文章：MySQL8.0官方文档: 索引合并优化\n","slug":"MySQL索引到底什么场景会失效？","date":"2022-11-12T13:28:26.000Z","categories_index":"","tags_index":"MySQL","author_index":"姚文彬"},{"id":"7f627cb8c8effb7ae31a975e2ea25f37","title":"为什么MySQL不使用Hash作为索引类型","content":"因为Hash这个数据结构的特征，导致每个key最后的哈希值是无序的。所以\n\n使用Hash作为索引类型的使用无法使用任何的范围查询。\n也无法使用像b+树那样满足like语句。\n优化器也无法将hash作用于order by操作符\n\n","slug":"为什么MySQL不使用Hash作为索引类型","date":"2022-11-12T13:26:39.000Z","categories_index":"","tags_index":"MySQL","author_index":"姚文彬"},{"id":"4ddb3a53c2271811530493a47c276c44","title":"Java应届生面试复盘","content":"\n\n\n主要内容\n日期 &#x2F; 地点\n面试结果\n\n\n\nJava基础、并发编程、JVM、MQ、业务设计。\n2022-11-05 线上\n一般\n\n\n业务设计\n\n\n\n\n\n\n\n\n说说业务设计时你会考虑什么\n首先，我会先确认好业务需求，明确这个功能会接受什么样的输入，产生什么样的输出。然后跟上游沟通好是否需要进行额外的特殊处理，比如说异步、幂等性或者高并发。然后根据功能的大小将其为多个明确的子功能。然后以验收测试驱动开发的思想，我会先准备好子功能的测试用例（至少包含正常状态和异常输入状态）。对每个子功能进行详细的思路设计（在大脑中找出实现的方案，然后找出最短的可行路径），编写单元测试。实现代码。\nJava基础\n\n\n\n\n\n\n\n\nequals和&#x3D;&#x3D;的区别\n对于基本类型，&#x3D;&#x3D;判断两个值是否相等，euqals方法不存在于基本类型中；对于引用类型，&#x3D;&#x3D;判断两个引用是否指向同一个对象，equals用于比较两个对象是否等价；Object类中的equals方法默认比较的就是地址。\n\n\n\n\n\n\n\n\n\n为什么说重写equals后要重写hashcode方法\nJava之所以使用了equals和hashCode两个方法来判断两个对象是否相等，是出于可靠性和性能。\n\nequals用于比较两个对象是否绝对相等。\nhashCode用于快速判断两个对象是否相等，由于哈希碰撞可能会出现误差。\n\n我们对于这两个方法会有以下约定：\n\n两个对象的euqals相等，hashCode一定相等。\n两个对象的hashCode相等，euqals方法不一定相等。\n\n如果只重写了equals而没有重写hashCode方法，会导致new出了两个相同属性的对象，其调用equals方法之后是相等的，但是他们hashCode方法继承于Object类，是比较两个对象的内存地址，散列值不同。这就违背了equals的可靠性\n\n\n\n\n\n\n\n\n\nArrayList和LinkedList的区别\nArrayList是基于数组这个数据结构实现的，LinkedList是基于链表实现的。前者在随机访问和尾部操作时的效率高，后者在插入和删除时的效率高，但链表的内存占用比较大。而LinkedList的一个优势是其使用了Deque接口，这就意味着其可以作为栈、队列两个数据结构的具体实现。\n\n\n\n\n\n\n\n\n\n并发场景下如何保证列表的线程安全\nJDK中提供了三种线程安全的列表：Vector、syncronizedList和copyOnWriteList\n\nVector是从JDK1.0开始就提供了线程安全列表，但是其实现方式十分的粗暴。就是通过在对外提供的public的方法上添加上syncronized关键字来实现的，所以性能较低。\nCollections.synchronizedList(List list) 是 JDK1.2 版本后推出的Collections工具类中的API，能够返回指定参数列表的线程安全版本。 其实现方式是通过减少syncronized的同步粒度来优化性能。\ncopyOnWriteList 是从JDK1.5版本后由juc工具包的开发大佬Doug Lea编写的基于写时复制机制实现的线程安全列表。目的在于将列表的读性能发挥极致，在类的使用过程中读和写操作都互不干涉。在写的时候会从原数组中复制出一个新的数组，然后对新数组进行写操作后再将其覆盖回原数组。而此时如果发生了读操作就只能够读取到旧的数组，由于读写操作位于不同的数组上，所以不会发生线程安全问题。\n\n\n\n\n\n\n\n\n\n\nJava的错误机制\nThrowable是Java中所有异常和错误的父类。其中包含了printStackTrace()接口用于获取堆栈信息。\n而Throwable的有两个子类：Error和Exception。Error是程序无法处理的错误，一般是由于运行程序中出现了严重错误。一般表示JVM在运行时出现问题，比如NoClassDefFoundError、OutOfMemoryError和StackOverFlow Error。\nException是程序本身可以捕获并且处理的异常，又分为两个子类：RuntimeException和编译器异常（Java中没有明确的定义类）RuntimeException是程序运行中才会出现的异常，比如说数组下标越界、空指针异常等。这类异常一般是由于程序逻辑错误导致的。\n编译器异常从程序语法角度讲是必须处理的异常，如果不能处理编译就不能通过。 比如IOException、SQLException。所以这类异常也叫做受检异常， 受检异常虽然是异常状态，但是从某种程度上他是可以预知的，所以Java编译器会对其进行检查，如果出现这类异常要么使用try-catch捕获他，要么使用throws声明抛出它，否则将会编译不通过。\n\n\n\n\n\n\n\n\n\nJava包装类的缓存机制\nJDK会为Boolean、Byte、Short、Integer、Character包装类中的值进行缓存，Integer中默认缓存池范围是-128～127。如果在缓存池范围内的包装类对象调用valueOf方法的话就取出缓存池中的值，而不是创建一个新对象了。这样做就能够节约内存并且提高运行效率，因为对象已经创建好了就无需再重复创建。\nJava并发编程\n\n\n\n\n\n\n\n\n并发编程三要素\n原子性、可见行和有序性。\n\n\n\n\n\n\n\n\n\n线程池中的线程出现了异常会发生什么？\n当线程池中的线程出现异常时，会被Executors框架捕捉到，对于Runnable接口的类会被隐藏；对于Callable方法在调用其返回值Future对象的get方法时，Executors会将异常信息传递给调用者。\nJVM\n\n\n\n\n\n\n\n\n泛型的作用是什么？JVM是如何实现泛型特性的？\n泛型的作用是参数化数据类型，在泛型使用过程中，操作的数据类型被指定为一个参数。JDK1.5在版本之后推出的新特性，考虑到向前兼容这个特性，所以使用到的实现方案是擦除法，本质上就是一个语法糖，参数类型会在代码在编译后就被擦除，就像没有泛型一样。\nTODO：其他语言中泛型的解决方案。\n\n\n\n\n\n\n\n\n\n volatile关键字的作用\nvolatile关键字的作用是保持变量的可见性和有序性和原子性。\n\n有序性是指防止操作系统对指令进行重排，比较经典的例子就是单例模式的双重检查，因为实例化对象的步骤可以分为：分配内存空间、初始化对象和内存空间地址赋值给引用。 但是OS会对指令进行重排序，导致实际执行顺序不同，为了防止这个重排序，所以我们需要在使用volatile关键字修饰单例对象。\n可见性问题是指当一个线程修改了共享变量而另外一个线程看不到。 这个问题的主要原因是因为没个线程都拥有自己的高速缓存区 – 线程工作内存。 而volatile关键字能够解决这个问题：当变量发生变化时，会立即刷回主内存中，并及时通知其他线程的缓存更新。\n原子性，volatile关键字能够保证单次读&#x2F;写的原子性。 long和double两种数据类型是64位的，而对于32位的JVM来说，会将其分为高32位和低32位进行缓存。因此普通的long或double类型的读&#x2F;写操作不是原子的。\n（目前各个平台的商用虚拟机包括HotSpot都会将64位数据的读写操作作为原子操作来对待，因此不使用volatile进行修饰也不会出错的）\n\n\n\n\n\n\n\n\n\n\n\n\nJVM内存屏障是什么？ 如何实现？\n\n\n\n\n\n\n\n\n\nJVM如何保证volatile的特性的？\nJVM是通过内存屏障来保证volatile的特性的。内存屏障是一个CPU指令，会告诉编译器和CPU不管什么指令都不能和这条Memory Barrier指令重排序。\n对于可见性：从编译后的代码来看，对于volatile关键字变量的写操作前会添加上lock前缀，这个前缀会发生两件事：将当前处理器的缓存行数据写回系统内存。通知总线该变量被修改了。而其他处理器通过嗅探总线上传播的数据来检查自己缓存的值是不是过期了， 如果过期了就将数据设置为无效状态。通过这样的机制使得每个线程都能够获得该变量的最新值。\n对于有序性：编译器会在生成指令序列时在适当的位置插入内存屏障指令来禁止特定类型的处理器重排序。对于volatile变量\n\n在每一个读操作后插入LoadLoad和LoadStore屏障。 \n在每一个写操作前插入StoreStore屏障，写操作后插入StoreLoad屏障。\n\n\n\n\n\n\n\n\n\n\nJVM如何保证synchronized的同步的？\nsynchronized是Java中进行同步操作最基本的关键字，可以作用在方法和代码块中。对于被syncronized修饰的方法或者代码块，编译器会在前后加上monitorenter和monitorexit两个指令。monitor可以看作是一把锁，每个对象在同一时间只能和一个monitor关联，且每个monitor只能够被一个线程获得。对于monitor的实现和可重入锁类似。monitorenter是否有这个monitor的所有权，如果有则表示重入锁，则计数器加一。若果没有，会检查计数器的次数，如果为0表示这个锁还没有被其他线程获得，则立即获得这把锁，并计数器加+1。对于对于monitorexit指令，就是将monitor计数器减一，如果为0则表示释放锁。\n网络\n\n\n\n\n\n\n\n\n三次握手\n三次握手是TCP的连接建立过程，与HTTP协议无关，只不过因为HTTP协议是基于于TCP实现的应用层协议。握手之前，客户端和服务端都处于CLOSE状态，然后先是服务端主动监听某个端口，处于LISTEN状态。第一次握手时，客户端初始化一个随机序列号放入报文首部的「序列号」字段中，然后将报文的SYN标志位设置为1，发送给服务端，随后客户端进入SYN_SENT状态。第二次握手时，服务端接收到客户端的SYN报文后，初始化自己的序列号，填入「序列号」字段，然后将「确认答应」字段填入客户端的seq+1，然后把SYN和ACK标志位设置为1，发送给客户端。服务端进入SYN_RCVD状态。第三次握手时，客户端收到服务端的报文后，最后回复一个答应报文，将ACK标志位设置为1，然后将服务端报文中的seq+1填入TCP报文首部的「确认答应」字段中，发送给客户端，进入ESTABLISH状态。服务端收到后也会进入ESTABLISH状态。\n\n\n\n\n\n\n\n\n\n三次握手能够附带数据吗？\n由于第三次握手时能够确认客户端和服务端能够建立连接，所以处于性能考虑，第三次握手时的报文是可以附带数据的。 而前两次握手的主要目的在于「建立连接」所以不能携带报文。\n\n\n\n\n\n\n\n\n\n为什么要三次握手？ 两次不行吗\n因为要考虑到网络阻塞的问题，当第一次握手时，客户端发送的\n\n\n\n\n\n\n\n\n\n什么是跨域问题？\n跨域问题是源于浏览器的同源策略。 这是浏览器中一个重要的安全策略，防止XSS、CSFR等浏览器层面的攻击。而同源是指：相同的协议、相同的域名和相同的端口所以当访问不同协议、不同域名、不同端口的资源时会出现跨域问题。\n当我们的项目是前后端分离架构时，前后端可能会部署在不同的服务器上，这时就会导致跨域的问题。\n\n\n\n\n\n\n\n\n\n解决跨域问题的常见方案？\n\nNginx方向代理。使用Nginx作为一层跳板机，将指定同源域名后的资源（比如说&#x2F;api）映射到具体后端服务器地址，实现跨域功能。\n\nhttp &#123;\n  server &#123;\n    location ~/api &#123;\n      # 反向代理\n      proxy_pass xxxxx; #\n    &#125;\n\n\nWebpack Server代理，可以在webpack.config.js中配置一个devServer，然后将指定资源映射到具体后端服务器的地址。\n\nmodule.exports = &#123; \n  devServer: &#123; \n    port: 3000, \n    proxy: &#123; \n      \"/api\": &#123; \n        target: \"http://localhost:3001\" \n      &#125; \n    &#125; \n  &#125;, \n&#125;;\n\n\n\n\n\n\n\n\n\n\n我们使用到的IDE：Webstorm、IDEA等是如何直接运行vue项目的？\nIntelliJ公司的IDE会内置一个轻量级的NodeJs的服务器，当我们在Configurations中配置好前端项目的package.json后会自动将前端项目编译打包放入node服务器进行运行。\nMQ\n\n\n\n\n\n\n\n\n线程池同样能够做到异步的作用，那么为什么还要使用MQ？\n第一个核心作用是削峰，比如当上游处理简单请求，平均处理速度为1w&#x2F;s，而下游处理秒杀的复杂请求，平均处理速度为2k&#x2F;s。 为了防止下游系统不被压垮，我们就需要引入MQ的削峰功能。我们就可以通过MQ的MQ-clinet模式，将下游的处理方式从MQ推送变为Client拉取。 这样下游就能够根据自己的处理能力，每隔一段时间拉取若干信息。\n第二个核心功能是解藕，这个解藕不单单是指生产者和消费者的解藕。 当上下游服务系统是异构的话，就需要MQ来帮助我们实现通讯的功能。\n参考文章：\n《图解HTTP》 作者：小林Coding\nJava全栈知识体系：https://pdai.tech/md/java/basic/java-basic-lan-basic.html\n\n","slug":"Java应届生面试复盘","date":"2022-11-07T03:07:40.000Z","categories_index":"","tags_index":"面经","author_index":"姚文彬"}]