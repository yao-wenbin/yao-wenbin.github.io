[{"id":"bb79ff4f10f53b63ae134cf1b5bcc787","title":"多租户架构解析","content":"多租户架构是SaaS系统开发中不可避免要考虑到的问题之一。 多租户的意义在在于数据完全的隔离，即公司A无法看到公司B的任何数据，反之亦然。 使得每一个公司从自身层面上看都是独占系统的。\n\n\n\n\n\n\n\n\n\n实现多租户功能需要尽可能的通过架构底层来实现，而不是让开发人员从业务SQL上进行拼接。否则极其容易出现问题，稍有不慎就会导致业务数据错乱。\n而业界实现多租户主要归纳为几种解决方案：单表租户列、多租户表单数据库、多数据库。\n单表租户列在每个业务表中添加上一个租户的列（比如说tenant_id），在每次查询时，都通过租户列进行过滤以实现数据隔离。 这是最简单、也是较为普遍的设计方案。 使得一个系统中只需要有一个数据库，一套业务表就能够实现多租户架构。\n实现这种方案，我们可以通过mybatis、或者是mybatis-plus中的插件来实现。\n以Mybatis-Plus为例，我们只需要实现一个插件，并且将其注入到IOC容器中，Myabtis-Plus就会自动识别到该插件并注册。\n单表租户列的方案适用于前期平台的业务量并不高，即使所有租户的数据量加起来都不会超过MySQL单表所能够承载的数据量（MySQL单表在保证性能下能够支撑的最大数据量参考TODO）。\n设计方案简单，仅需要引入Mybatis插件即可实现多租户隔离。\n租户多表&#x2F;租户多库\n\n\n\n\n\n\n\n\n对于MySQL中，这里的多库指的是多Schema，而不是多个MySQL服务器实例\n如果业务数据增长较快，很快就会抵达MySQL下单表的性能瓶颈时，我们就需要考虑将单数据库单表切分为多表或者是多数据库单表。\n租户多表的思想是：不同的租户数据使用使用不同的后缀表或者是库区分。在租户多表的情况下比如说公司A的user表叫user_001，公司B的user表叫user_002；在租户多库的情况下公司的A的数据存放在app_01.user表中，公司B的数据存放在app_02.user中。\n租户多表&#x2F;多库方案使得不同的公司的数据存放在不同的数据表&#x2F;库中，使得数据和性能的隔离性进一步提升。无论如何查询，公司A都无法查询到公司B的数据，并且每个租户都具有自身单独的业务数据量，不会因为公司A的数据量大超出而公司B的数据量小，却因为数据存放在同一张表中，导致公司B使用系统时性能也收到影响。\n在具体实现中，我们主要需要依靠动态表名的功能来实现租户多表&#x2F;库方案，业界目前主流的有：Myabtis-Plus的动态表名插件，同样以插件化的方式拦截SQL语句进行动态拼接表名、Sharding Sphere下的ShardingJDBC在业务应用上实现多租户表的动态拼接、或者是阿里的MyCat通过代理数据库的方式实现。\n租户独立数据库独立数据库方案和思想是：将每个租户的数据存放在独立的数据库中，使得每个租户的数据和性能完全隔离，不会因为租户A的数据库崩溃影响到租户B的使用，使得可用性得到了最大的保障。\n要实现独立数据库的方案，我们需要借助动态数据源来实现数据库的连接的切换，在具体的实现，我们可以考虑使用由Myabtis-Plus团队研发的dynamic-datasource组件实现动态数据源和切换、同样使用Sharding JDBC或者是MyCat实现数据源的切换都能够实现该方案。\n独立数据库的方案虽然享受着数据和性能隔离的最大优势，但是在机器的成本上也会随着每一个租户的增长而增长。所以独立数据库一般在成本不再是一个考量范围，或者是对性能隔离有着严格要求时使用。（对于数据库的高可用，在租户多表&#x2F;多库依然可以通过分片来实现）\n","slug":"多租户架构解析","date":"2023-09-19T02:18:48.000Z","categories_index":"","tags_index":"架构设计","author_index":"姚文彬"},{"id":"ba1c6774f6090755efbcf2d6c3343eae","title":"Performance Schema入门指南:你的MySQL性能监测杀器","content":"简介Performance提供了有关MySQL服务器内部运行的底层指标。在高负载下数据库调优是一个循环迭代的过程，每次更改以调整数据库的性能时，都需要了解更改是否有什么影响。而Performance Schama就是一个能够存储回答这个问题所需要的数据的数据库。\n工作机制插桩（instrument）插桩是指在MySQL代码中插入探测代码，以获取我们想要了解的信息。 比如要收集关于select语句的情况，我们就需要启用statement&#x2F;sql&#x2F;select插桩。\n在performance_schema中，setup_instruments包含了所有支持的插桩列表。每个插桩的名称都有斜杆分割的部件组成。比如statment&#x2F;sql&#x2F;select，从左到右依次表示从通用到特定的子系统。\n消费者表（consumer）消费者表就是记录某个插桩从插入到当前运行时的数据。比如某条SQL的执行总数、是否使用索引、话费的时间等信息。\nMySQL 8.0.25版本中包含了110个表，基于用途可以分为以下几类：\n\n当前和历史数据\n汇总和摘要\n实例表（Instance）\n设置表（Setup）\n其他表\n\n资源消耗Performance Schema收集的数据都保存在内存中，而每个插桩指令的调用都会额外的产生宏调用，将数据存放到performance_schema中，这就意味着插桩越多CPU的使用频率越高。但对CPU使用率实际影响取决于特定的插桩。比如与statement相关的插件，在查询过程中只会被调用一次；而wait类的插桩调用频率就高的多，比如在扫描一个一百万行的InnoDB表，引擎需要设置和释放一百万行锁，那么CPU的使用率就会显著增加。所以启动statment、内存和元数据锁类型的插桩，你不会注意到对CPU负载的任何增加。\n注意事项\n插桩目标必须得到MySQL组件的支持。 假设想要使用内存插桩，那么存储引擎应该是支持内存插桩的。\n只在启用后才收集数据。 在禁用所有插桩的情况下启动服务器，然后想要监测内存使用情况。那么全局缓冲区（比如InnoDB缓冲池）的确切数量就无法得知，因为启用内存插桩前就已经分配了该缓冲区。\n它很难释放内存。禁用了特定的插桩或消费者表也不会释放内存，除非重启服务器。\n\nsys Schema自5.7版本依赖，MySQL发行版就包括一个和performacne_schema配套的sys_schema。 它是基于performance_schema上的视图和存储过程组成。 他的目的是让performance_schema的体验更加流畅。\n启动或禁用performance_schema要启用或禁用，只需要修改MySQL实例中的performance_schema即可，这是一个只读变量，所以要么在配置文件中修改，要么在MySQL服务器启动时通过命令参数更改\n插桩可以通过setup_instruments来查看插桩的状态\nmysql> SELECT * FROM performance_schema.setup_instruments WHERE NAME = 'statement/sql/select'\\G\n*************************** 1. row ***************************\n         NAME: statement/sql/select\n      ENABLED: YES\n        TIMED: YES\n   PROPERTIES:\n   VOLATILITY: 0\nDOCUMENTATION: NULL\n1 row in set (0.00 sec)\nENABLED值为YES，表示我们已经开启了statement/sql/select这个插桩了。\n对于插桩的禁用或启用，有三种方式\n\nUPDATE语句\n\n通过标准的SQL语句我们可以直接操作setup_instruments表\nmysql> UPDATE performance_schema.setup_instruments SET ENABLED = 'YES' WHERE NAME = 'statement/sql/select';\n\nQuery OK, 0 rows affected (0.00 sec)\nRows matched: 1  Changed: 0  Warnings: 0\n\n\nsys存储过程\n\n通过sys提供的存储过程，我们可以用更短的语句进行操作\nmysql> CALL sys.ps_setup_enable_instrument('statement/sql/select');\n\n+-----------------------+\n| summary               |\n+-----------------------+\n| Enabled 0 instruments |\n+-----------------------+\n1 row in set (0.02 sec)\n\nQuery OK, 0 rows affected (0.02 sec)\n\n\n\n\n\n\n\n\n\n\n上面两种方式都会随则MySQL实例的重新启动而失效\n\n启动选项\n\n这是唯一一个不会随着MySQL的重启而失效的方式。 该选项还支持通配符。\nperformance-schema-instrument='statement/sql/select=ON'\n\n消费者表与插桩一样，同样通过setup_consumers来查看消费者表的状态同时支持三种方式启动或禁用\n\n直接使用SQL操作setup_consumers表\n调用sys_schema的ps_setup_enable_consumer和ps_setup_disable_consumer来启动和禁用\n使用performance-schema-consumer启动参数\n\n使用场景检查SQL语句Performance Schema将SQL语句指标都存储在了events_statments_current、events_statments_history、events_statments_long三张具有相同的结构的表中。\n我们可以直接使用perfomance_schema进行查询：\nmysql> SELECT * FROM performance_schema.events_statements_history\\G;\n*************************** 1. row ***************************\n              THREAD_ID: 1580\n               EVENT_ID: 133\n           END_EVENT_ID: 133\n             EVENT_NAME: statement/sql/select\n                 SOURCE: init_net_server_extension.cc:97\n            TIMER_START: 989005876364000000\n              TIMER_END: 989005877298000000\n             TIMER_WAIT: 934000000\n              LOCK_TIME: 12000000\n               SQL_TEXT: SELECT * FROM events_statments_current\n                 DIGEST: 287a21eaee8bafb4d09ae546dbc232495489a4a8fd60e50eb535e1eb1cd384f4\n            DIGEST_TEXT: SELECT * FROM `events_statments_current`\n         CURRENT_SCHEMA: performance_schema\n            OBJECT_TYPE: NULL\n          OBJECT_SCHEMA: NULL\n            OBJECT_NAME: NULL\n  OBJECT_INSTANCE_BEGIN: NULL\n            MYSQL_ERRNO: 1146\n      RETURNED_SQLSTATE: 42S02\n           MESSAGE_TEXT: Table 'performance_schema.events_statments_current' doesn't exist\n                 ERRORS: 1\n               WARNINGS: 0\n          ROWS_AFFECTED: 0\n              ROWS_SENT: 0\n          ROWS_EXAMINED: 0\nCREATED_TMP_DISK_TABLES: 0\n     CREATED_TMP_TABLES: 0\n       SELECT_FULL_JOIN: 0\n SELECT_FULL_RANGE_JOIN: 0\n           SELECT_RANGE: 0\n     SELECT_RANGE_CHECK: 0\n            SELECT_SCAN: 0\n      SORT_MERGE_PASSES: 0\n             SORT_RANGE: 0\n              SORT_ROWS: 0\n              SORT_SCAN: 0\n          NO_INDEX_USED: 0\n     NO_GOOD_INDEX_USED: 0\n       NESTING_EVENT_ID: NULL\n     NESTING_EVENT_TYPE: NULL\n    NESTING_EVENT_LEVEL: 0\n           STATEMENT_ID: 14059\n               CPU_TIME: 0\n       EXECUTION_ENGINE: PRIMARY\n里面提供了非常多的有用的信息，包括：SQL语句是什么（SQL_TEXT）、所查询的schema是什么（CURRENT_SCHEMA）、是否包含错误或警告（ERRORS&#x2F;WARNINGS）、是否影响列（ROWS_AFFECTED）、是否使用了全表扫描（SELECT_SCAN）、是否创建了磁盘临时表（CREATED_TMP_DISK_TABLES）、是否没有使用索引(NO_INDEX_USED)、是否没有使用最合适的索引（NO_GOOD_INDEX_USED）\n查询无索引SQL当我们希望找到某个shema中所有没有使用合适索引的查询，可以运行一下命令：\nmysql> SELECT THREAD_ID, SQL_TEXT, ROWS_SENT, ROWS_EXAMINED, CREATED_TMP_TABLES,\nNO_INDEX_USED, NO_GOOD_INDEX_USED\nFROM performance_schema.events_statements_history_long\nWHERE CURRENT_SCHEMA = 'sakila' AND (NO_INDEX_USED > 0 OR NO_GOOD_INDEX_USED > 0);\n\n查询执行时间超过5秒的查询MySQL中为了更加精准的时间，TIMER_WAIT的时间单位为纳秒（1s &#x3D; 10^9 ns）\nmysql> SELECT THREAD_ID, SQL_TEXT, ROWS_SENT, ROWS_EXAMINED, CREATED_TMP_TABLES,\nNO_INDEX_USED, NO_GOOD_INDEX_USED\nFROM performance_schema.events_statements_history_long\nWHERE TIMER_WAIT > 5000000000 \n\n使用sys_schema前面我们说到sys_schema是基于performance_schema之上为我们提供方便使用的视图和存储过程的。我们可以通过视图查询来查询优化的语句，比如：\nmysql> SELECT query, total_latency, no_index_used_count, rows_sent,rows_examined FROM sys.statements_with_full_table_scans where db = 'charon'\\G;\n\n*************************** 1. row ***************************\n              query: SELECT * FROM `sql_explain_statistics`\n      total_latency: 97.00 us\nno_index_used_count: 1\n          rows_sent: 1\n      rows_examined: 1\n\n以下是可以用于查询需要优化的SQL语句的视图\n\n\n\n视图\n描述\n\n\n\nstatements_analysis\n具有聚合信息的语句视图\n\n\nstatments_with_errors_or_warnings\n所有引起错误或警告的语句视图\n\n\nstatements_with_full_table_scans\n所有执行了全表扫描的语句视图\n\n\nstatements_with_runtimes_in_95th_percentile\n所有平均执行时间在前95%的语句视图\n\n\nstatements_with_sorting\n所有执行了排序的语句视图\n\n\nstatements_with_temp_tables\n所有使用了临时表的语句视图\n\n\n","slug":"Performance-Schema入门指南-你的MySQL性能监测杀器","date":"2022-11-28T14:23:44.000Z","categories_index":"","tags_index":"MySQL,监控","author_index":"姚文彬"},{"id":"4468f948c4ca4e742d657728958a113e","title":"你真的了解你的MySQL吗(MySQL基准测试)","content":"如果你没有真正的对服务器上的MySQL进行基准测试，就无法了解其真实情况到底是如何。\n\n\n\n\n\n\n\n\n\n基准测试是数据库工程师必备的技能之一，否则你如何知道自己真的在优化数据库？\n为什么需要基测？基测可以观察系统在不同压力下的行为：\n\n验证基于系统的一些假设是否符合实际情况。\n测试当前的运行情况，如果连这都不知道你如何进行优化？\n模拟比当前系统更高的负载，找出系统的平静。\n测试不同的硬件、软件操作系统配置。\n证明新采购的设备是否正确配置。\n\n基测工具针对MySQL的基准测试工具就有很多了，但是我们推荐使用sysbench（较为简单）和Percona的TPCC-MySQL（面向复杂的场景）。\nSysbech测试案例接下来的本章将着重讲解如何使用sysbench测试MySQL实例。\n下载\nDebian&#x2F;Ubuntu\ncurl -s https://packagecloud.io/install/repositories/akopytov/sysbench/script.deb.sh | sudo bash\nsudo apt -y install sysbench\n\nRHEL&#x2F;Centos\ncurl -s https://packagecloud.io/install/repositories/akopytov/sysbench/script.rpm.sh | sudo bash\nsudo yum -y install sysbench\n\nmacOS\n# Add --with-postgresql if you need PostgreSQL support\nbrew install sysbench\n\n查看脚本sysbench支持更加复杂的测试，同时自身也内嵌了一些关于OLTP系统的测试，我们可以使用find / -name oltp*.lua来查找目录，然后跳转至相应目录查看全部脚本\n\n\n\n\n\n\n\n\n\nOLTP(Online Transaction Processing) 一般指我们\n$ find / -name oltp*.lua  #查找sysbench自带的数据写入脚本的路径,后面执行命令需要用到\n\n/usr/share/sysbench/oltp_delete.lua\n/usr/share/sysbench/oltp_update_non_index.lua\n/usr/share/sysbench/oltp_read_write.lua\n/usr/share/sysbench/oltp_update_index.lua\n/usr/share/sysbench/oltp_read_only.lua\n/usr/share/sysbench/oltp_common.lua\n/usr/share/sysbench/oltp_write_only.lua\n/usr/share/sysbench/oltp_point_select.lua\n/usr/share/sysbench/oltp_insert.lua\n/usr/share/sysbench/tests/include/oltp_legacy/oltp_simple.lua\n/usr/share/sysbench/tests/include/oltp_legacy/oltp.lua\n\n$ ll /usr/share/sysbench/\ntotal 64\n-rwxr-xr-x 1 root root  1452 Apr 25  2020 bulk_insert.lua\n-rw-r--r-- 1 root root 14369 Apr 25  2020 oltp_common.lua\n-rwxr-xr-x 1 root root  1290 Apr 25  2020 oltp_delete.lua\n-rwxr-xr-x 1 root root  2415 Apr 25  2020 oltp_insert.lua\n-rwxr-xr-x 1 root root  1265 Apr 25  2020 oltp_point_select.lua\n-rwxr-xr-x 1 root root  1649 Apr 25  2020 oltp_read_only.lua\n-rwxr-xr-x 1 root root  1824 Apr 25  2020 oltp_read_write.lua\n-rwxr-xr-x 1 root root  1118 Apr 25  2020 oltp_update_index.lua\n-rwxr-xr-x 1 root root  1127 Apr 25  2020 oltp_update_non_index.lua\n-rwxr-xr-x 1 root root  1440 Apr 25  2020 oltp_write_only.lua\n-rwxr-xr-x 1 root root  1919 Apr 25  2020 select_random_points.lua\n-rwxr-xr-x 1 root root  2118 Apr 25  2020 select_random_ranges.lua\ndrwxr-xr-x 4 root root  4096 Nov  4 16:02 tests\n\n\n\n小试牛刀接下来我们将sysbench正式作用于我们的服务器， 笔者准备了一台2C4G的云服务器、MySQL5.7、默认MySQL配置。首先我们需要进入MySQL创建一个测试数据库名为sysbench_test：\n$ mysql -uroot -p\n\nmysql> create database sysbench_test\n\nmysql> quit\n\n\n对于数据库的测试，我们一般会使用到sysbench的oltp脚本， 而其中分为读、写、读写混合等多种场景。\noltp测试步骤基本上分为：准备数据(prepare) - 执行测试(run) - 清理数据(cleanup) 三个步骤。而一般我们只需要用相同的参数运行不同的命令即可。\n准备数据：\nsysbench oltp_read_write --mysql-host=127.0.0.1 --mysql-port=3306 --mysql-db=sysbench_test --mysql-user=root --mysql-password=xxxxx --table_size=100 --tables=10 --threads=20 --report-interval=10 --time=120 prepare\n\n# oltp_read_write 指定sysbench内嵌的测试脚本\n### 准备时参数\n# --mysql-host/db/user/passowrd/port 测试数据库地址/名称/用户名/密码/端口\n# --tables 指定生成的表数量　\n# --table_size 指定每张表表的数据量\n\n### 运行时桉树\n# --thread 指定测试时的线程数\n# --report-interval 指定运行时日志打印间隔\n# --time 指定测试时长\n\n\n执行测试：\n$ sysbench oltp_read_write --mysql-host=127.0.0.1 --mysql-port=3306 --mysql-db=sysbench_test --mysql-user=root --mysql-password=xxx --table_size=100 --tables=10 --threads=20 --report-interval=10 --time=120 run\nsysbench 1.0.20 (using bundled LuaJIT 2.1.0-beta2)\n\nRunning the test with following options:\nNumber of threads: 20\nReport intermediate results every 10 second(s)\nInitializing random number generator from current time\n\n\nInitializing worker threads...\n\nThreads started!\n\n[ 10s ] thds: 20 tps: 298.33 qps: 6897.07 (r/w/o: 4973.02/1270.50/653.55) lat (ms,95%): 161.51 err/s: 54.99 reconn/s: 0.00\n[ 20s ] thds: 20 tps: 325.41 qps: 7474.29 (r/w/o: 5380.34/1384.24/709.72) lat (ms,95%): 147.61 err/s: 58.80 reconn/s: 0.00\n[ 30s ] thds: 20 tps: 289.30 qps: 6699.44 (r/w/o: 4828.16/1236.99/634.29) lat (ms,95%): 164.45 err/s: 55.70 reconn/s: 0.00\n[ 40s ] thds: 20 tps: 326.50 qps: 7444.30 (r/w/o: 5354.60/1380.80/708.90) lat (ms,95%): 150.29 err/s: 55.90 reconn/s: 0.00\n[ 50s ] thds: 20 tps: 326.40 qps: 7480.10 (r/w/o: 5383.30/1385.90/710.90) lat (ms,95%): 150.29 err/s: 58.10 reconn/s: 0.00\n[ 60s ] thds: 20 tps: 338.10 qps: 7669.44 (r/w/o: 5508.03/1429.91/731.50) lat (ms,95%): 147.61 err/s: 55.30 reconn/s: 0.00\n[ 70s ] thds: 20 tps: 320.50 qps: 7305.44 (r/w/o: 5251.46/1358.39/695.59) lat (ms,95%): 153.02 err/s: 54.70 reconn/s: 0.00\n[ 80s ] thds: 20 tps: 361.80 qps: 8212.92 (r/w/o: 5901.02/1528.60/783.30) lat (ms,95%): 137.35 err/s: 59.60 reconn/s: 0.00\n[ 90s ] thds: 20 tps: 345.80 qps: 7903.20 (r/w/o: 5684.10/1467.20/751.90) lat (ms,95%): 139.85 err/s: 60.30 reconn/s: 0.00\n[ 100s ] thds: 20 tps: 351.00 qps: 8032.68 (r/w/o: 5779.16/1489.81/763.71) lat (ms,95%): 134.90 err/s: 61.70 reconn/s: 0.00\n[ 110s ] thds: 20 tps: 348.10 qps: 7964.29 (r/w/o: 5730.19/1476.70/757.40) lat (ms,95%): 137.35 err/s: 61.20 reconn/s: 0.00\n[ 120s ] thds: 20 tps: 343.40 qps: 7899.49 (r/w/o: 5690.09/1459.50/749.90) lat (ms,95%): 134.90 err/s: 63.10 reconn/s: 0.00\nSQL statistics:\n    queries performed:\n        read:                            654696\n        write:                           168754\n        other:                           86531\n        total:                           909981\n    transactions:                        39767  (331.22 per sec.)\n    queries:                             909981 (7579.29 per sec.)\n    ignored errors:                      6997   (58.28 per sec.)\n    reconnects:                          0      (0.00 per sec.)\n\nGeneral statistics:\n    total time:                          120.0600s\n    total number of events:              39767\n\nLatency (ms):\n         min:                                    4.99\n         avg:                                   60.36\n         max:                                  407.99\n         95th percentile:                      147.61\n         sum:                              2400503.17\n\nThreads fairness:\n    events (avg/stddev):           1988.3500/25.31\n    execution time (avg/stddev):   120.0252/0.02\n\n\n重要的参数指标为SQL statistics下的**queries(即QPS)**：达到了7579 per sec.也就是说当前MySQL在10张表100条数据的情况下面对20个线程的读写吞吐量达到了7579QPS。当然这只是在小数据小压力的情况下 ，并不能直接作为线上环境的参考。\n那么接下来让我们清理数据准备下一轮测试\n加大压力刚刚只是尝试一下sysbench的功能，这次我们开始使用较大的压力对MySQL进行读测试（一般对MySQL的读写测试应该分开单独的进行以更好的评估MySQL哪些方面需要提升）。\n\n测试操作：读\n线程数：100\n表大小：100w\n表数量：10\n测试时长：5分钟\n\n对应执行命令：\nsysbench oltp_read_only --mysql-host=127.0.0.1 --mysql-port=3306 --mysql-db=sysbench_test --mysql-user=root --mysql-password=[yourpassowrd] --table_size=1000000 --tables=10 --threads=100 --report-interval=10 --time=300 [command]\n\n测试结果：可以看到我们的MySQL在100个并发访问线程下对于10张表，每张表100w数据进行5分钟的读写测试。能够达到1w的读性能， 还是非常不错的。\n测试写性能现在让我们清理一下测试数据，最后测试一轮写性能\n\n测试操作：写\n线程数：100\n表大小：100w\n表数量：10\n测试时长：3分钟\n\n测试命令：\nsysbench oltp_write_only --mysql-host=127.0.0.1 --mysql-port=3306 --mysql-db=sysbench_test --mysql-user=root --mysql-password=[yourpassowrd] --table_size=1000000 --tables=10 --threads=100 --report-interval=10 --time=300 [command]\n\n\n测试结果：可以看到测试机器上的这个MySQL能够达到9k接近1w的写QPS。\n总结基本测试是我们了解MySQL所要掌握的必备技能之一，而sysbench是一款多线程的性能测试工具。 使用它我们就能够对MySQL的读写QPS有一个非常好的了解。\n","slug":"你真的了解你的MySQL吗-MySQL基准测试","date":"2022-11-12T13:31:33.000Z","categories_index":"","tags_index":"MySQL","author_index":"姚文彬"},{"id":"7685adb7b43e1e02fce5149c97fbe7da","title":"Maven Deamon-更快的Maven","content":"简介Maven Daemon 简称mvnd，使用与Gradle与Takari相同的技巧，为Maven项目提供了更快的构建。 \n\n内置了maven\n通过一个长期的存活的后台线程来完成实际的构建。\n同一个后台线程能够服务于多个项目之间的构建。\n每个mvnd的客户端，使用了GraaleVM构建，占用内存更低，启动速度更快。\n\n安装GitHub - apache&#x2F;maven-mvnd: Apache Maven Daemon\n选择与自身系统相符的压缩包下载，比如说MacOS M1的用户选择darwin-aarch64.zip\n解压后，将其写入系统环境变量\nvim ~/.zshrc\n\n# mvnd\nexport MVND_HOME=/Users/yaowenbin/Applications/apache-mvnd-1.0-m7\nexport PATH=$PATH:$MVND_HOME/bin\n\nsource ~/.zshrc\n\n使用mvnd --help校验是否安装成功\n\nmavend配置\n通过目录我们可以发现，mavend通过内置maven对其进行了一层包装。 所以我们对于mvnd的一些环境配置都可以通过直接更改下面的mvn文件实现，比如说切换阿里镜像源以及更改repository路径：\nvim $MVND_HOME/mvn/conf/setting.xml\n\n\n\n&lt;settings xmlns=\"http://maven.apache.org/SETTINGS/1.2.0\"\n          xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n          xsi:schemaLocation=\"http://maven.apache.org/SETTINGS/1.2.0 https://maven.apache.org/xsd/settings-1.2.0.xsd\">\n\t&lt;localRepository>/Users/yaowenbin/Applications/maven3.8.2/repository&lt;/localRepository>\n\n\t&lt;mirrors>\n\t\t&lt;!-- aliyun -->\n\t\t&lt;mirror>\n\t\t\t&lt;id>aliyunpublic&lt;/id>\n\t\t\t&lt;mirrorOf>*&lt;/mirrorOf>\n\t\t\t&lt;name>阿里云公共仓库&lt;/name>\n\t\t\t&lt;url>https://maven.aliyun.com/repository/public&lt;/url>\n\t\t&lt;/mirror>\n      &lt;mirror>\n\t\t\t&lt;id>aliyuncentral&lt;/id>\n\t\t\t&lt;mirrorOf>*&lt;/mirrorOf>\n\t\t\t&lt;name>阿里云公共仓库&lt;/name>\n\t\t\t&lt;url>https://maven.aliyun.com/repository/central&lt;/url>\n\t\t&lt;/mirror>\n\t&lt;/mirrors>\n&lt;/settings>\n\n集成IDEAIDEA中目前还并未对mvnd有正式的支持，不过好在maven helper插件中，能够直接将mvn指令更换为mvnd简单的实现对于mvnd的支持。\n\n![](Pasted%20image%2020230827225220.png)","slug":"Maven-Deamon-更快的Maven","date":"2023-08-27T14:53:23.000Z","categories_index":"","tags_index":"Build","author_index":"姚文彬"},{"id":"2c4486aa05507c9fe7bc6767ce95c998","title":"kafka-生产者原理","content":"kafka生产者生产者架构\n一个kafka的生产者将包含着\n\nMetaData（关于主题、分区的元数据缓存，会定时的从Broker中拉取）\nPartiioner分区器（用于根据消息的Key来决定其所在分区）\nSerializer序列化器（将从Java的POJO对象通过变为一串用于网络传输的数据）\nInterceptors拦截器（由用户对消息进行前置拦截）\nRecordAccumulator（消息累加器，内部是按照分区分配的一个又一个的队列）\nSender（将消息累加器的消息，通过NetworkClient发送至Broker）。六个核心组件。\n\nkafka是如何管理元数据的？每当kafka的生产者启动、或者是发送消息时拉取到空的MetaData元数据标识，那么就会触发kafka的Sender线程，通过NetworkClient更新MetaData。\n\n生产者如何实现发送kafka生产者中的send()是通过异步的方式执行的：\n\n执行producer启动时加载的interceptors拦截器\n执行Serializer序列化器对消息将进行序列化。\n通过Partioner分区器拉取MetaData元数据，并且选择分区。\n将消息放入RecordAccumulator消息累加器对应分区号的队列中。\n等到队列中的消息达到了发送阈值，那么唤醒Sender线程。\nSender线程创建Reqeust请求。 通过NetworkClient发送给Kafka集群。\n\n为了提高的提升数据传输的效率，kafka能够使用批处理将多条数据放在同一个请求中发送往broker。\n\n\n\n\n\n\n\n\n\n批处理在数据量很小的情况下能够很大程度的提升性能，因为能够将单次批量的网络传输的开销分摊到多条消息上。但是批处理不适用于单条消息数据量大的情况，否则会因为每次请求的数据量太大，影响处理的效率，进而阻塞网络。\n这里的消息发送阈值取自于两个影响kafka生产者批处理行为的参数：batch.size：批次大小，当kafka缓冲池下的单个分区队列达到该大小之后，会触发消息发送。（默认为16KB）linger.ms：停留时间，当kafka队列中的消息停留多久之后会触发消息发送。（默认为0ms，也就意味着kafka每次都会在一条消息写入缓冲区分区队列后立即发送）\n\n\n\n\n\n\n\n\n\nsender线程在发送网络请求的时候，会因为网络I&#x2F;O产生一定的时间开销。 所以即使linger.ms 为0，在同一时间内多次调用生产者的send()API，kafka依然有可能批量的进行消息发送。\n\n生产者核心：内存池与缓冲队列设计RecordAccumulatorkafka的RecordAccumulator是生产者的核心组件，其中使用到了内存池和缓冲队列两个概念来避免大量消息占据大量的堆内存，频繁的引起FullGC问题。\nRecordAccmuluator中有两块区域一个是Batches队列，对于每一个主题的分区下都会有一个队列用于存放当前分区的批次消息。另外一个就是BufferPool内存池了。\n生产者如何实现批处理的？在kafka中有两个关键的参数控制者生产者的批处理能力，分别为batch.size（16384，16KB）控制着每次发送批次的大小限制，而linger.ms（0ms）。控制着每个批次的等待时间。\n每一条消息在调用了Producer的发送API之后，都会进入一个Buffer缓冲区中，而kafka通过一个后台I&#x2F;O线程对Buffer缓冲区的消息进行处理。 当Buffer缓冲区的消息到达了linger.ms的时间或者是batch.size的大小后，一批消息就会发送往broker了。\n\n\n\n\n\n\n\n\n\n即使linger.ms被设置了为0，由于异步传输的特性，kafka依然有可能对于一些消息进行批处理。因为每个send()方法都会先序列化记录、分配分区号、然后才将其放入缓冲区中。\n生产者压缩如果说批处理仅适用于数据体量较小的应用的话，那么消息压缩就适用于所有的场景了。 kafka作为I&#x2F;O密集型的应用，对于CPU的负载率较小， 而压缩恰好是能够使用CPU来换取I&#x2F;O性能。\n所以使用压缩能够很大程度的提升kafka的整体性能。\n为什么不管压缩还是批处理，kafka都要选择producer进行实现？因为生产者是消息出生之后第一个接触到的组件，消息从producer开始就进行压缩，就能够保证从producer到broker再到consumer都能够享受到压缩所带来的性能提升。。\n客户端的 compression.type参数控制着压缩的算法，默认情况下为none也就是不压缩。而kafka目前支持gzip、snappy、lz4和zstd四种压缩算法。\n如何决定采用哪种压缩算法？如果你的CPU较为敏感，但是依然想要通过压缩来提升性能时，那么LZ4算法能够以较低的CPU开销来实现压缩，但相应的压缩率也会有所下降。\n而当你的网络开始成为瓶颈时，那么使用gzip（2.1.0之前）和zstd（2.1.0及之后）将会带来更大的压缩率，使得压缩的性能进一步提升。\nProducer的参数调优acks：对于生产者来说是非常重要的参数，决定了kafka写入性能与消息高可用之间的权衡。对于十分重要的业务，通过-1或者是all设定为写入分区的所有ISR副本才响应成功。对于持久化要求不高、但是对于性能的要求却很高的场景可以将其设置为0，也就是当生产者将消息发送给Broker之后立即响应。\nmax.reqeust.size：生产者能够发送消息的最大大小，默认为1M。对于一些非常多字段的业务对象来说，1M可能会不够用。所以可以根据业务类型适当的调大。\nreqeuest.timeout.ms：生产者等待Broker端最大的响应时间，默认值为30000(30s)，当请求超时之后，生产者就会根据retries和retry.backoff.ms进行重试。 \nbatch.size：缓冲区中每个批次大小，默认为16KB，也就是当分区队列下的消息达到16KB之后就发送。\nlinger.ms：停留时间，当kafka队列中的消息停留多久之后会触发消息发送。（默认为0ms，也就意味着kafka每次都会在一条消息写入缓冲区分区队列后立即发送）。\n两个批处理的参数是或的逻辑关系处理的，在数据体量较小的情况下建议开启批处理，以提升性能。 但是在消息数据体量较大的情况下，反而要关闭批处理，否则由于数据包太大处理时间太长，造成网络阻塞。\nretries：重试次数，默认为0。 这个参数对于at-least-once的语义很重要。kafka在发生了网络都懂、Leader重选等场景下会导致消息的发送失败，根据消息的价值，将其设置为3或者是999是一个理想的选择。\nretry.backoff.ms：每次重试间隔，默认100。根据经典局部性原理，当一个消息发送失败了，那么下次还可能发送失败，所以适当的设置间隔可以避免无效的消息重试\ncompression.type：压缩算法，默认为none，即不压缩。 处于综合吞吐量的考虑，可以设置为lz4算法进行压缩，如果追求高压缩比，则选择zstd\nbuffer.memory：生产者在其JVM中可以使用的缓冲区大小，默认为32M。 可以适当的调大。\nmax.in.fight.reqeust.per.connection ：单个分区下每次能够同时发起多少个请求，默认值为5。 将其设置为1可以有效的避免同一分区下的消息乱序问题。\n","slug":"kafka-生产者原理","date":"2023-08-17T10:26:08.000Z","categories_index":"","tags_index":"kafka","author_index":"姚文彬"},{"id":"20c628775f69ad677f830c91dd71d5bc","title":"RocketMQ-延时队列","content":"RocketMQ如何自定义延时级别？RocketMQ在4.3版本之后，MessageStoreConfig配置类中添加了一个十分有用的属性messageDelayLevel。\nprivate String messageDelayLevel = \"1s 5s 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m 1h 2h\";\n\n使得RocketMQ不再固定的18个延时级别等级，能够通过配置文件让用户自定义任意级别的延时消息。\nmessageDelayLevel &#x3D; 1s 5s 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m 1h 2h 24h 48h\n\nJava SDK如何发送延时任务？在RocketMQ的Java SDK中，通过Message下的setDelayTimeLevel方法就能够指定消息的延时级别。 Broker在接收到带有dayTimeLevel的消息会自动的将其放入延时队列中。\npublic class ScheduledExample &#123;  \n\tpublic static void main(String[] args) throws MQClientException, MQBrokerException, RemotingException, InterruptedException &#123;  \n\t  \n\tDefaultMQProducer producer = new DefaultMQProducer(\"scheduled-producer\");  \n\tproducer.setNamesrvAddr(\"127.0.0.1:9876\");  \n\t  \n\tproducer.start();  \n\t  \n\tMessage message = new Message();  \n\tmessage.setTopic(\"topic\");  \n\tmessage.setBody((\"Scheduled Message\").getBytes(CharsetUtil.UTF_8));  \n\tmessage.setDelayTimeLevel(19);  \n\tSendResult result = producer.send(message);  \n\tSystem.out.println(result);  \n\t  \n\tproducer.shutdown();  \n\t  \n\t&#125;  \n&#125;\n\n\n\n\n\n\n\n\n\n\nRocketMQ的延时消息指定的是消息级别，所以使用者应该需要提前于RocketMQ Broker的运维者沟通好每个级别对应的具体延时时间。以防止设定了预期外的延时时间。\nRocketMQ延时消息的内部细节Broker 处理延时消息在Broker中，BrokerController控制器将会给MessageStore注册一系列PutMessageHook钩子函数，其中就包括了延时消息的处理逻辑：\npublic class BrokerController &#123;\n\tpublic void registerMessageStoreHook() &#123;\n\t\tList&lt;PutMessageHook> putMessageHookList = messageStore.getPutMessageHookList();\n\t\tputMessageHookList.add(new PutMessageHook() &#123;  \n\t\t\t@Override  \n\t\t\tpublic String hookName() &#123;  \n\t\t\t\treturn \"handleScheduleMessage\";  \n\t\t\t&#125;  \n\t\t\t  \n\t\t\t@Override  \n\t\t\tpublic PutMessageResult executeBeforePutMessage(MessageExt msg) &#123;  \n\t\t\t\tif (msg instanceof MessageExtBrokerInner) &#123;  \n\t\t\t\t\treturn HookUtils.handleScheduleMessage(BrokerController.this, (MessageExtBrokerInner) msg);  \n\t\t\t\t&#125;  \n\t\t\t\treturn null;  \n\t\t\t&#125;  \n\t\t&#125;);\n\t\t\n\t&#125;\n&#125;\n\n\n\nclass DefaultMessageStore &#123;\n\n@Override\npublic CompletableFuture&lt;PutMessageResult> asyncPutMessage(MessageExtBrokerInner msg) &#123;\n\n\tfor (PutMessageHook putMessageHook : putMessageHookList) &#123;  \n\t// 延时任务的处理逻辑会以钩子函数的方式注册到DefaultMessageStore中\n\tPutMessageResult handleResult = putMessageHook.executeBeforePutMessage(msg); \n\t\tif (handleResult != null) &#123;  \n\t\t\treturn CompletableFuture.completedFuture(handleResult);  \n\t\t&#125;  \n\t&#125;\n\t\n\tthis.commitLog.asyncPutMessage(msg);\n&#125;\n\n&#125;\n\n\n当Producer发送了一个带有DelayTimeLevel、TIMER_DELIVER_MS、TIMER_DELAY_SEC任意一个参数时，Broker就会将其视为是一条延时消息。\n对于延时消息的处理，RocketMQ5.x版本推出了全新的TimerMessageStore来支持任意精度的延时消息，并且同样支持RocketMQ4.x版本中的延时等级的延时消息，所以对于延时消息的处理会有两种不同的逻辑，分别根据DelayTimeLevel（4.x）和TIMER_DELIVER_MS、TIMER_DELAY_SEC（5.x）进行判断：\n\nif (!isRolledTimerMessage(msg)) &#123;  \n\tif (checkIfTimerMessage(msg)) &#123;  \n\t\tif (!brokerController.getMessageStoreConfig().isTimerWheelEnable()) &#123;  \n\t\t//wheel timer is not enabled, reject the message  \n\t\t\treturn new PutMessageResult(PutMessageStatus.WHEEL_TIMER_NOT_ENABLE, null);  \n\t\t&#125;  \n\t\tPutMessageResult transformRes = transformTimerMessage(brokerController, msg);  \n\t\tif (null != transformRes) &#123;  \n\t\t\treturn transformRes;  \n\t\t&#125;  \n\t&#125;  \n&#125;  \n// Delay Delivery  \nif (msg.getDelayTimeLevel() > 0) &#123;  \n\ttransformDelayLevelMessage(brokerController, msg);  \n&#125;\n\n\n老版本4.x中经典的延时等级的转化会将Message中原本将要投递的Topic和QueueId主题取出来，作为REAL_TOPIC和REAL_QUEUE_ID两个属性存入Message的Properties属性Map中。 然后将msg的topic修改为SCHEDULE_TOPIC_XXXX，queueId修改为delayLevel对应的队列号。\n\nclass HookUtils &#123;\n\tpublic static void transformDelayLevelMessage(BrokerController brokerController, MessageExtBrokerInner msg) &#123;  \n\t...\n\t// Backup real topic, queueId  \n\tMessageAccessor.putProperty(msg, MessageConst.PROPERTY_REAL_TOPIC, msg.getTopic());  \n\tMessageAccessor.putProperty(msg, MessageConst.PROPERTY_REAL_QUEUE_ID, String.valueOf(msg.getQueueId()));  \n\tmsg.setPropertiesString(MessageDecoder.messageProperties2String(msg.getProperties()));  \n\t  \n\tmsg.setTopic(TopicValidator.RMQ_SYS_SCHEDULE_TOPIC);  \n\tmsg.setQueueId(ScheduleMessageService.delayLevel2QueueId(msg.getDelayTimeLevel()));\t  \n&#125;\n\n\n&#125;\n\n\n而5.x的新版本的延时消息使用的是时间轮算法，其中以1s作为一个单元格，划分为了 7 * 24 * 3600（总为7天）个单元格。 transformTimerMessage中根据Message中设定的TIMER_DELAY_MS（延时时间）或者是TIMER_DELAY_SEC的值 计算好该消息的到期时间 然后作为 TIMER_OUT_MS 再次放入Properties中使得随后的TimerMessageStore中能够根据该值放入对应的时间格中。 \ntransformTimerMessage() &#123;\n\tif (msg.getProperty(MessageConst.PROPERTY_TIMER_DELAY_SEC) != null) &#123;  \n\t\tdeliverMs = System.currentTimeMillis() + Long.parseLong(msg.getProperty(MessageConst.PROPERTY_TIMER_DELAY_SEC)) * 1000;  \n\t&#125; else if (msg.getProperty(MessageConst.PROPERTY_TIMER_DELAY_MS) != null) &#123;  \n\t\tdeliverMs = System.currentTimeMillis() + Long.parseLong(msg.getProperty(MessageConst.PROPERTY_TIMER_DELAY_MS));  \n\t&#125; else &#123;  \n\t\tdeliverMs = Long.parseLong(msg.getProperty(MessageConst.PROPERTY_TIMER_DELIVER_MS));  \n\t&#125;\n\n\t// .... 省略一些校验\n\tMessageAccessor.putProperty(msg, MessageConst.PROPERTY_TIMER_OUT_MS, deliverMs + \"\");  \n\tMessageAccessor.putProperty(msg, MessageConst.PROPERTY_REAL_TOPIC, msg.getTopic());  \n\tMessageAccessor.putProperty(msg, MessageConst.PROPERTY_REAL_QUEUE_ID, String.valueOf(msg.getQueueId()));  \n\t\t\tmsg.setPropertiesString(MessageDecoder.messageProperties2String(msg.getProperties()));  \n\tmsg.setTopic(TimerMessageStore.TIMER_TOPIC);  \n\tmsg.setQueueId(0);\n\n&#125;\n\n通过代码可以看到，5.x版本中的Timer是基于TimerMessageStore下的TIMER_TOPIC参数设定的，也就是rmq_sys_wheel_timer。\n当消息的延时属性转换结束之后，消息就会有DefaultMessageStore存入到对应的消息主题下。 接下来就是ScheduleMessageStore和TimerMessageStore两个延时消息服务的处理过程了。\nScheduleMessageService 4.x延时服务ScheduleMessageService中根据MessageStore配置中的delayLevel的等级维护着一个固定数量的线程池deliverExecutorService、一个与延时等级对应的offset偏移量映射表、和一个与延时等级对应的timeMills映射表。\npublic class ScheduleMessageService extends ConfigManager &#123;\n\tprivate final ConcurrentMap&lt;Integer /* level */, Long/* delay timeMillis */> delayLevelTable =  new ConcurrentHashMap&lt;>(32);\n\n\tprivate final Map&lt;Integer /* level */, LinkedBlockingQueue&lt;PutResultProcess>> deliverPendingTable =  \nnew ConcurrentHashMap&lt;>(32);\n\n\tprivate ScheduledExecutorService deliverExecutorService;\n&#125;\n\ndeliverExecutorService在启动时会遍历每一个延时等级的偏移量列表offsetTable，创建一个与之对应的DeliverDelayedMessageTimerTask线程任务，来执行该延时等级的消息拉取任务。\n\nfor (Map.Entry&lt;Integer, Long> entry : this.delayLevelTable.entrySet()) &#123;  \n\tthis.deliverExecutorService.schedule(new DeliverDelayedMessageTimerTask(level, offset), FIRST_DELAY_TIME, TimeUnit.MILLISECONDS);  \n\t&#125;  \n&#125;\n\n每个DeliverDelayedMessageTimerTask投递延迟消息定时任务都是一个每个100ms执行一次的线程\n\n// 在ScheduleMessageService中固定的值\nDELAY_FOR_A_WHILE = 100L\n\nclass DeliverDelayedMessageTimerTask implements Runnable &#123;\n\n\tprivate final int delayLevel;\n\tprivate final long offset;\n\n\t@Override\n\tpublic void run() &#123;\n\t\ttry &#123;\n\t\t\tthis.executeOnTimeUp();\n\t\t&#125;\n\t\tthis.scheduleNextTimerTask(offset, DELAY_FOR_A_WHILE);\t\n\t&#125;\n\n\n&#125;\n\npublic void executeOnTimeUp() &#123;\n\n\t....\n\tthis.scheduleNextTimerTask(nextOffset, DELAY_FOR_A_WHILE);\t\n&#125;\n\n\n\n每个任务中维护着一个自身所所负责的延时等级delayLevel，以及本次任务所应该拉取的消息偏移量offset（这个值取自ConsumeQueue中的逻辑偏移量offset）。\n任务会从根据自身负责的delayLevel获取到从ConsumeQueue中获取SCHEDULE_TOPIC_XXXXX下对应的queue队列。\n试图获取offset偏移量对应的消息索引CqUnit，取出CqUnit中的TagCode（在延时队列中索引的TagCode对应着该索引所指向消息的到期时间戳）。\n如果tagCode的值小于当前系统时间戳，那么就表示当前索引所指向的消息到期了，应该立刻被执行。 根据当前CqUnit索引的偏移量查询出该消息，然后立刻投递。\nclass DeliverDelayedMessageTimerTask &#123;\n\n\tpublic void executeOnTimeUp() &#123;\n\t\n\t...\n\t\n\tlong deliverTimestamp = cqUnit.getTagsCode();\n\tif (deliverTimestamp - now > 0) &#123;\n\t\treturn;\n\t&#125;\n\tMessageExt msgExt = ScheduleMessageService.this.brokerController.getMessageStore().lookMessageByOffset(cqUnit.getPos(), cqUnit.getSize())\n\n\t// 将延时消息重新封装为业务消息（还原Topic、删除delayTimeLevel等属性）\n\tMessageExtBrokerInner msgInner = ScheduleMessageService.this.messageTimeup(msgExt);\n\t\n\tthis.syncDeliver(msg);\n\t\n&#125;\n\n\n\nTimerMessageService 5.x延时服务TimerMessageService 是RocketMQ 5.x版本中 基于时间轮算法实现的延时消息服务， 与 ScheduleMessageService 其同样会从自己所负责的主题 rmq_sys_wheel_timer 中不断的拉取消息， 然后将其存入时间轮中。\nTimerMessageService下内聚了多个服务，为了最大程度的解耦每一个方法\n\ncurrWriteTimeMs的作用是什么？\n\nTimerEnqueueGetService 延时主题的消息拉取服务：核心方法enqueue()，从主题rmq_sys_timer中尝试获取消息，获取到消息后封装一个TimerRequest将消息加入到enqueuePutQueue队列中。\n\nTimerEnqueuePutService 延时主题的消息写入服务：核心方法fetchAndPutTimerRequest()，从enqueuePutQueue队列中批量的尝试获取TimerRequest，然后将request写入时间轮。\n\nTimerDequeueWarnService （暂未使用） ： \n\nTimerDequeuePutService 时间轮到期消息的执行服务 ： \n\nTimerFlushService\n\n\nEnqueue表示将要从主题中提取出来，加入到TimerWhell时间轮\n其中doEnqueue()方法承载着将消息从ConsumeQueue中取出，存入时间轮组件中的核心逻辑：\n\npublic boolean doEnqueue(long offsetPy, int sizePy, long delayedTime, MessageExt messageExt) &#123;  \n\t// 记录时间轮中当前时间戳 System.currentTimes() / precisionMs * precisionMs;\n\tlong tmpWriteTimeMs = currWriteTimeMs;\n\t// 校验延时时长，是否超过了一个时间轮大小 \n\tboolean needRoll = delayedTime - tmpWriteTimeMs >= (long) timerRollWindowSlots * precisionMs;  \nint magic = MAGIC_DEFAULT;  \nif (needRoll) &#123;  \nmagic = magic | MAGIC_ROLL;  \nif (delayedTime - tmpWriteTimeMs - (long) timerRollWindowSlots * precisionMs &lt; (long) timerRollWindowSlots / 3 * precisionMs) &#123;  \n//give enough time to next roll  \ndelayedTime = tmpWriteTimeMs + (long) (timerRollWindowSlots / 2) * precisionMs;  \n&#125; else &#123;  \ndelayedTime = tmpWriteTimeMs + (long) timerRollWindowSlots * precisionMs;  \n&#125;  \n&#125;  \nboolean isDelete = messageExt.getProperty(TIMER_DELETE_UNIQUE_KEY) != null;  \nif (isDelete) &#123;  \nmagic = magic | MAGIC_DELETE;  \n&#125;  \nString realTopic = messageExt.getProperty(MessageConst.PROPERTY_REAL_TOPIC);  \nSlot slot = timerWheel.getSlot(delayedTime);  \nByteBuffer tmpBuffer = timerLogBuffer;  \ntmpBuffer.clear();  \ntmpBuffer.putInt(TimerLog.UNIT_SIZE); //size  \ntmpBuffer.putLong(slot.lastPos); //prev pos  \ntmpBuffer.putInt(magic); //magic  \ntmpBuffer.putLong(tmpWriteTimeMs); //currWriteTime  \ntmpBuffer.putInt((int) (delayedTime - tmpWriteTimeMs)); //delayTime  \ntmpBuffer.putLong(offsetPy); //offset  \ntmpBuffer.putInt(sizePy); //size  \ntmpBuffer.putInt(hashTopicForMetrics(realTopic)); //hashcode of real topic  \ntmpBuffer.putLong(0); //reserved value, just set to 0 now  \nlong ret = timerLog.append(tmpBuffer.array(), 0, TimerLog.UNIT_SIZE);  \nif (-1 != ret) &#123;  \n// If it's a delete message, then slot's total num -1  \n// TODO: check if the delete msg is in the same slot with \"the msg to be deleted\".  \ntimerWheel.putSlot(delayedTime, slot.firstPos == -1 ? ret : slot.firstPos, ret,  \nisDelete ? slot.num - 1 : slot.num + 1, slot.magic);  \naddMetric(messageExt, isDelete ? -1 : 1);  \n&#125;  \nreturn -1 != ret;  \n&#125;\n\n","slug":"RocketMQ-延时队列","date":"2023-08-16T00:21:58.000Z","categories_index":"","tags_index":"MQ","author_index":"姚文彬"},{"id":"dd39451fcd6a79af30692832dae32cef","title":"kafka-存储原理","content":"本章节将讲解Kafka的底层存储机制，如何实现分区副本的分配、索引机制、如何管理文件以及重要的Kafka压缩机制。\n持久化文件系统Kafka重度的依赖于文件系统来存储和缓存消息，即使磁盘通常被认为性能十分缓慢。但实际上，磁盘的读写性能取决于其使用的场景。一个设计良好的磁盘结构能够和网络传输一样快。\n磁盘的顺序写入性能在过去的十年里面已经得到了大幅度的提升，得利于现代OS提供的预读（read-ahead）和后写（write-behind）技术。可以大的多倍的数量预读数据，并且将较小的逻辑写入分组为较大的物理写入。 使得顺序的磁盘访问在某些情况下可能比随机内存访问更快。\nqueue.acm.org&#x2F;detail.cfm?id&#x3D;1563874jacobs3.jpg (585×322) (acm.org)\n为了弥补磁盘和内存的性能差异，现代OS中会尽可能地、激进地将内存用于磁盘的缓存。比如说将32GB上产生高达28GB-30GB的缓存，使得写入和读取的都能够直接基于内存，以提高性能。但任何使用过基于JVM的语言使用者都知道两件事\n\nJava对于内存对象的开销非常高，通常是数据存储的两倍大小（甚至更多）\n随着堆内数据的增加，Java垃圾收集器变得越来越繁琐和缓慢。基于此，对比与使用JVM的堆内存， kafka 更倾向于使用Filesystem文件系统和Linux中的PageCache。\n\nKafka提出了一个简单的设计：与其在内存中维护尽可能多的数据，并且在内存空间快要耗尽的时候将其写回文件系统，不如将其翻转。所有的数据立即写入到文件上的持久日志，而不必写回磁盘。 而在Kafka中这样做仅仅意味着它只需要将数据写入Linux内核中的PageCache中。\n数据结构持久化的磁盘系统通常倾向于使用BTree或者是其变体的数据结构来存储数据，使得随机访问数据的性能能够达到O(LogN) 。  \n但消息队列不同，其数据都是顺序读和顺序写的，Kafka仅仅只需要将数据追加到文件中就能够实现O(1)的读&#x2F;写时间复杂度。 \n分区分配策略为了尽可能的保证高可用和数据容灾的特性，Kafka的分区分配策略会保证三个机制：\n\n如果Broker设置了机架编号，那么，Partition尽可能的分配到不同的机架上\nPartition分区Leader会尽可能的分布在不同的Broker上\nPartition Replica副本会尽可能的分布在不同的Broker上\n\n假设我们有两个机架，4个Broker，4个分区，3个副本，那么Kafka能够帮助我们保证实现如下的均衡分布：![Pasted image 20230810114343.png](Pasted image 20230810114343.png)\n一旦Kafka选定了分区所处的Broker，随后还要决定分区要存储在Broker上的某个数据目录中，因为Kafka支持多数据目录，使得Kafka Broker在磁盘空间不足时，允许增加磁盘来继续存储Kafka的数据。\n而Kafka的数据目录的选择很简单，选择具有最少分区数量的数据目录。 这就意味着当有一个新的文件夹增加时，新的分区总是会分配到该文件目录中，因为其是含有最少分区数量的文件夹。\n文件管理保留时间是一个重要的概念。消息不会永久的保存在Kafka中，因为消息队列的消息特点是即时，在生产者推送消息至Broker后，Broker立即推送给订阅了消息对应主题的Consumer。 \nKafka会根据配置文件中的log.retention.ms/bytes的配置决定在何时删除消息，而不是等待所有的消费者都消费完成之后再删除。\n但是删除消息也是一个技术活，消息都存放在文件中，要删除一条消息或者是一个时间戳以前的消息，每次都从文件中查询到对应的消息再删除的话，效率实在太低了。 所以Kafka的删除策略是基于文件的。\nKafka的消息会存储在log.dir所配置的数据目录的对应主题和分区目录下。![Pasted image 20230810134114.png](Pasted image 20230810134114.png)一个分区下的数据会被分割为多个小文件，每个文件被称之为一个消息段或者是日志段（Log Segments）。每个文件的大小由log.segement.bytes控制，每当文件到达上限时将会关闭对当前文件的写入创建一个新的文件。 每个文件固定20位的长度，表示当前文件所存放的第一条消息的偏移量所决定的。\n有了这样一个基于LogSegement日志段的文件存储方式，Kafka的日志清除策略就简单多了，只要对比每个日志段的时间戳，然后将过期的日志文件删除即可。使得Kafka的删除更加的高效，也能够保证磁盘的利用率不会因为一些过期的消息所占用。\n每个分区中正在被写入的日志段是被视为活跃的，其不会收到log.retention的影响而删除，也就是说，如果你配置了log.retention.ms的时间为一天，但是log.segement.ms为七天。那么日志文件不会在一天之后删除，因为其依然处于活跃的状态。而是在第七天达到log.segement.ms的上限后，关闭该日志文件，而创建新的文件时，该文件才会被删除。\n消息格式磁盘上的存储文件和网络中的数据传输都采用一致的消息格式。每一个消息都包含一个Key、Value、Offset、Size、Checksum（校验消息是否完整）、Magic Byte（消息消息格式）、Compression Codedc（压缩算法）、Timestamp（生产者发送消息的时间）\n同时，如果生产者想要发送一批压缩的消息，那么一批压缩的消息会包装为一个Wrapper的Value。 Wrapper也是一个消息的格式，包含着前面所说的消息的所有格式，一批压缩的消息会被作为Value存入这个Wrapper中，看起来像这样：![Pasted image 20230810140021.png](Pasted image 20230810140021.png)\n日志压缩 Log Compaction除了传统的直接删除的策略，Kafka还支持一种称之为Compact压缩的清除策略。这和我们常说的压缩算法不同，Kafka的压缩将保留主题下每一个Key的最新值。 当你的应用需要存储用户的地址时，保留最新的地址总是比保留七天前的记录要更有意义。\n数据库订阅是日志压缩的一个有用的实际场景。当你的其他应用，比如说搜索引擎，Hadoop，缓存等服务需要订阅来自数据库的记录时，使用日志压缩能够保证数据库的每条记录的最新值被保存下来。\n同时也适用于当应用程序出现故障时，恢复到先前的状态。\n当log.cleanup.polocy参数被配置为compact时，Kafka的清除策略从清除转变为就开始工作了，其会将每个日志段区分为。\n既然每个Key都会被保留一个最新值，那么如何删除无用的Key？ Kafka约定了当Key对应的Value被设置为Null时，Kafka会将其视为是一个墓碑，会在随后清理空间的时候删除。\n日志压缩式通过后台线程来复制Log Segment日志段来实现的，不会阻塞正常日志的读取。同时还支持配置不可超过的I&#x2F;O线程的数量，以防止影响到生产者和消费者线程。\n日志压缩的工作看起来像这样：![Pasted image 20230810221409.png](Pasted image 20230810221409.png)\n日志压缩能够保证以下几点特性使得Kafka正常工作：\n\n主题的min.compaction.lag.ms和max.compaction.lag.ms能够用于配置当前主题下消息被压缩前的最小时间和最大时间。也就意味着只有当消息写入超过了min.compaction.lag.ms时间之后，Kafka将会将其压缩。\n消息的顺序总是不变的，不会因为压缩而导致顺序混乱。\n消息的偏移量总是不变的，他是消息处于某个位置的永久标识符\n当消费者从消息的头部开始读取时，可能会读取到由delete.retention.ms控制的当前主题的日志清除时间的已经被删除的消息。\n\nQuoata限流什么是Quota？Quota叫做配额在英文中是指一个固定的、有限的、被允许的数量。 在RocketMQ和Kafka中意味着限流。\n限流能够保证一些行为异常的客户端的请求得以被限制，防止大量的流量占据了服务器的资源以影响到其他行为正常的客户端。\nKafka的中的限流主要从两个维度上进行度量的：\n\n网络带宽，也就是通过X bytes &#x2F; s来限制每个客户端的流量\n请求率限流，请求率被定义为一个客户端的请求能够占用Broker端上的I&#x2F;O线程和网络线程的时间百分比。n % 的配额表示一个线程的上的百分比，而\n\n限流是基于什么维度的？(user, client-id)是kafka中限流的基本维度，user是kafka集群中一组授权的client。而client-id是一组逻辑上相同意义的client，通常这意味着一组相同的应用。\nkafka对于配额的控制是按照细粒度到粗粒度的优先顺序进行加载的：\n\n&#x2F;config&#x2F;users&#x2F;&lt;user&gt;&#x2F;clients&#x2F;&lt;client-id&gt;\n&#x2F;config&#x2F;users&#x2F;&lt;user&gt;&#x2F;clients&#x2F;&lt;default&gt;\n&#x2F;config&#x2F;users&#x2F;&lt;user&gt;\n&#x2F;config&#x2F;users&#x2F;&lt;default&gt;&#x2F;clients&#x2F;&lt;client-id&gt;\n&#x2F;config&#x2F;users&#x2F;&lt;default&gt;&#x2F;clients&#x2F;&lt;default&gt;\n&#x2F;config&#x2F;users&#x2F;&lt;default&gt;\n&#x2F;config&#x2F;clients&#x2F;&lt;client-id&gt;\n&#x2F;config&#x2F;clients&#x2F;&lt;default&gt;\n\n限流是单节点还是集群的？kafka采取了单个broker节点单独限流的方案，而不是基于整个集群进行统一的限流，因为想要实现分布式集群下的统一配额必然要消耗大量的资源，得不偿失。所以基于单独的配额是一个更具性价比，同时在功能的实现上偏差却不太大的优秀方案。\n限流的具体实现是如何的？两种限流的方式：基于数据传输大小 和 基于线程数量的配额都是在一个30个1s的较小测量窗口中进行的。 使得kafka能够及时的检测和纠正限流的情况。 \n如果将测量窗口设置的太大，会导致大量的流量突发，然后产生较大的delay，这种情况下用户的体验就很差了。\n限流触发后，broker和client还会正常工作吗？当一个客户端触发了限流之后，kafka的broker会为其计算一个delay的延迟并且立即响应该延迟。\n随后，在延迟时间为到达之前，broker对于该客户端的每次fetch请求都不会响应任何数据。\n而当kafka的客户端在将接收到broker的delay延迟响应之后，也不会再向broker端发送任何请求。\n这种双向限流的方式使得kafka能够在一些旧版本的broker或者是client中也能够轻松的实现限流。\n日志副本Replica日志副本是Kafka架构中的核心， 也是kafka能够保证高可用和持久化的根本。 使得Broker节点集群在发生故障的时候，仍然保持集群的高可用，也是的Kafka Broker节点的数据发生损坏时，依然能够恢复。\n如何设定日志副本kafka中的日志副本是基于主题来设置，基于主题的分区作为副本的基本单元。\n默认情况下每个分区都只会有一个副本，在创建主题时，你可以通过–replication-factor &lt;N&gt;来指定主题的副本数量\nbin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 3 --topic my-topic\n\n\n设定多个副本之后，kafka会为每一个分区都产生多个副本，并且均衡的分布在kafka集群中。每个副本中的日志具有完全相同的消息和偏移量。\n多副本之间的如何工作？每个分区的Replica都会有一个Leader Replica，消息的生产和消费都有Leader Replica所在的节点来处理。\n而分区中的其他Replica，都为Follower Replica，其并不会处理来自客户端的请求，其日常任务就是不断的向Leader Replica中发送同步消息的请求。 但是一旦Leader Replica的节点出现了故障，那么Follower Replica将会选举出一个新的Leader Replica，并且接管原本Leader Replica的任务。\nFollower Replica 是如何工作的？Follower 会不断的向 Leader 节点发送 Fetch 请求，请求就如同消费客户端请求类型一样，会顺序的获取Leader节点上的数据，并且包含了Follower下一个想要获取的消息的偏移量。\nFollower 的每一个 Fetch 请求都是同步进行的，先请求消息1，再请求消息2，等待消息2相应成功之后才会请求消息3。Leader 节点对于来自 Follower 的 Fetch 请求时，还会告知Follower 其现在落后的进度。\n什么是ISR？每一个 Leader Replica 都会维护一个ISR（In-Sync Replica）集合，其包含着该分区的当前可用的副本集合。 默认情况下ISR中存放着每一个 Follower Replica，但是当一个 Follower 被认为是挂掉的时候，就会从ISR中剔除。\n剔除ISR副本是按照什么规则来的？kafka中 Leader Replica 剔除ISR中的节点的规则有两个：\n\n当 Leader 节点超过 10s 未收到 一个Follower 的Fetch请求时。（kraft模式通过broker.session.timeout.ms配置，zookeeper模式通过zookeeper.session.timeout.ms设置）\n当 Follower 节点的同步进度落后于 Leader  节点超过10s时（通过replica.lag.time.max.ms参数所配置）。\n\n什么是Preferred Leader？kafka中将负载均衡做到了极致，每一个Partition都会有有一个Preferred Leader Replica节点，随着Partition的创建而设定，来自多个分区的 Preferred Leader 其会均匀的分布在Broker节点中。 \n默认情况下，kafka 节点还会检查每个分区中 Current Leader 是否是Preferred Leader，如果不是的话，会尝试将 Leader 转移到 Preferred Leader节点中。（auto.leader.reblance.enable &#x3D; true）\n副本对于生产者的影响通过生产者的配置，kafka的生产者可以选择当消息被发送至kafka broker节点中的0个或满足quorum或所有节点时才响应发送成功。 \n这意味着你需要对消息基于延迟和持久性的进行权衡。\n副本对消费者的影响kafka中只有消息被存入其对应分区下所有的ISR中才会被视为提交。 只有提交后的消息才能够被消费者，这也就意味着每当消费者接收到一条消息之后，能够保证这个消息是正确的被投递的，而不会产生回滚之类的可能。\n为什么kafka要选择ISR，而不是Raft协议或者是ZK？目前许多分布式系统都会采用Raft或者是ZK的ZAB算法来实现副本同步，其核心的策略就是当你定义了2n\nRaft协议中在提交日志或者是Leader选举中都是用了“多数票”的方法。 假设有 2n + 1 个副本，仅当 有 n + 1个副本同步了Leader节点的数据时，Leader节点的日志才会被视为时提交的。 这样我们能够保证在所有 n + 1的副本中，至少有一个副本包含所有已提交的消息。 该副本的日志将是最完整的，能够被选举为新的Leader。\nRaft协议基于quorum的多数投票方法的一个优点就是总是能够根据响应最快的几个节点来响应同步请求或者是选举请求。 一些较慢的服务器不会拖延整个集群的后腿。\nRaft协议有什么缺陷？这就意味着每部署 2 n + 1 个副本，Raft协议仅能够容忍n &#x2F; 2个故障。 3个节点只能够允许1节点在一个时刻发生故障，而5个节点只能够允许2个节点。 这样的部署成本就比较大，想要多增加n个节点的故障能力就需要增加2n个副本数量。这就是为什么quorum算法更常用于共享集群配置比如ZK，而不是重要数据存储的原因。 因为quorum实现的重要数据存储更加的昂贵。\nISR是如何解决Raft缺陷的？kafka的实现与quorum投票的方式稍有不同，kafka内部动态地维护一组同步副本ISR。每个ISR中的节点都够保证跟上leader的步伐。这也就意味着，当故障发生时，kafka的ISR中的存活节点能够被直接选举为一个可用的Leader，而不必担心候选者的日志是否与达到最新的状态，与故障Leader一致。\n这种模型下，n + 1的副本能够保证n的容忍故障，而不会有日志丢失的风险。\n当然，这种更优的部署是在性能的权衡下保证的。在日志的提交中 quorum 仅需要 n &#x2F; 2个节点响应同步请求即可，而 ISR 需要集合中的所有节点都响应\n什么是最小同步副本？Controller 集群控制器什么是控制器？kakfa集群中，第一个启动的节点会在Zookeeper中注册一个ephemeral临时节点，名为&#x2F;controller。其他节点在启动时也会执行同样的操作，但是当发现&#x2F;controller已存在时，触发一个异常，使其意识到当前集群中controller控制器已经存在了。\ncontroller会通过Zookeeper来注册一个Zookeeper Watch观察者，用于监听集群中每一个节点中的状态。  同时为了防止“脑裂”，控制器会使用一个 epoch （任期或者说纪元）来标明当前controller。\n当一个节点下线时会发生什么？当一个节点发生下线时， controller会意识到该节点上的Leader Replica，需要一个新的leader了。其会遍历该分区的ISR的下一个节点，将其作为新的Leader，并且通知 ISR 中的其他节点，当前分区的Leader已经易主了， 该通知请求中包含了新的Leader 和 新的Follower 们。\n新的 Leader 在接收到请求之后，就意识到自身变为了新的 Leader 便开始接受来自 Consumer 和 Producer 的请求了。\n当节点重新上线又会发生什么？当一个节点重新上线后，Controller 控制器会通过配置文件中的 brokerid 来检验当前节点是否存在副本，如果存在副本的话，会通知副本分区中的所有节点，有曾经下线的节点重新上线了，随后该节点会开始向 Leader 发送日志同步请求。\n消息幂等性","slug":"kafka-存储原理","date":"2023-08-14T14:00:17.000Z","categories_index":"","tags_index":"MQ,Raft,Rate Limiting","author_index":"姚文彬"},{"id":"d709ff68faafebbf1f49a2d66baac1a6","title":"kafka-请求处理","content":"kafka中broker与client的交互都是通过TCP协议来实现的，通过固定的请求格式和请求响应。\n任何请求都能够按照顺序被响应，使得 kafka 能够向队列一样保证消息的存储数据。\n请求头由什么构成？每个请求头都包含四个固定的参数：\n\nRequest type，请求类型，也叫做API\nRequest version，请求版本，使得broker能够针对不同的请求版本产生不同的行为。\nCorrelation ID，关联ID，一个唯一的请求标识符，同时在响应或者是错误日志也会带上关联ID，使得kafka能够根据关联ID快速找到一个请求，进行问题排查。\nClient ID，客户端ID，客户端的标识符。\n\n请求交互kafka的请求处理是基于主从Reactor模式的。 每一个Broker的。都会有一个统一的Acceptor线程用于建立请求链接，随后将其传递给request类型所对应的processor处理。 processor对应的线程也被称之为network thread 网络I&#x2F;O线程。 默认数量为3是可配置的（num.network.threads）。\n网络线程并不参与实际请求的处理，而只是将其放入request queue中，随后从response queue中将响应取出返回给客户端。交互模型看起来像这样：![Pasted image 20230814214234.png](Pasted image 20230814214234.png)\nTODO：kafka源代码中的处理模型是什么样的？ 是否是像RocketMQ那样，每一个Request都有当度配置的线程池？\n请求类型绝大多数请求都有来自Producer的Produce Request和来自Consumer的Fetch Request组成。 另外还有一类型的请求用于获取kafka集群当前元数据的被称之为Meta Request。\n每当一个client连接上kafka集群后，会向集群中的任意节点发送一个meta request，以获取client感兴趣的topic主题列表，以及topic下的分区Leader存在于哪一个节点。\n当kafka发送给错误的Leader会发生什么？当出现分区副本的Leader节点易主时，client（不管是producer还是consumer）向原leader发送的请求会被响应一个”Not a Leader for Partition”的请求。 当client收到该响应时，会立即的发送一个MetaRequest来更新kafka集群中的Metadata元数据。\nProduce Request 生产请求当一个生产者将消息发送到所在分区Leader的Broker节点后，Broker会执行一些校验：\n\nProducer是否有对该主题的写入权限？\nProducer中配置的ack参数是否有效？\n当ack参数为all时，当前分区的ISR副本中是否能够满足安全的写入？\n\n随后消息会被写入到本地磁盘中，在Linux系统上，会通过mmap机制写入一个名为Page Cache的内存映射区域中。 随后如果ack的参数为0或者1，那么该请求将会直接响应给Producer， 而如果为-1（all）那么响应将进入一个名为purgatory的Buffer缓冲中，直至ISR同步完成之后再返回。\n","slug":"kafka-请求处理","date":"2023-08-14T13:59:14.000Z","categories_index":"","tags_index":"MQ","author_index":"姚文彬"},{"id":"8d37cbd2fbc0c698ac8b858553ced3c3","title":"Kafka搭建、基本配置和调优","content":"Apache Kafka Quick Start\nKafka安装1. 下载资源包Apache Kafka Downloads 下载Binary资源包\nApache ZooKeeper 下载Binary资源包（不带Source Release的那个）\n\n\n\n\n\n\n\n\n\nApache项目发布的时候基本都会有Binary和Source两个版本， Binary是编译好的能够直接使用的，Source是未编译的源代码。 为了更快地时候，我们一般会直接下载Binary二进制包\n2. 启动ZookeeperZooKeeper: Because Coordinating Distributed Systems is a Zoo (apache.org)添加配置文件：注意这个后缀是cfg\nvim conf&#x2F;zoo.cfg\n\ntickTime&#x3D;200\ndataDir&#x3D;&#x2F;var&#x2F;lib&#x2F;zookeeper\nclientPort&#x3D;2181\n\n启动zookeeper\nbin/zkServer.sh start\n\n如果Macos上运行Zookeeper出现了问题，可以参考一下文章：解决Mac上安装Zookeeper问题：FAILED TO WRITE PID - 夜深nps - 博客园 (cnblogs.com)\n3. 启动Kafka进入Kafka应用目录\n# 后台运行\nbin/kafka-server-start.sh -daemon config/server.properties\n# 查看日志\ntail -f $KAFKA_HOME/logs/zookeeper.out\n\nKafka服务会默认占用9092端口\n4. 创建主题测试收发使用kafka-topics.sh命令行工具创建主题\n➜ bin/kafka-topics.sh --create --topic quickstart-events --bootstrap-server localhost:9092\nCreated topic quickstart-events.\n\n➜ bin/kafka-topics.sh --describe --topic quickstart-events --bootstrap-server localhost:9092\nTopic: quickstart-events\tTopicId: 9wroEws9SkiToPXeGV9sDg\tPartitionCount: 1\tReplicationFactor: 1\tConfigs:\n\tTopic: quickstart-events\tPartition: 0\tLeader: 0\tReplicas: 0\tIsr: 0\n\n可以看出kafka-topics.sh工具的命令行格式为\nkafka-topics.sh --&lt;operatetion> --&lt;topicName> --boostrap-server &lt;kafkaServerHost>\n\n\n使用控制台工具测试收发：\n\n➜ bin&#x2F;kafka-console-producer.sh --topic quickstart-events --bootstrap-server localhost:9092\n&gt;first event\n&gt;second event\n&gt;^C%\n\n\n➜ bin&#x2F;kafka-console-consumer.sh --topic quickstart-events --from-beginning --bootstrap-server localhost:9092\nfirst event\nsecond event\n\n安装Kafdrop 运维面板如果你想通过Web UI的方式来对Kafka进行运维以减少使用Kafka运维工具的记忆存储的压力。那么obsidiandynamics&#x2F;kafdrop: Kafka Web UI (github.com)会是一个不错的选择其支持Jar和Docker的运行方式，只需要简单的配置即可运行。\nDocker运行为了更快的部署Kafdrop，我们可以通过Docker快速构建：\ndocker run -d --rm -p 9000:9000 \\\n    -e KAFKA_BROKERCONNECT&#x3D;host.docker.internal:9092 \\\n    -e JVM_OPTS&#x3D;&quot;-Xms32M -Xmx64M&quot; \\\n    -e SERVER_SERVLET_CONTEXTPATH&#x3D;&quot;&#x2F;&quot; \\\n    obsidiandynamics&#x2F;kafdrop\n\n\n\n\n\n\n\n\n\n\n你可以通过host.docker.internal地址将主机的IP地址映射至docker容器中\n随后Kafdrop会运行在9000端口，我们可以在浏览器中访问，它看起来像这样：![Pasted image 20230809173831.png](Pasted image 20230809173831.png)\n基本配置Brokerbroekr.id 任何broker都需要指定一个集群内部唯一的、整型的标识符。 默认配置下这个值为0。\nport Kafka服务器占用端口，默认为9092。这个值可以被设置为任意可用的端口，但是如果低于1024，Kafka必须通过root身份进行启动，这并不是一个推荐的配置方式。\nzookeeper.connect连接ZK的地址，默认值为2181，可以通过hostname:port&#x2F;path的格式来着指定，如果不指定path则默认为&#x2F;根目录。如果你的ZK集群除了Kafka还被其他应用所使用，那么指定&#x2F;path能够保证数据不会冲突。\nlog.dirs  Kafka中Broker数据（包括消息、偏移量等）的存储位置，默认为&#x2F;tmp&#x2F;kafka-logs。可以通过,逗号分割多个文件路径，而Broker会将新的分区方在多个路径中最少存储分区的那个路径下，并且能够保证同一个分区的数据总是会放入同一个文件路径。\nnum.recovery.threads.per.data.dir Kafka中用于打开每一个目录下的分区日志文件的线程数量。\n\n当Kafka正常启动时，用于打开每个线程的日志文件\n\n当Kafka启动失败时，用于检查和阶段日志文件\n\n当Kafka关闭时，用于关闭日志文件。这个配置仅用于Kafka刚启动和关闭阶段用于文件的打开和关闭，所以理论上将其配置的更大一点以保证多个线程并行操作是合理的。当从异常关闭的Broker中进行回复时，这意味重新启动Broker可能需要较长的时间。线程数量是对于一个文件路径而言的，如果设置了3个文件存储路径，并且将这个线程数来那个设置为8，那么Kafka总共会启动24个线程。\n\nauto.create.topics.enable当任何客户端访问某一个不存在的主题时，是否允许Broker自动创建主题。  这个配置会可能会引发非预期的行为，所以尽可能的保证这个配置项是关闭的。\n\n\nTopic默认配置项Kafka的服务器（Broker）配置还能够为主题的创建指定许多默认的配置。其中包括一些较为重要的分区数和消息保留。 服务器应该将其中的配置项设置为基线值，以适应大多数主题。\n\nnum.partitions分区数量，默认值为1。主题的分区数量可以通过管理工具进行增加，但是注意，分区数量不能减少。 如果想要手动创建比默认设置更少的分区数，那么可以在通过kafka-topics工具手动指定分区数量。\n\n分区数量是Kafka集群能够横向扩展的具体已实现，使用更多分区数量能够在集群中新增更多的的Broker时进行负载均衡。\n尽可能的保证Partition分区数量等于或者是倍数于Broker的数量。使得Partition分区能够均衡的负载在每一个Broker上。 这并不是一个必须的选项，你也可以通过创建多个Topic来平衡消息的负载。\n分区数量的设置是Kafka使用者面临的一个关键点，以下是影响分区数量的选择的几个重要因素：\n\n你期望Topic达到多少的吞吐量？ 100KB&#x2F;s还是1GB&#x2F;s？\n你期望消费单个分区的最大吞吐量是多少？你总是至少一个消费者从分区中读取数据。 如果你的消费者消费速度较慢，比如说需要将数据写入DB，那么DB的写入速度限制这你的消费者速度。（也就是说要考虑到单个分区的吞吐量是否和消费者的消费速度相匹配）。\n同样，你期望生产者的最大吞吐量是多少？通常情况下生产者的生产速度会比消费者要快，\n如果你需要使用Keys来指定消息存放的分区，那么在创建主题的时候就基于未来的使用率，而不是当前的使用率。（这样就能够避免未来需要增加分区数量，进而导致同样Key的新消息和旧消息不再存放于同一个分区上。）\n考虑到磁盘的可用空间和网络的带宽。\n要避免过度高估分区的数量，因为每个分区都会占用Broker上的资源，以及增加Leader的选举时间。\n\n\nlog.retention.ms/hours/minutes  这是三个参数，都表示着日志的过期时间，默认情况下这个参数为168hours，也就是7天。  如果三个参数都指定了，那么kafka只会保证最小的时间单位的配置项生效，所以推荐使用ms作为配置。\n  过期时间是根据消息的最后修改时间决定的，正常情况下这就是消息被消费的时间。但是当使用管理工具修改了分区数量，可能会导致分区消息的最后修改时间变更，进而产生非预期的行为。\n\nlog.retention.bytes  一个分区中的日志保留的大小，当分区下的日志文件超过这个大小的时候，超过这个大小的日志文件会被清除。 注意这是一个分区中的大小，意味着一个主题的日志的大小取决于这个值 * 分区数量，并且也会随着分区数量的增加而增加。\n  log.rentention.bytes和ms 基于时间和大小的两个策略是“或”的逻辑运算。 也就是当日志文件只要超过了时间点或者是大小，那么其就会被清除。\n\nlog.segement.bytes  分区中一个日志文件（日志段这个概念可能会有些不好理解）的大小，默认值为1GB。一个日志文件包含着当前分区中的一段连续的日志，当达到文件大小上限之后关闭，创建出一个新的日志文件。\n  减少日志文件的大小，意味着文件会更加频繁的关闭和创建，会减少整个磁盘的写入效率。 但是当主题有着较低的生产效率时，减少日志的文件的大小，却变得十分重要，因为log.retention的两个策略仅对于关闭的日志文件生效。\n  假设一个文件的写入效率为100M每天，而log.retention.ms使用了默认值七天，这意味着消息文件在达到第7天的时候，会由于日志还未关闭，进而无法按预期被删除。而仅仅在第十天消息日志文件中的日志达到了上限，进而关闭该文件，随后打开新的日志文件时。这个本应该过期的日志文件才会被删除。\n\nlog.segement.ms  由时间控制的日志文件的关闭策略，同retention一样，两者策略之间是“或”的逻辑运算。\n  使用log.segement.ms时，如果多个分区的日志文件总是由于时间问题被同时关闭和创建，那么可能会产生磁盘的突刺，从而影响磁盘的性能。\n\nmessage.max.bytes  消息的最大字节数，默认值为1MB，当生产者发送超过了这个大小的消息时，Broker会返回一个错误。  注意这个参数需要和消费者客户端的fetch.message.max.bytes以及Broker端的replica.fetch.max.bytes配置保持一致。否则，会导致消息的拉取无法获得完整的消息数据。\n\n\n硬件选择一个应用绕不开底层硬件的依赖，而影响应用性能的核心无外乎硬盘、内存、网络和CPU四个核心硬件。\n磁盘生产者的性能会直接的影响到Broker的磁盘吞吐量，因为只有当Broker将消息写入磁盘时，一个消息才能够被视为真正被生产出来。而生产者客户端会等待直至消息成功写入，才会响应成功。这意味着更快的磁盘写入能够有更低的生产者写入延迟。\n这就是来到了经典的SSD和HHD的抉择。即使Kafka对于消息的写入是顺序写的，能够很大的利用磁盘的特性。但使用SSD磁盘总是要比HHD的性能更好；如果Kafka对于存储的容量有要求（比如说一天流量为1TB，而消息过期策略为7天，那么至少磁盘要有7TB的空间）。那么采用机械硬盘是更加经济的选择。\n磁盘还和Broker中的Retention过期配置相关，你想要保留的日志时间越长、日志体积越大，那么你就应该需要越多的磁盘空间。\n内存内存影响着消费者的消费速度，正常情况下生产者的消息追加到Broker之后会立即的被消费者消费，生产者和消费者之间的消息进度差异十分的小。 这种情况下，Broker会将消息存储在系统的PageCache页缓存中，使得消费者的读取速度比Broker从磁盘中重新读取消息更快。 因此提供更多的内存空间用于页缓存将提高客户端的性能。\n需要注意的是操作系统的PageCache页缓存会在多个应用程序之间共享，所以不推荐的将Kafka与其他的重要应用部署在同一台机器上。\n网络每当一个消息从生产者中通过网络入站连接至Broker写入数据， 可能会产生N个消费者的出站连接用于消息的消费。 一个生产者可能产生1MB的数入站据每秒，那么消费者可能消费nMB的出战数据每秒。\n同样为了保证Kafka数据的高可用，其他Kafka应用级别的操作也会占用网络带宽，比如说集群复制。当网络的带宽被生产和消费打满时，集群复制的滞后会变成一个十分常见的问题。\n\n所以每当发生集群复制滞后时，你就需要检查是否网络带宽一直处于占满的状态。\n\nCPUCPU在Kafka中仅用于消息的压缩，理想情况下，生产者和消费者今天当消息压缩后从发送至Broker能够减少磁盘和网络的占用。 但是为了校验消息中的checksum校验和以及分配消息的offset偏移量，Kafka需要在接收到来自生产中的压缩消息后，将其解压出来进行操作。校验无误后，然后再压缩存入磁盘。 这是Kafka中对于CPU有所要求的地方。\n但是CPU不应该是Kafka选择硬件的主要因素，优先考虑磁盘和内存。\nOS调优Linux的众多发行版中都有能够对内核参数进行配置。 默认配置会对绝大多数的应用都工作的很好，但是依然有一小部分的配置项更改能够使得kafka broker具有更好的性能。\n虚拟内存虚拟内存优化的核心点在于避免内存交换(memory swapping)的成本。kafka是一个重度依赖于虚拟内存和页缓存的应用，如果虚拟内存页交换到磁盘，那么将会显著的影响到Kafka的性能。交换空间能够防止OS由于内存不足而突然禁止进程。\n关于虚拟内存的优秀文章：Linux虚拟内存（swap）调优篇-“swappiness”,“vm.dirty_background_ratio”和“vm.dirty_ratio” - 尹正杰 - 博客园 (cnblogs.com)\n使用虚拟内存会导致数据频繁的从内存写入磁盘，导致性能的下降。而通过配置项vm.swappiness = 1 能够使得操作系统尽可能的不使用交换内存，而是使用真实物理内存。查看当前配置\ncat /proc/sys/vm/swappiness\n临时修改配置\nsysctl vw.swappiness=1\n永久修改配置\necho &quot;vm.swappiness&#x3D;1&quot; &gt;&gt; &#x2F;etc&#x2F;sysctl.conf\n激活设置\nsysctl -p\n\n脏页Dirty Page脏页是Linux内核中的概念，因为磁盘中的读写速度远赶不上内存的速度，所以系统可以将读写频繁的数据先放到内存中，这就叫高速缓存。而Linux中页是高速缓存的单位，当进程应用修改了高速缓存中的页数据时，该内存就会被标记为脏页（因为内存中的数据和磁盘中不一致），直至合适的时候将脏页的数据写入磁盘，以保持高速缓存中的数据和磁盘的一致。\nLinux中提供了两个参数用于调整脏页的处理方式\n\nvm.dirty_background_ratio表示当前脏页所占内存比例达到多少时，Linux会启动后台会写线程，来将脏页异步的写回磁盘。 这个参数的默认值为10，而将其设置为5能够适用于绝大部分应用；\nvm.dirty_ratio 表示当脏页占内存比例达到多少时，Linux会阻塞直到脏页被写回磁盘。这个参数的默认值为20，但是我们可以将其设置为更大的值，60~80是一个合理的选择，但是这会带来一些风险，包括未刷新磁盘的数量和同步刷新引起的长时间I&#x2F;O等待。 如果设置了较高的值，建议开启Kafka的复制功能，以防止系统崩溃造成数据丢失。\n\n设置这两个参数前，你可以先观察当前操作系统的脏页数量，以调优至一个合理的值\ncat /proc/vmstat | egrep \"dirty|writeback\"\n\n\n磁盘文件系统除了磁盘硬件的选择，Linux上的文件系统也影响着进程的性能。 目前有许多文件系统可供选择，最常见的两个就是EXT4（Fourth Extended File System）和XFS（Extends File System）。 XFS已经变成许多Linux发行版的默认文件系统了，因为其更适用于绝大多数的场景。\n而EXT4在经过一些参数的调整之后能够表现得更好。\n还有一个Linux参数会影响着磁盘的效率，那就是开启noatime选项，在文件系统的元数据中包含了三个时间戳：创建时间（ctime）、修改时间（mtime）和访问时间（atime）。前两者仅在文件被创建和修改的时候所变更，但接触时间不一样，每一次对文件的访问，不论是否修改了数据都会导致atime的变更。而atime的实际作用却并不大，很少有场景会使用它。所以禁用atime是一个明智的选择。\n网络Networking调整Linux的默认网络参数是一个使得任何应用程序产生更高网络流量的常见手段。因为默认的Linux参数并不是为了大体量、高速数据传输而设置的。在这一步的设置除了kafka，也是由于大多数的Web应用程序。\n\nnet.core.wmem_default和net.core.rmem_default控制着套接字发送和接收缓冲区大小，建议设置为131072（128K）是一个较为合理的数值。\n\nnet.core.wmem_max和net.core.rmem_max控制着套接字发送和接收缓冲区的最大大小，设置为2097152（2MB）是一个较为合理的数值。\n\n另外要控制TCP套接字的缓存区大小还需要额外通过ipv4.tcp_w/rmem来进行配置，其接受由空格隔开的由三个数值。建议将其设置为4096 65536 2048000其意味着缓冲区的最小、默认和最大大小为4KB、64KB和2MB。 注意这个参数不应该超过net.core.w/rmem_max所设定的值。\n\n还有一个十分有用的TCP调优参数net.ipv4.tcp_window_scaling=1。 能够使得客户端数据传输得更加有效率。允许数据在Broker端传输。\n\n\n另外设置tcp_max_syn_backlog大于默认的1024能够使得LInux接受更大数量的并发连接。net.core.netdev_max_max_backlog大于默认的1000能够应对网络流量的突发，允许更多的流量包在内核排队等待处理。 \nG1 JVM垃圾收集器G1垃圾收集器已经在JDK11之后的多个版本中被设置为了默认垃圾收集器，并且在生产环境中得到了稳定的保证。\n在G1垃圾收集器中有两个参数影响着其垃圾收集的性能，分别为\n\nMaxGCPausMills：预期的最大GC停顿时间，G1能够保证尽可能的使得每次垃圾回收都接近这个数值，默认为200。而Kafka对于堆内存的使用十分的高效，所以我们能够将其设置在20，以保证GC停顿不影响到Kafka正常的工作。\nInitiatingHeapOccupancyPercent：初始GC时内存占用的百分比，默认值为45，也意味着只有当JVM的堆内存占用达到45%的时候，G1收集器才会开始第一次GC。而我们可以将其设置得更低一点，比如说35。\n\n但Kafka中默认使用的JDK8中，使用的是CMS + Parnew。而G1垃圾收集器会有有着比CMS更高的内存占用，所以在切换Kafka的垃圾收集器前，先确保Kafka的堆内存至少能够分配4GB的大小，否则或许使用CMS能够以更低的内存开销获得更高的性能。\n要想使得Kafka更换垃圾收集器，我们可以设置环境变量KAFKA_JVM_PERFORMANCE_OPTS\n# export KAFKA_JVM_PERFORMANCE_OPTS&#x3D;&quot;-server -XX:+UseG1GC -XX:MaxGCPauseMills&#x3D;20 -XX:InitiatingHeapOccupancyPercent&#x3D;35 -XX:+DisableExplicitGC -Djava.awt.headless&#x3D;true&quot;\n\n# bin&#x2F;kafka-server-start.sh -daemon config&#x2F;server.properties\n\n\nZookeeperKafka使用Zookeeper来存储自身的元数据包括Brokers、Topics和Partitions。 Kafka仅当集群自身发生调整时或者是Consumer Group的关系发生了变化时会对Zookeeper进行写入。 所以Kafka对于Zookeeper的流量占用是较小的。\n所以我们能够使用一个Zookeeper集群来治理多个Kafka集群，并且通过Zookeeper的chroot path来隔离每一个Kafka集群中的数据。\n前面提到了Consumer Group会对Kafka造成写入，Consumer消费者能够配置使用Zookeeper或者是Kafka来提交自身的消费便宜来那个。如果选择了Zookeeper，那么每一个Consumer会对他所消费的分区的偏移量对Zookeeper进行一次写入操作。这会对Zookeeper产生相当大的流量。 所以推荐的做法是使用Kafka Broker本身来提交偏移量从而消除对Zookeeper的依赖。\n另外不建议将使用一个Zookeeper集群来同时管理Kafka集群和其他应用。因为当其他应用对Zookeeper产生了较大的流量时，可能会影响Kafka集群自身的治理，进而导致Kafka集群的所有Broker节点由于丢失了Zookeeper的连接导致同时下线。\n","slug":"Kafka搭建、基本配置和调优","date":"2023-08-09T09:39:17.000Z","categories_index":"","tags_index":"MQ","author_index":"姚文彬"},{"id":"b616fd44c5f7ef9bc2632badbc783987","title":"Kafka架构与核心概念","content":"![](Pasted image 20230808161829.png)\nKafka Cluster集群：Kafka集群会由一个或多个Broker（有的地方也叫server或者是node）组成，Broker负责Producer和Consumer之间的消息传递和持久化。 是整个架构中Kafka的核心服务。\nZookeeper Node：Kafka集群需要ZK来保证节点之间的通信，并且使用Zookeeper来存储每个Broker下的核心信息，包括Broker地址，Topic主题的信息，Broker所负责的Topic下的Partition分区。\nTopic：是一系列相关主题的合集，Kafka中每个消息都会有有且仅有一个主题，不同的消息通过主题从逻辑上区分。\nPartition分区：是Topic的实际存储区域，一个Topic下会有多个Partition，分布在Kafka Cluster中的不同节点上，使得一个主题能够具有更大的吞吐量。Partition是Kafka能够具有横向扩展能力的核心，通过不断的增加Broker，Partition从理论上也能够无限衍生。使得吞吐量也能够无限增长。Kafka Cluster能够保证Partition会尽可能均衡的存放于每一个Broker中。\nProducer生产者：是消息的创建方，能够显示的或者隐式的将消息发送往主题下的某一个具体的分区中。\nConsumer消费者：是消息的使用方，在创建时能够指定订阅的主题。 连接上Broker后，Broker会根据当前主题和Consumer之间的关系，为其分配一个或多个Partition。 分配完成后，Consumer开始根据Consumer Offset消费进度进行消费。\nConsumer Group消费者组：标明一组相同业务逻辑的Consumer，使得消费者也能够横向扩展。当出现消息积压的时候，能够通过不断的增加消费者实例来快速消费消息。\nConsumer Offset消费进度：当消费者消费完一条消息时，会提交自身当前对于已消费记录的消费进度，使得消费者宕机或者断开连接时，其他消费者能够根据分区当前的消费进度继续消费该分区的消息。\nOffset Commit Point消息提交点：Kafka的Consumer在消费完消息之后的提交位置，通过变更消息，使得kafka的交付策略能够在at-most-once、at-least-once、exactly-once三重语义中切换。\nRecord记录&#x2F;Message消息：数据存储的最小单元，也是一个具体发生事件的承载体。\nKey消息键：每个消息都能够标记一个非唯一的任意类型的数据，在Kafka Producer的消息发送策略中，如果没有显示的指定分区号，则会通过Key进行Hash，然后和分区数量求余，取得一个分区号使得具有相同Key的消息总是会分布在同一个分区中。\n\n\n\n\n\n\n\n\n\n这里存在着一个问题，因为Hash的算法还存在一个变量就是分区数量，所以当Kafka中的分区通过Kafka管理工具进行修改之后，具有相同Key的新新消息可能不再和旧消息存放于同一个分区中。 这在需要保证消息顺序的场景下需要注意的地方。\nPartially Order部分顺序：Kafka中一个Partition分区下的消息是天然有序（Total Order）的，而由Partition分区组成的Topic主题是部分有序的（Partially Order）\n","slug":"Kafka架构与核心概念","date":"2023-08-08T08:57:39.000Z","categories_index":"","tags_index":"MQ","author_index":"姚文彬"},{"id":"51d09946e274f8b72c614484ac075533","title":"基于Prometheus实现SpringBoot应用数据采集与业务埋点","content":"前提概要：假设你已经了解了Prometheus是做什么的，以及如何基于Prometheus搭建一个指标监控体系。本章节将讲解如何基于Prometheus对SpringBoot进行指标采集\nSpringBoot监控添加prometheus依赖：\n&lt;!-- for monitor -->  \n&lt;dependency>  \n    &lt;groupId>org.springframework.boot&lt;/groupId>  \n    &lt;artifactId>spring-boot-starter-actuator&lt;/artifactId>  \n&lt;/dependency>\n\n&lt;dependency>  \n    &lt;groupId>io.micrometer&lt;/groupId>  \n    &lt;artifactId>micrometer-registry-prometheus&lt;/artifactId>  \n&lt;/dependency>\n\n使用actuator暴露prometheus端口：\n# 供prometheus监控\nmanagement:\n  endpoints:\n    web:\n      exposure:\n        # 上报所有接口\n        include: \"*\"\n  metrics:\n    tags:\n      # 为上报数据添加applictoin的tag\n      application: $&#123;spring.application.name&#125;\n    export:\n      prometheus:\n        enabled: true\n\n![Pasted image 20230522174238.png](Pasted image 20230522174238.png)Prometheus会为自动的为SpringBoot应用产生生成非常多个监控指标，在正常情况下足够我们使用了：\n业务埋点spring-boot-actuator只能够帮助我们采集技术上的指标。如果我们想对业务指标进行监控，则可以对其进行拓展，Prometheus注解和API的方式，帮助我们快速的实现统计。\n基于注解实现\n@Gauge - 用于度量可变值的当前状态，例如内存使用量、CPU负载、线程数等。适用于度量瞬态数据。\n@Time - 用于记录方法的执行频率\n@Counted - 用于记录方法的调用次数，适合记录应用程序中重要事件的发送情况。例如用户请求的成功或失败计数\n@ExceptionMetered - 用于记录异常发生的次数和频率\n\n注意：这些注解只是将指标名称和度量值注册到Micrometer中，并没有为所监控的指标创建实际的计数器对象。因此，我们还需要显示的将指标注册到Micrometer中：\n首先我们需要额外引入micrometer-core组件\n&lt;dependency>  \n    &lt;groupId>io.micrometer&lt;/groupId>  \n    &lt;artifactId>micrometer-core&lt;/artifactId>  \n&lt;/dependency>\n\n比如说我们想要监控下单接口的执行时间，则可以这样做：\n@RestController\npublic class OrderController&#123;\n\n\tpublic OrderController(MeterRegistry registry) &#123;  \n\t    registry.counter(\"demo.order.latency\");  \n\t&#125;\n\n\t@PostMapping(\"/place\")\n\t@Timed(\"demo.order.latency\")\n\tpublic void place() &#123;\n\t\t// do business code.\n\t&#125;\n\t\n&#125;\n\n基于API实现除了注解，我们还可以通过API的方式处理代码块中更复杂一些的情况对其进行埋点，基本的使用方式如下：\n\n@RestController  \npublic class TestController &#123;  \n  \n    Counter counter;  \n\t// 1. 通过自动注入MeterRegistry\n    public TestController(MeterRegistry registry) &#123;  \n    // 2. 通过对应的API接口构建采集器\n        counter = Counter.builder(\"charon_test_counts\")  \n                .register(registry);  \n    &#125;  \n  \n  \n    @GetMapping(\"/test\")  \n    public String get() &#123;  \n\t    // 3. 执行采集任务\n        counter.increment();  \n        return \"true\";  \n    &#125;  \n&#125;\n\n\n统计线程情况通过ExecutorServiceMetrics.monitor()构造方法，我们可以对线程池进行监控以采集关键指标，以定时任务线程池为例，代码如下图所示：\n@Autowired  \nprivate MeterRegistry meterRegistry;  \n\nprivate final ScheduledExecutorService EXECUTOR = ExecutorServiceMetrics.monitor(  \n        meterRegistry,  \n        Executors.newScheduledThreadPool(1, new ThreadFactory() &#123;  \n            @Override  \n            public Thread newThread(Runnable r) &#123;  \n                Thread t = new Thread(r);  \n                t.setName(\"schedule-thread-\" + t.getId());  \n                return t;  \n            &#125;  \n        &#125;), \"Scheduled-Executor\"  \n);\n\n结果展示![Pasted image 20230522214103.png](Pasted image 20230522214103.png)\n","slug":"基于Prometheus实现SpringBoot应用数据采集与业务埋点","date":"2023-05-22T14:01:22.000Z","categories_index":"","tags_index":"SpringBoot,Prometheus,业务","author_index":"姚文彬"},{"id":"3e10980a1e0900db8865138486accab8","title":"Maven版本管理：使用单个属性来管理项目所有模块的版本号","content":"本章节将讲解如何使用单个变量来集中一个由Maven组成的多模块项目的版本号\n在开发过程中，我们总是要对项目进行不断的迭代升级，而在Maven的版本控制中，我们可以通过不同的版本号，来将不同版本的项目打包放于同样的地方。![](Pasted image 20230518212916.png)\n默认情况下，我们是这样通过固定的值来管理版本号的：![](Pasted image 20230518212944.png)\n单个项目下维护不存在什么困难，但是如果存在着多个子项目，那么每个子项目中都会有固定的父项目的版本引用，一旦改动，就需要改动所有的pom.xml。\n![](Pasted image 20230518213146.png)\n也许你可以通过全局替换来升级版本号，但是有没有更好的办法呢？当然，你可以通过Maven的properties（属性）中的revision值来实现：父项目版本parent’s version：![](Pasted image 20230518213525.png)\n父项目中对于子模块依赖管理：统一都使用revision变量：![](Pasted image 20230518213715.png)\n子项目中对于父项目的引用也是revision进行：![](Pasted image 20230518213545.png)\n通过revision变量，你可以通过修改一处的值来升级整个项目的版本号。\n\n\n\n\n\n\n\n\n\n能够更换为其他的变量名呢？可能不行，在IDEA中会有properties are prohibited属性被禁止的错误提示。\n![](Pasted image 20230518213428.png)\n","slug":"maven/Maven版本管理：使用单个属性来管理项目所有模块的版本号","date":"2023-05-18T13:26:54.141Z","categories_index":"","tags_index":"Maven","author_index":"姚文彬"},{"id":"90732c789d41e9ff8627e26b080796f4","title":"MMAP/Page Cache - 以Javaer的角度看待内存映射文件机制","content":"什么是MMAP？MMAP(Memory-mapped file)是一种内存映射文件的机制。用于建立从文件到内存之间的一种映射，将对磁盘的写入映射为对内存的写入，以增加对于文件IO的效率。\n在Linux上主要通过mmap()、munmap()、msync()等系统调用来实现。\n什么是Page Cache？Page Cache页缓存是Linux实现MMAP机制的底层关键组件，其将文件分为多个大小为4KB的数据块。\n前面说文件传输过程中，将磁盘文件拷贝到内核缓冲区中，这个缓存区实际上就是Page Cache。\nPageCache的大小在32位OS中为4K，在64位操作系统中大小为8K。PageCache有两个特性：\n\n缓存在操作系统中，具有「局部性」，即刚被访问的数据再次被访问的概率更高。 所以Linux中使用Page Cache来缓存最近被访问的数据。所以读取磁盘的时候会有先从Page Cache中读取，如果没有再从磁盘中读取。\n\n预读PageCache的一个很好的特性就是预读，也就是当应用程序发生读操作时，文件系统会提前为应用程序读取的文件加载更多的数据到Page Cache中，这样下一次读取的时候就可以直接命中Page Cache了。（很多地方都运用到了这样的技术，比如说JVM中）有效的减少了磁盘的访问次数。\n\n\n在Java NIO中通过Channel.map()方法即可将文件区域映射到内存中实现MMAP机制，其会返回一个MappedByteBuffer对象。这个对象在Linux映射的内存区域实际上就是Page Cache。\nFileChannel fileChannel =  new RandomAccessFile(new File(\"/tmpFile\"), \"rw\").getChannel();  \n\nMappedByteBuffer mappedByteBuffer = fileChannel.map(FileChannel.MapMode.READ_WRITE, 0, 1024 * 4);\n\n\n\n零拷贝的限制\n不适合大文件，因为MMAP在加载的时候是预先加载内存中的空间的，如果映射文件太大就会造成占用过的内存空间。\n不适合变长文件，因为MMAP在使用的时候需要指定映射文件的大小，一般为内存页的整数倍。\n磁盘延迟，MMAP是通过缺页中断向磁盘发起真正的IO请求，而具体是何时写入将交由系统来决定，而不是应用程序。\n\nRocketMQ中对零拷贝的优化\n内存预映射机制RocketMQ通过维护一个MappedFileQueue队列，在每次消息的写入都获取消息队列中的最后一个MappedFile，如果没有则会创建一个并且将下一个也创建好。\n文件预热在调用mmap()进行内存映射时，其实只是建立起虚拟内存与物理地址的映射关系，不会加载任何文件到内存中。而RocketMQ的在MappedFile#warmMappedFile方法中，每隔一个页(4k)就put一个0，将文件加载到内存中，这样当消息读取或者写入的时候可以直接命中Page Cache。\nmlock内存锁定在warmMappedFile()预热后还会调用mlock方法， 将预热后的内存空间进行锁定，来防止操作系统将内存空间调到swap空间中。\n\n","slug":"java/MMAP与Page Cache","date":"2023-05-09T08:20:21.340Z","categories_index":"","tags_index":"Java,OS","author_index":"姚文彬"},{"id":"7d219fe8f2483d731e9b880a374df88c","title":"注册JVM钩子函数来实现优雅停机","content":"在各种连接池场景下，当应用发生了关闭。那么所有连接池（数据源、请求业务线程）都会强行的中止线程，如果此时正在发生一些较为重要的业务操作也会被强制停止。\n因此作为有经验的工程师，我们会通过Runtime::addShutdownHook()来帮助我们注册JVM的关闭钩子，已达到JVM虚拟机正在关闭时，主动调用shutdown方法关闭线程池释放资源。\ntry &#123;\n\tpoolExecutor.shutdown();\n\tif (!poolExecutor.awaitTermination(AWAIT_SECOND, TimeUnit.SECONDS) &#123;\n\t\tpoolExecutor.shuwdownNow();\n\t&#125;\n&#125; catch (InterruptException e) &#123;\n\tpoolExecutor.shutdownNow();\n\tThread.currentThread().interrupt();\n&#125;\n\n甚至你可以通过注册关闭钩子来完成内存数据的持久化。\nRuntime.getRuntime().addShutdownHook(new Thread(() -> &#123; \n\ttry (FileWriter writer = new FileWriter(\"data.txt\")) &#123; \n\t\tfor (String str : data) &#123; \n\t\t\twriter.write(str + \"\\n\"); \n\t\t&#125; \n\t&#125; catch (IOException e) &#123; \n\t\te.printStackTrace(); \n\t&#125; \n&#125;));","slug":"java/注册JVM钩子函数来实现优雅停机","date":"2023-05-08T05:37:12.125Z","categories_index":"","tags_index":"JVM,Java,线程池","author_index":"姚文彬"},{"id":"4b2a67d332f9fb2b23f7506e99d7e81b","title":"Java中属性初始化的差异","content":"在Java中，你可以直接在声明属性时初始化、也可以在构造函数中初始化、甚至可以在static代码块中初始化，那么三者有什么差异呢？\n\n初始化的时机不同static代码块中的代码会在类第一次加载时就进行，不需要实例的存在。而构造函数和属性中初始化的变量在对象创建时才进行。所以static代码块适用于被static关键字修饰的属性初始化。\n\n错误的处理如果在属性中初始化时发生错误很难处理异常。但是在static和构造函数中可以通过try-catch的语句来捕获并处理异常，适用于更复杂的场景。\n\n\n所以如果变量需要在多个对象之间共享，或者需要在对象不存在时就进行初始化，应该使用static代码块进行初始化。\n","slug":"Java中属性初始化的差异","date":"2023-05-05T13:14:39.000Z","categories_index":"","tags_index":"Java","author_index":"姚文彬"},{"id":"752d53d9b20a96f3018bcb72f69595d6","title":"Volatile关键字小结","content":"Voaltile关键字用于保证变量在多线程环境下的可见性、禁止指令重排。\n为什么多线程环境下会出现这些问题？主要是由于JMM的存在。\n可见性：而多线程环境下如果一个线程修改了变量，那么其他线程保存中的副如果没有同步的从主内存中读取，就造成了可见性的问题。\n指令重排：指令重排技术是编译器和处理器对于程序的优化，通过改变某些代码的执行顺序从而提高执行效率，但是也可能因此破坏程序的正确性。\nVolatile的底层原理实际上就是在JVM中每个volatile修饰的变量赋值后增加一个JMM定义的lock操作，来保证每次对变量的读取都是从主内存中读取的。以保证可见性。虽然能够保证可见性，但是volatile不能够确保对变量的写操作能够立刻反应到其他线程中功能，\n适用于：\n\n状态标记，比如停止线程和停止任务等操作，当一个线程修改了volatile变量的值时，其他线程能够立即看到这个变化。\n双重检查锁定，实现延迟加载和单例模式时，都可以通过volatile实现双重检查锁定，确保变量的可见性以及禁止指令重排。\n\n","slug":"Volatile关键字","date":"2023-05-05T13:13:39.000Z","categories_index":"","tags_index":"Java,多线程","author_index":"姚文彬"},{"id":"ad2c272cfe3adee335f5f82ded13d5f6","title":"RocketMQ事务消息","content":"什么是事务首先我们先回顾一下事务的主要作用，是要保证多个操作的原子性，多个事务操作要么一起成功，要么一起失败。\n分布式事务使用场景使用场景模拟：用户支付订单后，同时涉及到多个下游：物流发货、积分变更、购物车状态清空。在这个场景中的事务操作有四种：\n\n订单系统的状态状态更新。\n物流系统的订单物流记录新增。\n积分系统的用户积分更新。\n购物车系统的用户购物车状态更新。\n\n传统解决方案为了实现这样的事务操作，传统的解决方案就是基于XA协议（二阶段和三阶段提交）的分布式事务，将四个操作分装为四个独立的大事务来解决。这样能够帮助业务处理结果的正确性，但是无法由于资源锁定的粒度大，并发性能低。\nRocketMQ的事务消息而RocketMQ中的事务消息，为我们提供了最终一致性的解决方案：处理流程如下图所示：\n()[&#x2F;Pasted image 20230424141848.png]\n\n实现过程类似二阶段提交，通过Half Message半消息来帮助我们在Server中作为提交第一阶段作为事务的开始。\n如果服务端未收到发送着提交的第二阶段的二次确认结果，那么会主动的向消息生产者集群的任一消费者发起消息回查。\n\n使用限制：根据RocketMQ官方文档中给出的的事务消息的使用限制如下：\n\n仅支持MessageType消息为Tranasction的Topic主题使用。\nApache RocketMQ不保证消息消费结果与上游事务的一致性，所以需要下游业务分支做好消费重试自行保证消息被正确处理。\n中间状态可见性，RocketMQ的事务消息为最终一致性，所以消息在提交到消费完成之前，上游和下游看到的消息的状态可能会出现不一致性。\n事务超时时间，RocketMQ事务生命周期默认存在超时机制，若执行超时时间内服务端无法确认提交还是回滚，那么最终会回滚消息。\n\n实践：\n引入RocketMQ官方的springboot集成包\n&lt;dependency>  \n    &lt;groupId>org.apache.rocketmq&lt;/groupId>  \n    &lt;artifactId>rocketmq-spring-boot-starter&lt;/artifactId>  \n    &lt;version>2.2.3&lt;/version>  \n&lt;/dependency>\n\n使用application.yml配置nameserv地址：\nrocketmq:  \n  name-server: 127.0.0.1:9876  \n  producer:  \n    group: producer-group\n\n创建Application\n@SpringBootApplication  \npublic class ProducerApplication implements CommandLineRunner &#123;\n\n\t@Resource  \n\tOrderPaidEventProducer paidEventProducer;\n\t\n\t@Override  \n\tpublic void run(String... args) &#123;  \t  \n\t    // Transaction Message  \n\t    OrderPaidMessage paidMessage = new OrderPaidMessage(order);  \n\t    paidEventProducer.send(paidMessage);  \n\t&#125;\n\t\n\tpublic static void main(String[] args) &#123;  \n\t    SpringApplication.run(ProducerApplication.class, args);  \n\t&#125;\n&#125;\n\n创建OrderPadiEventProducer订单支付事务消息发送者\n/**  \n * @Author yaowenbin\n * @Date 2023/4/24 \n */\n@Service  \n@RequiredArgsConstructor  \npublic class OrderPaidEventProducer &#123;  \n  \n    private final Logger logger = LoggerFactory.getLogger(OrderPaidEventProducer.class);  \n  \n    private final RocketMQTemplate template;  \n  \n  \n    public void send(OrderPaidMessage message) &#123;  \n        TransactionSendResult result = template.sendMessageInTransaction(\"application-transaction\",  \n                MessageBuilder.withPayload(message).build(),  \n                null  \n        );  \n  \n        logger.info(\"Transaction Send Result: &#123;&#125;\", result);  \n    &#125;  \n%\n    @RocketMQTransactionListener  \n    static class TransactionListenerImpl implements RocketMQLocalTransactionListener &#123;  \n  \n        @Override  \n        public RocketMQLocalTransactionState executeLocalTransaction(Message message, Object o) &#123;  \n            return null;  \n        &#125;  \n  \n        @Override  \n        public RocketMQLocalTransactionState checkLocalTransaction(Message message) &#123;  \n            OrderPaidMessage payload = (OrderPaidMessage) message.getPayload();  \n            return checkOrderStatus(payload.getOrderId()) ? RocketMQLocalTransactionState.COMMIT : RocketMQLocalTransactionState.ROLLBACK;  \n        &#125;  \n  \n        public boolean checkOrderStatus(Long id) &#123;  \n            return id != null;  \n        &#125;  \n    &#125;  \n  \n&#125;\n\n","slug":"rocketmq/事务消息","date":"2023-04-24T06:08:43.782Z","categories_index":"","tags_index":"Java,RocketMQ,分布式事务","author_index":"姚文彬"},{"id":"13c371c2ff633aa9e8eadac4915ee855","title":"FastJson安全漏洞分析","content":"FastJson如何进行序列化的？FastJson和Jackson的序列化方式都是通过反射获取对象的getter方法来获得属性值的（Gson是通过直接反射属性）。\nFastJson的序列化问题当一个类实现了一个接口的时候，在FastJson中进行序列化时，会将这个类的实际类型抹去，只保存该类所实现的接口类型。而这会导致反序列化时无法拿到原始类型。\n在类嵌套类的序列化中，FastJson并不能够为我们将子类给成功反序列化：\n// 序列化\nStore store = new Store();\nstore.setName(\"Hollis\");\nApple apple = new Apple();\napple.setPrice(new BigDecimal(0.5));\nstore.setFruit(apple);\nString jsonString = JSON.toJSONString(store);\nSystem.out.println(\"toJSONString : \" + jsonString);\n// result:\ntoJSONString : &#123;\"fruit\":&#123;\"price\":0.5&#125;,\"name\":\"Hollis\"&#125;\n\n// 反序列化\nStore newStore = JSON.parseObject(jsonString, Store.class);\nSystem.out.println(\"parseObject : \" + newStore);\nApple newApple = (Apple)newStore.getFruit();\nSystem.out.println(\"getFruit : \" + newApple);\n\n// result:\ntoJSONString : &#123;\"fruit\":&#123;\"price\":0.5&#125;,\"name\":\"Hollis\"&#125;\nparseObject : Store&#123;name='Hollis', fruit=&#123;&#125;&#125;\nException in thread \"main\" java.lang.ClassCastException: com.hollis.lab.fastjson.test.$Proxy0 cannot be cast to com.hollis.lab.fastjson.test.Apple\nat com.hollis.lab.fastjson.test.FastJsonTest.main(FastJsonTest.java:26)\n\nAutoType而为了解决这个问题，在FastJson中引入了一个叫AutoType的机制，能够在序列化时把原始类型记录下来。通过SerializerFeature.WriteClassName\nString jsonString = JSON.toJSONString(store,SerializerFeature.WriteClassName);\n\n// reuslt\n&#123;\n    \"@type\":\"com.hollis.lab.fastjson.test.Store\",\n    \"fruit\":&#123;\n        \"@type\":\"com.hollis.lab.fastjson.test.Apple\",\n        \"price\":0.5\n    &#125;,\n    \"name\":\"Hollis\"\n&#125;\n\n可以看到JSON字符串中多出来了@type类型，标注了类的原始类型。在反序列化时，FastJson看到@type的类型就会从Java类库中查找到对应的类。\nRMI而这样本身是没有问题的。而刚好在sun官方提供的类库中有一个JdbcRowSetImpl类，其datasourceName支持rmi传入。没错和log4j2一样的RMI源的问题，当解析这个uri的时候，会去指定的rmi地址中调用方法。所以如果黑客在这个rmi地址中注入了想要执行的指令，那么就会造成严重的后果了。\n如何避免\n升级到最新版本的FastJson\n打开SafeMode，FastJson会自动禁用AutoType。\n\nFastJson的优势虽然FastJson的AutoType导致了安全漏洞，由于其自己定义了序列化工具类，并且使用asm技术避免反射、使用缓存、并且做了很多算法优化等方式，大大提升了序列化及反序列化的效率。\n","slug":"FastJson安全漏洞","date":"2023-03-30T01:27:51.735Z","categories_index":"","tags_index":"Java,序列化","author_index":"姚文彬"},{"id":"ecdf1c9aa66b051d88622cd0bc08ef91","title":"JDK里程碑:JDK8到JDK17的重要特性汇总","content":"\n\n\n\n\n\n\n\n\n从BenchMark可以看到，仅需要从JDK8到JDK17就能够获得64%的改进 — JavaOne开幕式\n本文章的灵感来源于Java One开幕式上JDK开发者的这句话。JDK向前兼容的重要特性意味着，我们不需要改动任何代码只需要将运行源代码的JDK从8替换为JDK17就能够获得大量的提升（极地的成本）。 \n这篇文章讲汇总JDK8到JDK17的关键特性，理解为什么从JDK8到JDK17有着重要意义。\nJDK底层&#x2F;API语法：JDK9\n\n接口方法可以使用private修饰\n支持http2.0和websocketAPI\n模块化编程（对于依赖于Maven的开发者来说使用率和其产生的作用不高）\nString类底层数组从char[]变更为了byte[]。（byte能够在同样支持拉丁字符的情况下只占用1个字节，而char需要2个）\n支持NumberFormat API，能够对数字进行格式化或者压缩\n\nJDK10\n\n推出var关键字支持局部变量类型推断，类似JS推断出值的真实类型\n\nJDK11\n\n集合API的增强\n对Stream、Optional、集合API增强\n\nJDK13\n\nSocket底层优化，引入NIO\nswitch表达式添加yield关键字，用于返回结果，作用类似return\n引入”””三双引号语法（文本块），内部不需要使用换行的转义字符。\n\nJDK14\n\ninstanceof关键字优化，代码块中会直接给对象赋值\n引入record类，会自动生成构造器、getter、setter等方法，类似Lombok。\nNPE优化，可以打印具体哪个方法跑出了NPE，方便单行代码多个函数调用时的异常排查。（开发者福音）\n\nJDK15\n\n隐藏类Hiden Class\n密封类 Sealed Class，密封类能够限定继承或者实现的子类。\n对应java.lang.Class问类中新增了isSealed和getPermittedSubClasses()两个方法用于判断密封类和密封类所允许的拓展Class列表\n\nJDK16正式的将record、instanceof特性引入到JDK版本中\nJDK17正式引入密封类sealed class\n\n统一日志一步刷新\n\nJVM优化：JDK9\n\n设置为G1默认垃圾回收器\n\nJDK10\n\n并行FullGC，优化G1的延迟（JDK9的Full GC只有单线程，而10之后会采用Young和Mixed相同数量的线程进行FullGC）\n线程局部管控，支持在不执行全局VM安全点的情况下对线程执行回调方法，停止单个线程，而不需要停止所有线程。\n\nJDK11\n\n推出ZGC（随后经过了多次的优化，是JDK17中综合性能最好的垃圾收集器，根据JavaOne的介绍其目标是能够支持亚秒级别回收TB界别的堆，还没达到但已经非常强了）\n\nJDK12\n\n推出Shenandoan垃圾收集器（12， 仅支持Linux，）\n拓展swtich表达式，支持返回值（12\n优化G1收集器，将G1的垃圾分为强制回收和可选部分，提高GC效率\n\nJDK13\n\nZGC优化，将标记长时间的空闲对内存空间返回给OS，保证堆大小不会小于配置的最小堆内存\n\nJDK15\n\nZGC性能优化\n\n新语法&#x2F;API实际使用val局部变量类型推断标识符var不是关键字，而只是一个保留的类型名称，仅适用于局部变量，能够通过值来推断出值的数据类型过是什么。示例\nvar str = \"ABC\"; //根据推断为 字符串类型\nvar l = 10L;//根据10L 推断long 类型\nvar flag = true;//根据 true推断 boolean 类型\nvar flag1 = 1;//这里会推断boolean类型。0表示false 非0表示true\nvar list = new ArrayList&lt;String>();  // 推断 ArrayList&lt;String>\nvar stream = list.stream();          // 推断 Stream&lt;String>\n反编译class文件：\nString str = \"ABC\";\nlong l = 10L;\nboolean flag = true;\nint flag1 = true;\nArrayList&lt;String> list = new ArrayList();\nStream&lt;String> stream = list.stream();\n\nswitch表达式\n现在支持使用switch赋值\n引入了lambda表达式，\n引入了yield语法，能够返回值，而不是使用break；\nString result = switch (day) &#123;\n    case \"M\", \"W\", \"F\" -> \"MWF\";\n    case \"T\", \"TH\", \"S\" -> \"TTS\";\n    default -> &#123;\n        if(day.isEmpty())\n            yield \"Please insert a valid day.\";\n        else\n            yield \"Looks like a Sunday.\";\n&#125;\n\ninstanceof模式匹配JDK14之前\nif (obj instanceof Article) &#123;\n  Article a= (Article) obj;\n  System.out.println(a.getAuthor());\n&#125;\nJDK14之后\nif (obj instanceof Article a) &#123;\n  System.out.println(a.getAuthor());\n&#125;\n\n\n\nNPE错误信息更详细JDK14之前\nException in thread \"main\" java.lang.NullPointerException at NullPointerExample.main(NullPointerExample.java:5)\nJDK14之后\nException in thread \"main\" \njava.lang.NullPointerException: Cannot invoke \"Blog.getAuthor()\" because the return value of \"Article.getBlog()\" is null at NullPointerExample.main(NullPointerExample.java:4)\n\nrecord记录类record Author()&#123;&#125;\n//or\nrecord Author (String name, String topic) &#123;&#125;\n编译器会自动生成构造函数、g&#x2F;setter、equals&#x2F;hashCode、toString等方法\nsealed 密封类// 创建一个密封的Hello接口，只允许Hello2类对其进行实现\npublic interface sealed class Hello permits Hello2&#123;&#125;\n\nNumberFormat增加了对压缩数字的支持String number = NumberFormat.getCompactNumberInstance(Locale.US, \n\t\tNumberFormat.Style.SHORT).format(1000);\n// 其中number = “1k“\n\nString类API的增强添加了strip()，isBlack()、isEmpty()、lines()、repeat()等多个方法\n\nstrip()看作是trim的增强，能够去掉unicode空格，对应的还有stripLeading()和stripTailing()\nisBlack()和isEmpty()两者是String类中内置的方法，但依然建议使用各个工具包下的StringUtils。因为使用内置方法时，如果对象为空会产生NPE；而工具类下的StringUtils则不会。\nlines()会将单个多行字符才成多个单行字符\nrepeat()能够构建一个或多个相同字符串组合的字符串\n\n","slug":"JDK里程碑-JDK8到JDK17的重要特性汇总","date":"2022-11-27T03:59:14.000Z","categories_index":"","tags_index":"JDK,JVM","author_index":"姚文彬"},{"id":"c7562fc289c09e37ab4a4b417948e80b","title":"OkHttp：更加优雅的客户端OkHttps","content":"上文中，我们介绍了OkHttp的一些常见的用法，以及对其API进行了一些便于调用的封装。而笔者在深入学习的过程中发现了一个基于Lambda表达式、链式调用进行封装的OkHttps，使其的调用方式更加的优雅、简介。\n简介https://ok.zhxu.cn/v4/introduction.htmlOkHttps 是 2020 年开源的对 OkHttp3 轻量封装的框架，它独创的异步预处理器，特色的标签，灵活的上传下载进度监听与过程控制功能，在轻松解决很多原本另人头疼问题的同时，设计上也力求纯粹与优雅。\nMaven中要想使用OkHttps，我们需要引入一个OkHttps核心包和一个OkHttps-xxxx(序列化框架)的序列化包，比如FastJson2，我们可以通过     cn.zhxu     okhttps     4.0.0     cn.zhxu     okhttps-fastjson2     4.0.0\n如果是Jackson的话，我们只需要将fastjson2换成     cn.zhxu     okhttps-jackson     4.0.0即可。\n基本使用\n首先，我们需要向使用OkHttp那样，构建一个HTTP请求， 为了使用方便，我们更愿意指定一个BaseUrl和MsgConverter：HTTP http &#x3D; HTTP.builder()        .baseUrl(“http://api.example.com&quot;)        &#x2F;&#x2F; GsonMsgConverter源自okhttps-gons包中，使用时更换为实际导入的序列化包        .addMsgConvertor(new GsonMsgConvertor())        .build();\n随后，我们可以通过链式调用的方式来开始一个同步请求了：List users &#x3D; http.sync(“&#x2F;users”) &#x2F;&#x2F; http://api.example.com/users        .get()                         &#x2F;&#x2F; GET请求        .getBody()                     &#x2F;&#x2F; 获取响应报文体        .toList(User.class);           &#x2F;&#x2F; 得到目标数据或者是一个异步请求：http.async(“&#x2F;users&#x2F;1”)                &#x2F;&#x2F;  http://api.example.com/users/1        .setOnResponse((HttpResult res) -&gt; {            &#x2F;&#x2F; 得到目标数据            User user &#x3D; res.getBody().toBean(User.class);        })        .get();                       &#x2F;&#x2F; GET请求如果你想要一个WebSocket通讯的话，可以这样：http.webSocket(“&#x2F;chat”)        .setOnOpen((WebSocket ws, HttpResult res) -&gt; {            ws.send(“向服务器问好”);        })        .setOnMessage((WebSocket ws，Message msg) -&gt; {            &#x2F;&#x2F; 从服务器接收消息（自动反序列化）            Chat chat &#x3D; msg.toBean(Chat.class);            &#x2F;&#x2F; 相同的消息发送给服务器（自动序列化 Chat 对象）            ws.send(chat);        })        .listen();                     &#x2F;&#x2F; 启动监听\n请求三部曲对于任何请求，OkHttps都可以将其看作为三个步骤：\n\n确定请求方式（同步、异步） \n构建请求任务（添加请求参数、设置回调函数） \n调用请求方法（get、post、delete、put）而对于这种模板化的开发方式，作为IDEA的开发者我们可以通过Live Template来提高我们的开发效率：\n\n注入配置OkHttps还支持通过SPI的方式注入自定义配置类\n第一步：新建一个配置类实现cn.zhxu.okhttps.Config接口package com.example.okhttps;\nimport cn.zhxu.okhttps.Config;import cn.zhxu.okhttps.HTTP;\npublic class OkHttpsConfig implements Config {\n@Override\npublic void with(HTTP.Builder builder) &#123;\n    // 在这里对 HTTP.Builder 做一些自定义的配置\n    builder.baseUrl(&quot;https://api.domo.com&quot;);\n    // 如果项目中添加了 okhttps-fastjson 或 okhttps-gson 或 okhttps-jackson 依赖\n    // OkHttps 会自动注入它们提供的 MsgConvertor \n    // 所以这里就不需要再配置 MsgConvertor 了 (内部实现自动注入的原理也是 SPI)\n    // 但如果没有添加这些依赖，那还需要自定义一个 MsgConvertor\n    builder.addMsgConvertor(new MyMsgConvertor());\n&#125;\n\n}第二步：在项目的&#x2F;src&#x2F;main&#x2F;目录下新建resources&#x2F;META-INF&#x2F;services&#x2F;cn.zhxu.okhttps.Config文件，文件内容是上一个配置类的全类名\n为什么要使用SPI注入配置而不是直接给OkHttps设置一个静态变量实例呢？在一般情况下这样是没问题的，但是某些JVM上（特别是Android）会在某些情况下回收静态变量，而OkHttps的HTTP实例一旦被回收之后，配置都会丢失，导致严重的问题。\n对于SpringBoot项目，如果还想要导入application.yml的配置的话，我们可以现在Spring的Bean中加载配置，然后提供静态访问方法，再去配置类中访问这个Bean。@Componentpublic class ConfigBean {\n@Value(&quot;$&#123;okhttps.baseUrl&#125;&quot;)    // 加载配置\nprivate String baseUrl;\n\nprivate static ConfigBean instance;\n\npublic ConfigBean() &#123;\n    instance = this;            // 将该 Bean 单例化\n&#125;\n\npublic static ConfigBean getInstance() &#123;\n    return instance;            // 提供静态访问方法\n&#125;\n\n// 省略 Getter Seatter\n\n}public class OkHttpsConfig implements Config {\n@Override\npublic void with(HTTP.Builder builder) &#123;\n    // 获取 ConfigBean\n    ConfigBean confg = ConfigBean.getInstance();\n    // 使用配置\n    builder.baseUrl(confg.getBaseUrl());\n    // 省略其它配置\n&#125;\n\n}\n","slug":"OkHttp：更加优雅的客户端OkHttps","date":"2022-11-21T00:55:52.000Z","categories_index":"","tags_index":"工具,OkHttp","author_index":"姚文彬"},{"id":"db73c7a78b5189912e109ef7e8566686","title":"减少你的代码量：基于AOP实现字典翻译","content":"什么是字典？开发者更加倾向于使用短小的字段类型比如 tinyint、char(1)等来存储一些类型字段，以让每个数据都能够尽可能少的占用空间。 而用户当然不买单，希望能够看到每个字段都真正含义（比如 status &#x3D; 1时，其真正含义是状态进行中， status &#x3D; 2时，其真正含义是状态完成）而数据从1到进行中的数据变换过程我们称之为字典翻译。\n实现方案而字典是任何后台管理系统比不可少的系统功能模块之一。 而根据架构选型的不同，其内容翻译的主要实现方案有两种：\n\n基于后端查询时自动进行翻译（本文主要讨论内容）\n\n在进行查询、列表时后端能够自动对字段进行翻译，把对于status字段，能够有一个statusName（存储翻译结果的名称，随你定）来说明status的含义，并展示给用户。\n\n基于前端展示时的自动翻译\n\n在后端返回结果后，通过status字段再次去调用后端的API或者是本地的字典缓存，来获取status字段的真实含义，然后展示给用户。\n而本文的剩下内容将讨论如何基于Spring AOP来实现方案1（后端自动翻译）功能。\n使用方式\n添加字段用于填充翻译后的文本\n添加@Dict注解，并指定翻译的字典类型\n在API的返回方法中添加@DictTranslation注解class Entity&#123;\n    ...\n\t// 未翻译的字典值\n    private String type;\n\n    @Dict(EVENT_TYPE)\n    private String typeName;\n    ...\n&#125;\n\nclass Controller&#123;\n    ...\n    \n    @GetMapping(\"/detail\")\n    @DictTranslation\n    public ApiResult getById(@NotBlank @RequestParam(\"id\") String id) &#123;\n        return ApiResult.ok().data(service.getInfo(id));\n    &#125;\n    \n    ...\n&#125;\n\n\n\n\n\n\n\n\n\n\n实现结果\n\n实现步骤\n添加注解@Dict用于指定字典的元数据（字典类型和字典值）\n@Retention(RetentionPolicy.RUNTIME)\n@Target(ElementType.FIELD)\npublic @interface Dict &#123;\n\n    /**\n     * 字典类型：t_sys_dict_type中的dict_type字段.\n     */\n    String value();\n\n    String codeField() default \"\";\n&#125;\n\n\n添加注解@DictTranslation用于手动指定哪些方法会通过AOP实现自动翻译\n@Retention(RetentionPolicy.RUNTIME)\n@Target(ElementType.METHOD)\npublic @interface DictTranslation &#123;\n&#125;\n\n\n实现注解AOP切面\n/**\n * @Author yaowenbin\n * @Date 2022/11/18\n */\n@Aspect\n@Component\n@Slf4j\npublic class DictAspect &#123;\n\n    @Resource\n    private SysDictDataService dictDataService;\n\n    @Pointcut(\"@annotation(dictTranslation)\")\n    public void pointcut(DictTranslation dictTranslation) &#123;\n\n    &#125;\n\n    @Around(\"pointcut(dictTranslation)\")\n    public Object doAround(ProceedingJoinPoint pjp, DictTranslation dictTranslation) throws Throwable &#123;\n        Object proceed = pjp.proceed();\n\n        if (! (proceed instanceof ApiResult) ) &#123;\n            return proceed;\n        &#125;\n\n        ApiResult result = (ApiResult) proceed;\n        // 翻译单个对象\n        notNull(result.get(ApiResult.FIELD_DATA), val -> &#123;\n            translationDict(result.get(\"data\"));\n        &#125;);\n    \t// 翻译列表对象\n        notNull(result.get(ApiResult.FIELD_PAGE), val -> &#123;\n            PageVo pageVo = (PageVo) result.get(\"page\");\n            List dataList = pageVo.getList();\n            for (Object o : dataList) &#123;\n                translationDict(o);\n            &#125;\n        &#125;);\n\n        return proceed;\n    &#125;\n\n    public void translationDict(Object data) &#123;\n        Field[] fields = data.getClass().getDeclaredFields();\n\n        for (Field field : fields) &#123;\n            if (field.isAnnotationPresent(Dict.class)) &#123;\n                Dict dict = field.getAnnotation(Dict.class);\n\n                String codeFieldName;\n                if (dict.codeField().equals(\"\")) &#123;\n                    String fieldName = field.getName();\n                    codeFieldName = fieldName.substring(0, fieldName.length() - 4);\n                &#125; else &#123;\n                    codeFieldName = dict.codeField();\n                &#125;\n\n                char[] chars = codeFieldName.toCharArray();\n                chars[0] = toUpperCase(chars[0]);\n                codeFieldName = String.valueOf(chars);\n                String getterName = \"get\" + codeFieldName;\n\n                Method codeGetter;\n                try &#123;\n                    codeGetter = data.getClass().getMethod(getterName);\n                &#125; catch (NoSuchMethodException | SecurityException e) &#123;\n                    log.warn(\"翻译失败, &#123;&#125;未找到&#123;&#125;()方法，无法翻译该字段\", data.getClass(), getterName);\n                    continue;\n                &#125;\n                String code;\n                try &#123;\n                    code = (String) codeGetter.invoke(data);\n                &#125; catch (IllegalAccessException | InvocationTargetException e) &#123;\n                    log.warn(\"翻译失败, &#123;&#125;调用&#123;&#125;()异常\", data.getClass(), getterName);\n                    continue;\n                &#125;\n\n                DropdownVo dropdown = dictDataService.getDataByType(dict.value(), code);\n                if (dropdown == null) &#123;\n                    log.warn(\"翻译失败, &#123;&#125;类中的&#123;&#125;未找到数据库中&#123;&#125;对应字典数据\", getClass(), dict, code);\n\n                    continue;\n                &#125;\n                ReflectUtil.setFieldValue(data, field.getName(), dropdown.getLabel());\n\n            &#125;\n        &#125;\n    &#125;\n\n    public &lt;V> void notNull(V obj, Consumer&lt;V> consumer) &#123;\n        if (obj != null) &#123;\n            consumer.accept(obj);\n        &#125;\n    &#125;\n    private char toUpperCase(char c) &#123;\n        if (97 &lt;= c &amp;&amp; c&lt;= 122) &#123;\n            c ^= 32;\n        &#125;\n        return c;\n    &#125;\n\n&#125;\n\n\n","slug":"减少你的代码量：基于AOP实现字典翻译","date":"2022-11-18T09:40:43.000Z","categories_index":"","tags_index":"SpringAOP,减少你的代码量","author_index":"姚文彬"},{"id":"132bc548a30620e50cfa7c41af4da051","title":"OkHttp使用指南:封装一个好用的Http客户端","content":"OkHttp是一个高效的HTTP请求客户端对于Android和Java应用。其有着许多高级的功能，比如线程池、GZip压缩和响应缓存。 同时不仅能够发送同步的请求，也能够支持异步调用。\n&lt;dependency>\n    &lt;groupId>com.squareup.okhttp3&lt;/groupId>\n    &lt;artifactId>okhttp&lt;/artifactId>\n    &lt;version>4.9.1&lt;/version>\n&lt;/dependency>\n\n同步请求我们需要基于URL地址来构建一个Reqeust对象，然后调用newCall()来生成一个Call对象，然后调用execute()方法来同步发送请求并获取相应结果\n@Test\npublic void whenGetRequest_thenCorrect() throws IOException &#123;\n    Request request = new Request.Builder()\n      .url(BASE_URL + \"/date\")\n      .build();\n\n    Call call = client.newCall(request);\n    Response response = call.execute();\n\n    assertThat(response.code(), equalTo(200));\n&#125;\n\n\n异步请求@Test\npublic void whenAsynchronousGetRequest_thenCorrect() &#123;\n    Request request = new Request.Builder()\n      .url(BASE_URL + \"/date\")\n      .build();\n\n    Call call = client.newCall(request);\n    call.enqueue(new Callback() &#123;\n        public void onResponse(Call call, Response response) \n          throws IOException &#123;\n            // ...\n        &#125;\n        \n        public void onFailure(Call call, IOException e) &#123;\n            fail();\n        &#125;\n    &#125;);\n&#125;\n\n添加请求参数@Test\npublic void whenGetRequestWithQueryParameter_thenCorrect() \n  throws IOException &#123;\n    \n    HttpUrl.Builder urlBuilder \n      = HttpUrl.parse(BASE_URL + \"/users\").newBuilder();\n    urlBuilder.addQueryParameter(\"name\", \"zhangsan\");\n\n    String url = urlBuilder.build().toString();\n\n    Request request = new Request.Builder()\n      .url(url)\n      .build();\n    Call call = client.newCall(request);\n    Response response = call.execute();\n\n    assertThat(response.code(), equalTo(200));\n&#125;\n\n添加请求体表单请求通过FormBody我们能够构造出表单请求（application&#x2F;x-www-form-urlencoded），使用方式如下：\n@Test\npublic void whenPostRequestWithBody_thenCorrect() \n  throws IOException &#123;\n    RequestBody formBody = new FormBody.Builder()\n      .add(\"username\", \"test\")\n      .add(\"password\", \"test\")\n      .build();\n\n    Request request = new Request.Builder()\n      .url(BASE_URL + \"/users\")\n      .post(formBody)\n      .build();\n\n    Call call = client.newCall(request);\n    Response response = call.execute();\n    \n    assertThat(response.code(), equalTo(200));\n&#125;\n\n\nJSON请求为了适应目前RESTAfulAPI风格的JSON请求格式，OkHttp提供了String类型的json支持，我们可以通过一些json序列化库来帮助我们将一些实体类型转化为json后发送请求：\nprivate MediaType JSON_TYPE = MediaType.get(\"application/json; charset=utf-8\");\n\n@Test\npublic void whenPostRequestWithJSON_thenCorrect() \n  throws IOException &#123;\n    User user = new User().userId(1L).username(\"yaowenbin\");\n    String json = JSON.toJSONString(user);\n    RequestBody requestBody = RequestBody.create(JSON_TYPE, json);\n\n    Request request = new Request.Builder()\n      .url(BASE_URL + \"/users\")\n      .post(formBody)\n      .build();\n\n    Call call = client.newCall(request);\n    Response response = call.execute();\n    \n    assertThat(response.code(), equalTo(200));\n&#125;\n\n\nPUT&#x2F;DELETE请求PUT&#x2F;DELETE和POST请求也类似，只不过把Request调用的post方法转化成了对应的put()和delete()罢了\n@Test\npublic void whenPuttRequestWithBody_thenCorrect() \n        throws IOException &#123;\n    ...\n    Request request = new Request.Builder()\n        .put(requestBody)\n        .url(BASE_URL + \"/users\")\n        .build();\n\t...\n&#125;\n\n@Test\npublic void whenPuttRequestWithBody_thenCorrect() \n        throws IOException &#123;\n    ...\n    Request request = new Request.Builder()\n        .delete(requestBody)\n        .url(BASE_URL + \"/users\")\n        .build();\n\t...\n&#125;\n\n\n请求头单次请求头我们只需要在Request的Builder中使用addHeader即可在请求中添加请求头了：\n@Test\npublic void whenSetHeader_thenCorrect() throws IOException &#123;\n    Request request = new Request.Builder()\n      .url(SAMPLE_URL)\n      .addHeader(\"Content-Type\", \"application/json\")\n      .build();\n\n    Call call = client.newCall(request);\n    Response response = call.execute();\n    response.close();\n&#125;\n\n全局请求头如果我们希望每一个发送的请求都能够携带上全局的请求参数，比如说登录Token时，OkHttp使用了Interceptor来帮助我们达到这样的效果：\n@Test\npublic void whenSetDefaultHeader_thenCorrect() \n  throws IOException &#123;\n    \n    OkHttpClient client = new OkHttpClient.Builder()\n      .addInterceptor(\n        new DefaultContentTypeInterceptor(\"application/json\"))\n      .build();\n\n    Request request = new Request.Builder()\n      .url(SAMPLE_URL)\n      .build();\n\n    Call call = client.newCall(request);\n    Response response = call.execute();\n    response.close();\n&#125;\n\n封装一个更加易用API\n\n\n\n\n\n\n\n\n对于序列化工具你想要使用FastJSON2还是Jackson还是Gson都是没有问题的\nimport com.alibaba.fastjson.JSONObject;\nimport okhttp3.*;\n\nimport java.io.IOException;\n\n/** \n * @author : yaowenbin\n */\npublic class HttpUtil &#123;\n    static OkHttpClient HTTP_CLIENT = new OkHttpClient.Builder()\n            .build();\n\n    public static final MediaType JSON\n            = MediaType.get(\"application/json; charset=utf-8\");\n\n    public static String get(String url) &#123;\n        Request request = new Request.Builder()\n                .get()\n                .url(url)\n                .build();\n\n        return synchronizedCall(request);\n    &#125;\n\n    public static &lt;T> T get(String url, Class&lt;T> clz) &#123;\n        return parse(get(url), clz);\n    &#125;\n\n    public static String post(String url, String json) &#123;\n        RequestBody requestBody = RequestBody.create(JSON, json);\n        Request request = new Request.Builder()\n                .post(requestBody)\n                .url(url)\n                .build();\n\n        return synchronizedCall(request);\n    &#125;\n    public static &lt;T> T post(String url, String json, Class&lt;T> clz) &#123;\n        return parse(post(url, json), clz);\n    &#125;\n\n    public static String put(String url, String json) &#123;\n        RequestBody body = RequestBody.create(JSON, json);\n\n        Request request = new Request.Builder()\n                .url(url)\n                .put(body)\n                .build();\n\n        return synchronizedCall(request);\n    &#125;\n\n    public static String put(String url) &#123;\n        return put(url, \"\");\n    &#125;\n\n\n    public static &lt;T> T put(String url, String json, Class&lt;T> clz) &#123;\n        return parse(put(url, json), clz);\n    &#125;\n\n\n    public static String delete(String url, String json) &#123;\n        RequestBody body = RequestBody.create(JSON, json);\n\n        Request request = new Request.Builder()\n                .url(url)\n                .delete(body)\n                .build();\n        return synchronizedCall(request);\n    &#125;\n\n    public static String delete(String url) &#123;\n        return delete(url, \"\");\n    &#125;\n\n    public static &lt;T> T delete(String url, Class&lt;T> clz) &#123;\n        return parse(\n                delete(url, \"\"),\n                clz\n        );\n    &#125;\n\n    public static &lt;T> T delete(String url, String json, Class&lt;T> clz) &#123;\n        return parse(delete(url, json), clz);\n    &#125;\n\n    private static String synchronizedCall(Request request) &#123;\n        try ( Response response = HTTP_CLIENT.newCall(request).execute() )&#123;\n            return response.body().toString();\n        &#125; catch (IOException e) &#123;\n            throw new RuntimeException(e);\n        &#125;\n    &#125;\n\n    private static&lt;T> T synchronizedCall(Request request, Class&lt;T> clz) &#123;\n        return parse(synchronizedCall(request), clz);\n    &#125;\n\n    public static &lt;T> T parse(String json, Class&lt;T> clz) &#123;\n        return JSONObject.parseObject(json, clz);\n    &#125;\n&#125;\n\n","slug":"OkHttp使用指南-封装一个好用的Http客户端","date":"2022-11-17T14:45:41.000Z","categories_index":"","tags_index":"工具,OkHttp","author_index":"姚文彬"},{"id":"c0da45d30bd1cfd1cf13bf28f1302d4d","title":"Redis BitMap学习:实现海量二值数据统计","content":"为什么要使用bitmapBitmap 的底层数据结构用的是 String 类型的 SDS 数据结构来保存位数组，把每个字节数组的 8 个 bit 位利用起来，每个 bit 位 表示一个元素的二值状态（不是 0 就是 1）。可以将 Bitmap 看成是一个 bit 为单位的数组，所以能够极大程度的节省空间。数组的每个单元只能存储 0 或者 1，数组的下标在 Bitmap 中叫做 offset 偏移量。\nbitmap如何使用bitmap在Redis2.2.0版本后就被引入了，所以我们可以直接使用。其常用操作指令有：SETBIT &lt;key&gt; &lt;offset&gt; &lt;[0,1]&gt;、GETBIT &lt;key&gt; &lt;offset&gt;、BITCOUNT &lt;key&gt;、BITPOS KEY、BITOP operation destkey key [key ...]。\n使用场景：用户打卡假设我们使用一个bitmap来统计所有的用户打卡状态，现在用户1001要打卡。那么通过SETBIT sign_status 1001 1来表示用户1001打卡；\n判断用户是否打卡通过GETBIT sign_status 1001来查看该用户是否打卡。\n查看用户每个月打卡情况如果要按照时间维度来进行统计的话，我们一般会把key设置成sign:&#123;uid&#125;:&#123;yyyyMM&#125;的格式。而对于每个key最多只会占用31个bit也就是4个字节， 非常的节省了。\n\n用户1001在2015年5月16日打卡\nSETBIT sign:1001:202105 16 1\n\n判断用户在2015年5月的打卡情况\nBITCOUNT sign:1001:202105\n\n查看用户首次打卡日期通过BITPOS，我们可以快速的得出一个BITMAP中第一个值为的offset\nBITPOS sign:1001:202105 1\n\n\n统计某个时间段的所有用户登陆次数如果在记录了一亿个用户连续7天打卡数据，如何统计出这7天连续打卡的用户总数呢？Redis提供了BITOP &lt;operation&gt; &lt;destinationKey&gt; &lt;key1&gt; [&lt;key2&gt; ...]这个指令，能够对多个BITMAP进行AND、OR、NOT、XOR的位计算。为了应付这种业务需求，我们会采用sign:&#123;yyyyMMdd&#125; &#123;uid&#125;的设计方式来实现需求。 而想要统计2022年11月01日到03日的用户连续打卡次数只需要通过指令：\nBITOP and sign:20221101to20221103 sign:20221101 sign:20221102 sign:20221103\n即可实现。\n小结当我们遇到的统计场景只需要统计数据的二值状态，比如用户是否存在、 ip 是否是黑名单、以及签到打卡统计等场景就可以考虑使用 Bitmap。只需要一个 bit 位就能表示 0 和 1。在统计海量数据的时候将大大减少内存占用。\n","slug":"Redis-BitMap学习-实现海量二值数据统计","date":"2022-11-15T01:08:44.000Z","categories_index":"","tags_index":"Reids","author_index":"姚文彬"},{"id":"738b9e0126fd231ff00bb0e09eaaa743","title":"如何使用Swagger3与OpenAPI规范","content":"swagger 是一个 api 文档维护组织，后来成为了 Open API 标准的主要定义者，现在最新的版本为17年发布的 Swagger3（Open Api3）。 国内绝大部分人还在用过时的swagger2（17年停止维护并更名为swagger3） swagger2的包名为 io.swagger，而swagger3的包名为 io.swagger.core.v3。 \n17年就停更了东西，目前Swagger2依然占大多数市场，我也不知道为啥\n相比于Swagger2，Swagger3的配置非常的简洁。只需要引入一个Starer的包，并自己定义一个配置文件即可。\n实现步骤（项目中已经实现）：\n\n引入依赖\n&lt;!--Swagger3-->\n&lt;dependency>\n  &lt;groupId>io.springfox&lt;/groupId>\n  &lt;artifactId>springfox-boot-starter&lt;/artifactId>\n  &lt;version>3.0.0&lt;/version>\n&lt;/dependency>\n\n定义配置文件并使用spring.factories自动装配\nimport io.swagger.annotations.ApiOperation;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\nimport springfox.documentation.builders.ApiInfoBuilder;\nimport springfox.documentation.builders.PathSelectors;\nimport springfox.documentation.builders.RequestHandlerSelectors;\nimport springfox.documentation.service.ApiInfo;\nimport springfox.documentation.service.Contact;\nimport springfox.documentation.spi.DocumentationType;\nimport springfox.documentation.spring.web.plugins.Docket;\n\n@Configuration\npublic class Swagger3Config &#123;\n    @Bean\n    public Docket createRestApi() &#123;\n        return new Docket(DocumentationType.OAS_30)\n                .apiInfo(apiInfo())\n                .select()\n                .apis(RequestHandlerSelectors.withMethodAnnotation(ApiOperation.class))\n                .paths(PathSelectors.any())\n                .build();\n    &#125;\n    private ApiInfo apiInfo() &#123;\n        return new ApiInfoBuilder()\n                .title(\"Swagger3接口文档\")\n                .description(\"房易租-租房管理系统\")\n                .contact(new Contact(\"房易租\", \"http://www.baidu.com\",\"ywb992134@163.com\"))\n                .version(\"1.0\")\n                .build();\n    &#125;\n&#125;\n\norg.springframework.boot.autoconfigure.EnableAutoConfiguration = \\\n  com.fangyz.swagger.config.Swagger3Config\n\n\n在对应的Controller类上添加@Api和方法上添加@ApiOperation即可\n\n\n需要说明的是：Swagger会造成严重的代码侵入，因为每个Controller的接口都需要@ApiOperation，但是这也是最方便生成API文档的方法了。所以对于没有时间写API文档的小项目来说，Swagger还是占有非常重要的地位的。\n为什么Swagger3不需要在启动类上@OpenApi（Swagger3中的启动类注解）？因为在Swagger3依赖包中，我们可以找到一个spring.factories，，熟悉Spring Boot的同学都知道这个是一个Spring Boot 特有的SPI文件，能够自动的发现并注册Starter组件的配置。里面有这样的配置：\n# Auto Configure\norg.springframework.boot.autoconfigure.EnableAutoConfiguration=\\\nspringfox.boot.starter.autoconfigure.OpenApiAutoConfiguration\n其OpenApiAutoConfiguration具体的实现源码就是\n@Configuration\n@EnableConfigurationProperties(SpringfoxConfigurationProperties.class)\n// 下面这个是关键默认值就是true，也就是通过这个为我们完成了自动配置\n@ConditionalOnProperty(value = \"springfox.documentation.enabled\", havingValue = \"true\", matchIfMissing = true)\n@Import(&#123;\n    OpenApiDocumentationConfiguration.class,\n    SpringDataRestConfiguration.class,\n    BeanValidatorPluginsConfiguration.class,\n    Swagger2DocumentationConfiguration.class,\n    SwaggerUiWebFluxConfiguration.class,\n    SwaggerUiWebMvcConfiguration.class\n&#125;)\n@AutoConfigureAfter(&#123; WebMvcAutoConfiguration.class, JacksonAutoConfiguration.class,\n    HttpMessageConvertersAutoConfiguration.class, RepositoryRestMvcAutoConfiguration.class &#125;)\npublic class OpenApiAutoConfiguration &#123;\n \n&#125;\n在@EnableOpenApi中我们也可以发现其就是为我们导入了OpenApiAutoConfiguration这个类\n@Retention(RetentionPolicy.RUNTIME)\n@Target(&#123;ElementType.TYPE&#125;)\n@Documented\n@Import(&#123;OpenApiDocumentationConfiguration.class&#125;)\npublic @interface EnableOpenApi &#123;\n&#125;\nSwagger2中的那个Enable巴拉巴拉的也是这样，不过Swagger2并不会为我们自动配置，需要我们手动开启而已\n","slug":"如何使用Swagger3与OpenAPI规范","date":"2022-11-13T08:09:01.000Z","categories_index":"","tags_index":"OpenAPI","author_index":"姚文彬"},{"id":"09b0502743a194205f4a0a60c94f4ead","title":"MySQL深分页问题及解决方案","content":"在MySQL中的需要分页操作我们会使用limit关键字加上偏移量来实现。但实际上limit 10000， 10分页关键字的实际原理是，查询出10000 + 10行数据后，舍弃前面的10000行数据，然后取最后的10行数据。\n而要想优化这种分页查询，我们会尽可能的使用覆盖索引扫描 + 延迟关联来解决，具体的方案就是：\nSELECT * FROM `audit` INNER JOIN \n\t(SELECT id FROM `audit` ORDER BY `create_time` LIMIT 10000, 10) AS lim \nUSING(id);\n这样做的原理就是让执行器在二级索引中树找到10行数据，然后返回到聚集索引树取出所有记录后返回。\n如果是普通的LIMIT语句的话，那么其MySQL执行流程是这样的：\n\n会在二级索引树中去找出过滤出10010行数据。\n然后根据这10010行数据去聚集索引树中找到这10010行数据的取出（回表）。\n舍弃前10000万数据返回，取后10行数据返回（舍弃）。\n\n","slug":"MySQL深分页问题及解决方案","date":"2022-11-12T13:28:38.000Z","categories_index":"","tags_index":"MySQL","author_index":"姚文彬"},{"id":"f09369e68b8b45b4d7396ee196bab5db","title":"MySQL索引到底什么场景会失效？","content":"MySQL在一些场景下会出现设置了索引也会失效的情况。\n1. 数据量过小当MySQL的数据量只有几行十几行的时候，这时候MySQL可能会认为全表扫描的效率甚至高于使用索引扫描，所以产生索引失效。\n数据量过小的情况无论是否使用索引速度都很快，所以无需优化。\n2. 当表过大在MySQL之前的版本中，如果一个索引的返回数据行范围超过了全表的30%，那么MySQL同样会认为全表扫描的效率可能更好，导致了不走索引。 而目前这个基于30的百分比变成了更加复杂的判断条件，会基于表的大小、数据行数和IO块大小来判断。\n这时候的解决方案有两个\n\n如果是MySQL误判的话，使用analy table来重新对表进行分析。\n如果是索引的返回数据量真的超过了MySQL的阈值，但我们依然想要走索引的话可以使用force index来强制某条SQL语句使用索引。\n\n错误结论：使用了or语句且左右是不相同的列MySQL推出了索引合并的概念，来提高or语句时左右是不同列时的查询。  当查询的where语句下包含了or，并且or连接两个不同的索引列的时候， 我们使用explain能够观察到。 其会使用一个名为index_merget的索引类型。\n\n\n\n\n\n\n\n\n\nMySQL索引合并的失效场景：多表连接、较为复杂的OR和AND查询和全文索引。\nselect * from `user` where id = 1 or name = 2;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nunion会对结果进行去重，导致额外的开销。如果能够确保查询结果不会有重复则使用union all性能更高。\n参考文章：MySQL8.0官方文档: 索引合并优化\n","slug":"MySQL索引到底什么场景会失效？","date":"2022-11-12T13:28:26.000Z","categories_index":"","tags_index":"MySQL","author_index":"姚文彬"},{"id":"7f627cb8c8effb7ae31a975e2ea25f37","title":"为什么MySQL不使用Hash作为索引类型","content":"因为Hash这个数据结构的特征，导致每个key最后的哈希值是无序的。所以\n\n使用Hash作为索引类型的使用无法使用任何的范围查询。\n也无法使用像b+树那样满足like语句。\n优化器也无法将hash作用于order by操作符\n\n","slug":"为什么MySQL不使用Hash作为索引类型","date":"2022-11-12T13:26:39.000Z","categories_index":"","tags_index":"MySQL","author_index":"姚文彬"},{"id":"4ddb3a53c2271811530493a47c276c44","title":"Java应届生面试复盘","content":"\n\n\n主要内容\n日期 &#x2F; 地点\n面试结果\n\n\n\nJava基础、并发编程、JVM、MQ、业务设计。\n2022-11-05 线上\n一般\n\n\n业务设计\n\n\n\n\n\n\n\n\n说说业务设计时你会考虑什么\n首先，我会先确认好业务需求，明确这个功能会接受什么样的输入，产生什么样的输出。然后跟上游沟通好是否需要进行额外的特殊处理，比如说异步、幂等性或者高并发。然后根据功能的大小将其为多个明确的子功能。然后以验收测试驱动开发的思想，我会先准备好子功能的测试用例（至少包含正常状态和异常输入状态）。对每个子功能进行详细的思路设计（在大脑中找出实现的方案，然后找出最短的可行路径），编写单元测试。实现代码。\nJava基础\n\n\n\n\n\n\n\n\nequals和&#x3D;&#x3D;的区别\n对于基本类型，&#x3D;&#x3D;判断两个值是否相等，euqals方法不存在于基本类型中；对于引用类型，&#x3D;&#x3D;判断两个引用是否指向同一个对象，equals用于比较两个对象是否等价；Object类中的equals方法默认比较的就是地址。\n\n\n\n\n\n\n\n\n\n为什么说重写equals后要重写hashcode方法\nJava之所以使用了equals和hashCode两个方法来判断两个对象是否相等，是出于可靠性和性能。\n\nequals用于比较两个对象是否绝对相等。\nhashCode用于快速判断两个对象是否相等，由于哈希碰撞可能会出现误差。\n\n我们对于这两个方法会有以下约定：\n\n两个对象的euqals相等，hashCode一定相等。\n两个对象的hashCode相等，euqals方法不一定相等。\n\n如果只重写了equals而没有重写hashCode方法，会导致new出了两个相同属性的对象，其调用equals方法之后是相等的，但是他们hashCode方法继承于Object类，是比较两个对象的内存地址，散列值不同。这就违背了equals的可靠性\n\n\n\n\n\n\n\n\n\nArrayList和LinkedList的区别\nArrayList是基于数组这个数据结构实现的，LinkedList是基于链表实现的。前者在随机访问和尾部操作时的效率高，后者在插入和删除时的效率高，但链表的内存占用比较大。而LinkedList的一个优势是其使用了Deque接口，这就意味着其可以作为栈、队列两个数据结构的具体实现。\n\n\n\n\n\n\n\n\n\n并发场景下如何保证列表的线程安全\nJDK中提供了三种线程安全的列表：Vector、syncronizedList和copyOnWriteList\n\nVector是从JDK1.0开始就提供了线程安全列表，但是其实现方式十分的粗暴。就是通过在对外提供的public的方法上添加上syncronized关键字来实现的，所以性能较低。\nCollections.synchronizedList(List list) 是 JDK1.2 版本后推出的Collections工具类中的API，能够返回指定参数列表的线程安全版本。 其实现方式是通过减少syncronized的同步粒度来优化性能。\ncopyOnWriteList 是从JDK1.5版本后由juc工具包的开发大佬Doug Lea编写的基于写时复制机制实现的线程安全列表。目的在于将列表的读性能发挥极致，在类的使用过程中读和写操作都互不干涉。在写的时候会从原数组中复制出一个新的数组，然后对新数组进行写操作后再将其覆盖回原数组。而此时如果发生了读操作就只能够读取到旧的数组，由于读写操作位于不同的数组上，所以不会发生线程安全问题。\n\n\n\n\n\n\n\n\n\n\nJava的错误机制\nThrowable是Java中所有异常和错误的父类。其中包含了printStackTrace()接口用于获取堆栈信息。\n而Throwable的有两个子类：Error和Exception。Error是程序无法处理的错误，一般是由于运行程序中出现了严重错误。一般表示JVM在运行时出现问题，比如NoClassDefFoundError、OutOfMemoryError和StackOverFlow Error。\nException是程序本身可以捕获并且处理的异常，又分为两个子类：RuntimeException和编译器异常（Java中没有明确的定义类）RuntimeException是程序运行中才会出现的异常，比如说数组下标越界、空指针异常等。这类异常一般是由于程序逻辑错误导致的。\n编译器异常从程序语法角度讲是必须处理的异常，如果不能处理编译就不能通过。 比如IOException、SQLException。所以这类异常也叫做受检异常， 受检异常虽然是异常状态，但是从某种程度上他是可以预知的，所以Java编译器会对其进行检查，如果出现这类异常要么使用try-catch捕获他，要么使用throws声明抛出它，否则将会编译不通过。\n\n\n\n\n\n\n\n\n\nJava包装类的缓存机制\nJDK会为Boolean、Byte、Short、Integer、Character包装类中的值进行缓存，Integer中默认缓存池范围是-128～127。如果在缓存池范围内的包装类对象调用valueOf方法的话就取出缓存池中的值，而不是创建一个新对象了。这样做就能够节约内存并且提高运行效率，因为对象已经创建好了就无需再重复创建。\nJava并发编程\n\n\n\n\n\n\n\n\n并发编程三要素\n原子性、可见行和有序性。\n\n\n\n\n\n\n\n\n\n线程池中的线程出现了异常会发生什么？\n当线程池中的线程出现异常时，会被Executors框架捕捉到，对于Runnable接口的类会被隐藏；对于Callable方法在调用其返回值Future对象的get方法时，Executors会将异常信息传递给调用者。\nJVM\n\n\n\n\n\n\n\n\n泛型的作用是什么？JVM是如何实现泛型特性的？\n泛型的作用是参数化数据类型，在泛型使用过程中，操作的数据类型被指定为一个参数。JDK1.5在版本之后推出的新特性，考虑到向前兼容这个特性，所以使用到的实现方案是擦除法，本质上就是一个语法糖，参数类型会在代码在编译后就被擦除，就像没有泛型一样。\nTODO：其他语言中泛型的解决方案。\n\n\n\n\n\n\n\n\n\n volatile关键字的作用\nvolatile关键字的作用是保持变量的可见性和有序性和原子性。\n\n有序性是指防止操作系统对指令进行重排，比较经典的例子就是单例模式的双重检查，因为实例化对象的步骤可以分为：分配内存空间、初始化对象和内存空间地址赋值给引用。 但是OS会对指令进行重排序，导致实际执行顺序不同，为了防止这个重排序，所以我们需要在使用volatile关键字修饰单例对象。\n可见性问题是指当一个线程修改了共享变量而另外一个线程看不到。 这个问题的主要原因是因为没个线程都拥有自己的高速缓存区 – 线程工作内存。 而volatile关键字能够解决这个问题：当变量发生变化时，会立即刷回主内存中，并及时通知其他线程的缓存更新。\n原子性，volatile关键字能够保证单次读&#x2F;写的原子性。 long和double两种数据类型是64位的，而对于32位的JVM来说，会将其分为高32位和低32位进行缓存。因此普通的long或double类型的读&#x2F;写操作不是原子的。\n（目前各个平台的商用虚拟机包括HotSpot都会将64位数据的读写操作作为原子操作来对待，因此不使用volatile进行修饰也不会出错的）\n\n\n\n\n\n\n\n\n\n\n\n\nJVM内存屏障是什么？ 如何实现？\n\n\n\n\n\n\n\n\n\nJVM如何保证volatile的特性的？\nJVM是通过内存屏障来保证volatile的特性的。内存屏障是一个CPU指令，会告诉编译器和CPU不管什么指令都不能和这条Memory Barrier指令重排序。\n对于可见性：从编译后的代码来看，对于volatile关键字变量的写操作前会添加上lock前缀，这个前缀会发生两件事：将当前处理器的缓存行数据写回系统内存。通知总线该变量被修改了。而其他处理器通过嗅探总线上传播的数据来检查自己缓存的值是不是过期了， 如果过期了就将数据设置为无效状态。通过这样的机制使得每个线程都能够获得该变量的最新值。\n对于有序性：编译器会在生成指令序列时在适当的位置插入内存屏障指令来禁止特定类型的处理器重排序。对于volatile变量\n\n在每一个读操作后插入LoadLoad和LoadStore屏障。 \n在每一个写操作前插入StoreStore屏障，写操作后插入StoreLoad屏障。\n\n\n\n\n\n\n\n\n\n\nJVM如何保证synchronized的同步的？\nsynchronized是Java中进行同步操作最基本的关键字，可以作用在方法和代码块中。对于被syncronized修饰的方法或者代码块，编译器会在前后加上monitorenter和monitorexit两个指令。monitor可以看作是一把锁，每个对象在同一时间只能和一个monitor关联，且每个monitor只能够被一个线程获得。对于monitor的实现和可重入锁类似。monitorenter是否有这个monitor的所有权，如果有则表示重入锁，则计数器加一。若果没有，会检查计数器的次数，如果为0表示这个锁还没有被其他线程获得，则立即获得这把锁，并计数器加+1。对于对于monitorexit指令，就是将monitor计数器减一，如果为0则表示释放锁。\n网络\n\n\n\n\n\n\n\n\n三次握手\n三次握手是TCP的连接建立过程，与HTTP协议无关，只不过因为HTTP协议是基于于TCP实现的应用层协议。握手之前，客户端和服务端都处于CLOSE状态，然后先是服务端主动监听某个端口，处于LISTEN状态。第一次握手时，客户端初始化一个随机序列号放入报文首部的「序列号」字段中，然后将报文的SYN标志位设置为1，发送给服务端，随后客户端进入SYN_SENT状态。第二次握手时，服务端接收到客户端的SYN报文后，初始化自己的序列号，填入「序列号」字段，然后将「确认答应」字段填入客户端的seq+1，然后把SYN和ACK标志位设置为1，发送给客户端。服务端进入SYN_RCVD状态。第三次握手时，客户端收到服务端的报文后，最后回复一个答应报文，将ACK标志位设置为1，然后将服务端报文中的seq+1填入TCP报文首部的「确认答应」字段中，发送给客户端，进入ESTABLISH状态。服务端收到后也会进入ESTABLISH状态。\n\n\n\n\n\n\n\n\n\n三次握手能够附带数据吗？\n由于第三次握手时能够确认客户端和服务端能够建立连接，所以处于性能考虑，第三次握手时的报文是可以附带数据的。 而前两次握手的主要目的在于「建立连接」所以不能携带报文。\n\n\n\n\n\n\n\n\n\n为什么要三次握手？ 两次不行吗\n因为要考虑到网络阻塞的问题，当第一次握手时，客户端发送的\n\n\n\n\n\n\n\n\n\n什么是跨域问题？\n跨域问题是源于浏览器的同源策略。 这是浏览器中一个重要的安全策略，防止XSS、CSFR等浏览器层面的攻击。而同源是指：相同的协议、相同的域名和相同的端口所以当访问不同协议、不同域名、不同端口的资源时会出现跨域问题。\n当我们的项目是前后端分离架构时，前后端可能会部署在不同的服务器上，这时就会导致跨域的问题。\n\n\n\n\n\n\n\n\n\n解决跨域问题的常见方案？\n\nNginx方向代理。使用Nginx作为一层跳板机，将指定同源域名后的资源（比如说&#x2F;api）映射到具体后端服务器地址，实现跨域功能。\n\nhttp &#123;\n  server &#123;\n    location ~/api &#123;\n      # 反向代理\n      proxy_pass xxxxx; #\n    &#125;\n\n\nWebpack Server代理，可以在webpack.config.js中配置一个devServer，然后将指定资源映射到具体后端服务器的地址。\n\nmodule.exports = &#123; \n  devServer: &#123; \n    port: 3000, \n    proxy: &#123; \n      \"/api\": &#123; \n        target: \"http://localhost:3001\" \n      &#125; \n    &#125; \n  &#125;, \n&#125;;\n\n\n\n\n\n\n\n\n\n\n我们使用到的IDE：Webstorm、IDEA等是如何直接运行vue项目的？\nIntelliJ公司的IDE会内置一个轻量级的NodeJs的服务器，当我们在Configurations中配置好前端项目的package.json后会自动将前端项目编译打包放入node服务器进行运行。\nMQ\n\n\n\n\n\n\n\n\n线程池同样能够做到异步的作用，那么为什么还要使用MQ？\n第一个核心作用是削峰，比如当上游处理简单请求，平均处理速度为1w&#x2F;s，而下游处理秒杀的复杂请求，平均处理速度为2k&#x2F;s。 为了防止下游系统不被压垮，我们就需要引入MQ的削峰功能。我们就可以通过MQ的MQ-clinet模式，将下游的处理方式从MQ推送变为Client拉取。 这样下游就能够根据自己的处理能力，每隔一段时间拉取若干信息。\n第二个核心功能是解藕，这个解藕不单单是指生产者和消费者的解藕。 当上下游服务系统是异构的话，就需要MQ来帮助我们实现通讯的功能。\n参考文章：\n《图解HTTP》 作者：小林Coding\nJava全栈知识体系：https://pdai.tech/md/java/basic/java-basic-lan-basic.html\n\n","slug":"Java应届生面试复盘","date":"2022-11-07T03:07:40.000Z","categories_index":"","tags_index":"面经","author_index":"姚文彬"}]