[{"id":"db73c7a78b5189912e109ef7e8566686","title":"减少你的代码量：基于AOP实现字典翻译","content":"什么是字典？开发者更加倾向于使用短小的字段类型比如 tinyint、char(1)等来存储一些类型字段，以让每个数据都能够尽可能少的占用空间。 而用户当然不买单，希望能够看到每个字段都真正含义（比如 status &#x3D; 1时，其真正含义是状态进行中， status &#x3D; 2时，其真正含义是状态完成）而数据从1到进行中的数据变换过程我们称之为字典翻译。\n实现方案而字典是任何后台管理系统比不可少的系统功能模块之一。 而根据架构选型的不同，其内容翻译的主要实现方案有两种：\n\n基于后端查询时自动进行翻译（本文主要讨论内容）\n\n在进行查询、列表时后端能够自动对字段进行翻译，把对于status字段，能够有一个statusName（存储翻译结果的名称，随你定）来说明status的含义，并展示给用户。\n\n基于前端展示时的自动翻译\n\n在后端返回结果后，通过status字段再次去调用后端的API或者是本地的字典缓存，来获取status字段的真实含义，然后展示给用户。\n而本文的剩下内容将讨论如何基于Spring AOP来实现方案1（后端自动翻译）功能。\n使用方式\n添加字段用于填充翻译后的文本\n添加@Dict注解，并指定翻译的字典类型\n在API的返回方法中添加@DictTranslation注解class Entity&#123;\n    ...\n\t// 未翻译的字典值\n    private String type;\n\n    @Dict(EVENT_TYPE)\n    private String typeName;\n    ...\n&#125;\n\nclass Controller&#123;\n    ...\n    \n    @GetMapping(\"/detail\")\n    @DictTranslation\n    public ApiResult getById(@NotBlank @RequestParam(\"id\") String id) &#123;\n        return ApiResult.ok().data(service.getInfo(id));\n    &#125;\n    \n    ...\n&#125;\n\n\n\n\n\n\n\n\n\n\n实现结果\n\n实现步骤\n添加注解@Dict用于指定字典的元数据（字典类型和字典值）\n@Retention(RetentionPolicy.RUNTIME)\n@Target(ElementType.FIELD)\npublic @interface Dict &#123;\n\n    /**\n     * 字典类型：t_sys_dict_type中的dict_type字段.\n     */\n    String value();\n\n    String codeField() default \"\";\n&#125;\n\n\n添加注解@DictTranslation用于手动指定哪些方法会通过AOP实现自动翻译\n@Retention(RetentionPolicy.RUNTIME)\n@Target(ElementType.METHOD)\npublic @interface DictTranslation &#123;\n&#125;\n\n\n实现注解AOP切面\n/**\n * @Author yaowenbin\n * @Date 2022/11/18\n */\n@Aspect\n@Component\n@Slf4j\npublic class DictAspect &#123;\n\n    @Resource\n    private SysDictDataService dictDataService;\n\n    @Pointcut(\"@annotation(dictTranslation)\")\n    public void pointcut(DictTranslation dictTranslation) &#123;\n\n    &#125;\n\n    @Around(\"pointcut(dictTranslation)\")\n    public Object doAround(ProceedingJoinPoint pjp, DictTranslation dictTranslation) throws Throwable &#123;\n        Object proceed = pjp.proceed();\n\n        if (! (proceed instanceof ApiResult) ) &#123;\n            return proceed;\n        &#125;\n\n        ApiResult result = (ApiResult) proceed;\n        // 翻译单个对象\n        notNull(result.get(ApiResult.FIELD_DATA), val -> &#123;\n            translationDict(result.get(\"data\"));\n        &#125;);\n    \t// 翻译列表对象\n        notNull(result.get(ApiResult.FIELD_PAGE), val -> &#123;\n            PageVo pageVo = (PageVo) result.get(\"page\");\n            List dataList = pageVo.getList();\n            for (Object o : dataList) &#123;\n                translationDict(o);\n            &#125;\n        &#125;);\n\n        return proceed;\n    &#125;\n\n    public void translationDict(Object data) &#123;\n        Field[] fields = data.getClass().getDeclaredFields();\n\n        for (Field field : fields) &#123;\n            if (field.isAnnotationPresent(Dict.class)) &#123;\n                Dict dict = field.getAnnotation(Dict.class);\n\n                String codeFieldName;\n                if (dict.codeField().equals(\"\")) &#123;\n                    String fieldName = field.getName();\n                    codeFieldName = fieldName.substring(0, fieldName.length() - 4);\n                &#125; else &#123;\n                    codeFieldName = dict.codeField();\n                &#125;\n\n                char[] chars = codeFieldName.toCharArray();\n                chars[0] = toUpperCase(chars[0]);\n                codeFieldName = String.valueOf(chars);\n                String getterName = \"get\" + codeFieldName;\n\n                Method codeGetter;\n                try &#123;\n                    codeGetter = data.getClass().getMethod(getterName);\n                &#125; catch (NoSuchMethodException | SecurityException e) &#123;\n                    log.warn(\"翻译失败, &#123;&#125;未找到&#123;&#125;()方法，无法翻译该字段\", data.getClass(), getterName);\n                    continue;\n                &#125;\n                String code;\n                try &#123;\n                    code = (String) codeGetter.invoke(data);\n                &#125; catch (IllegalAccessException | InvocationTargetException e) &#123;\n                    log.warn(\"翻译失败, &#123;&#125;调用&#123;&#125;()异常\", data.getClass(), getterName);\n                    continue;\n                &#125;\n\n                DropdownVo dropdown = dictDataService.getDataByType(dict.value(), code);\n                if (dropdown == null) &#123;\n                    log.warn(\"翻译失败, &#123;&#125;类中的&#123;&#125;未找到数据库中&#123;&#125;对应字典数据\", getClass(), dict, code);\n\n                    continue;\n                &#125;\n                ReflectUtil.setFieldValue(data, field.getName(), dropdown.getLabel());\n\n            &#125;\n        &#125;\n    &#125;\n\n    public &lt;V> void notNull(V obj, Consumer&lt;V> consumer) &#123;\n        if (obj != null) &#123;\n            consumer.accept(obj);\n        &#125;\n    &#125;\n    private char toUpperCase(char c) &#123;\n        if (97 &lt;= c &amp;&amp; c&lt;= 122) &#123;\n            c ^= 32;\n        &#125;\n        return c;\n    &#125;\n\n&#125;\n\n\n","slug":"减少你的代码量：基于AOP实现字典翻译","date":"2022-11-18T09:40:43.000Z","categories_index":"","tags_index":"SpringAOP,减少你的代码量","author_index":"yaowenbin"},{"id":"4468f948c4ca4e742d657728958a113e","title":"你真的了解你的MySQL吗(MySQL基准测试)","content":"如果你没有真正的对服务器上的MySQL进行基准测试，就无法了解其真实情况到底是如何。\n\n\n\n\n\n\n\n\n\n基准测试是数据库工程师必备的技能之一，否则你如何知道自己真的在优化数据库？\n为什么需要基测？基测可以观察系统在不同压力下的行为：\n\n验证基于系统的一些假设是否符合实际情况。\n测试当前的运行情况，如果连这都不知道你如何进行优化？\n模拟比当前系统更高的负载，找出系统的平静。\n测试不同的硬件、软件操作系统配置。\n证明新采购的设备是否正确配置。\n\n基测工具针对MySQL的基准测试工具就有很多了，但是我们推荐使用sysbench（较为简单）和Percona的TPCC-MySQL（面向复杂的场景）。\nSysbech测试案例接下来的本章将着重讲解如何使用sysbench测试MySQL实例。\n下载\nDebian&#x2F;Ubuntu\ncurl -s https://packagecloud.io/install/repositories/akopytov/sysbench/script.deb.sh | sudo bash\nsudo apt -y install sysbench\n\nRHEL&#x2F;Centos\ncurl -s https://packagecloud.io/install/repositories/akopytov/sysbench/script.rpm.sh | sudo bash\nsudo yum -y install sysbench\n\nmacOS\n# Add --with-postgresql if you need PostgreSQL support\nbrew install sysbench\n\n查看脚本sysbench支持更加复杂的测试，同时自身也内嵌了一些关于OLTP系统的测试，我们可以使用find / -name oltp*.lua来查找目录，然后跳转至相应目录查看全部脚本\n\n\n\n\n\n\n\n\n\nOLTP(Online Transaction Processing) 一般指我们\n$ find / -name oltp*.lua  #查找sysbench自带的数据写入脚本的路径,后面执行命令需要用到\n\n/usr/share/sysbench/oltp_delete.lua\n/usr/share/sysbench/oltp_update_non_index.lua\n/usr/share/sysbench/oltp_read_write.lua\n/usr/share/sysbench/oltp_update_index.lua\n/usr/share/sysbench/oltp_read_only.lua\n/usr/share/sysbench/oltp_common.lua\n/usr/share/sysbench/oltp_write_only.lua\n/usr/share/sysbench/oltp_point_select.lua\n/usr/share/sysbench/oltp_insert.lua\n/usr/share/sysbench/tests/include/oltp_legacy/oltp_simple.lua\n/usr/share/sysbench/tests/include/oltp_legacy/oltp.lua\n\n$ ll /usr/share/sysbench/\ntotal 64\n-rwxr-xr-x 1 root root  1452 Apr 25  2020 bulk_insert.lua\n-rw-r--r-- 1 root root 14369 Apr 25  2020 oltp_common.lua\n-rwxr-xr-x 1 root root  1290 Apr 25  2020 oltp_delete.lua\n-rwxr-xr-x 1 root root  2415 Apr 25  2020 oltp_insert.lua\n-rwxr-xr-x 1 root root  1265 Apr 25  2020 oltp_point_select.lua\n-rwxr-xr-x 1 root root  1649 Apr 25  2020 oltp_read_only.lua\n-rwxr-xr-x 1 root root  1824 Apr 25  2020 oltp_read_write.lua\n-rwxr-xr-x 1 root root  1118 Apr 25  2020 oltp_update_index.lua\n-rwxr-xr-x 1 root root  1127 Apr 25  2020 oltp_update_non_index.lua\n-rwxr-xr-x 1 root root  1440 Apr 25  2020 oltp_write_only.lua\n-rwxr-xr-x 1 root root  1919 Apr 25  2020 select_random_points.lua\n-rwxr-xr-x 1 root root  2118 Apr 25  2020 select_random_ranges.lua\ndrwxr-xr-x 4 root root  4096 Nov  4 16:02 tests\n\n\n\n小试牛刀接下来我们将sysbench正式作用于我们的服务器， 笔者准备了一台2C4G的云服务器、MySQL5.7、默认MySQL配置。首先我们需要进入MySQL创建一个测试数据库名为sysbench_test：\n$ mysql -uroot -p\n\nmysql> create database sysbench_test\n\nmysql> quit\n\n\n对于数据库的测试，我们一般会使用到sysbench的oltp脚本， 而其中分为读、写、读写混合等多种场景。\noltp测试步骤基本上分为：准备数据(prepare) - 执行测试(run) - 清理数据(cleanup) 三个步骤。而一般我们只需要用相同的参数运行不同的命令即可。\n准备数据：\nsysbench oltp_read_write --mysql-host=127.0.0.1 --mysql-port=3306 --mysql-db=sysbench_test --mysql-user=root --mysql-password=xxxxx --table_size=100 --tables=10 --threads=20 --report-interval=10 --time=120 prepare\n\n# oltp_read_write 指定sysbench内嵌的测试脚本\n### 准备时参数\n# --mysql-host/db/user/passowrd/port 测试数据库地址/名称/用户名/密码/端口\n# --tables 指定生成的表数量　\n# --table_size 指定每张表表的数据量\n\n### 运行时桉树\n# --thread 指定测试时的线程数\n# --report-interval 指定运行时日志打印间隔\n# --time 指定测试时长\n\n\n执行测试：\n$ sysbench oltp_read_write --mysql-host=127.0.0.1 --mysql-port=3306 --mysql-db=sysbench_test --mysql-user=root --mysql-password=xxx --table_size=100 --tables=10 --threads=20 --report-interval=10 --time=120 run\nsysbench 1.0.20 (using bundled LuaJIT 2.1.0-beta2)\n\nRunning the test with following options:\nNumber of threads: 20\nReport intermediate results every 10 second(s)\nInitializing random number generator from current time\n\n\nInitializing worker threads...\n\nThreads started!\n\n[ 10s ] thds: 20 tps: 298.33 qps: 6897.07 (r/w/o: 4973.02/1270.50/653.55) lat (ms,95%): 161.51 err/s: 54.99 reconn/s: 0.00\n[ 20s ] thds: 20 tps: 325.41 qps: 7474.29 (r/w/o: 5380.34/1384.24/709.72) lat (ms,95%): 147.61 err/s: 58.80 reconn/s: 0.00\n[ 30s ] thds: 20 tps: 289.30 qps: 6699.44 (r/w/o: 4828.16/1236.99/634.29) lat (ms,95%): 164.45 err/s: 55.70 reconn/s: 0.00\n[ 40s ] thds: 20 tps: 326.50 qps: 7444.30 (r/w/o: 5354.60/1380.80/708.90) lat (ms,95%): 150.29 err/s: 55.90 reconn/s: 0.00\n[ 50s ] thds: 20 tps: 326.40 qps: 7480.10 (r/w/o: 5383.30/1385.90/710.90) lat (ms,95%): 150.29 err/s: 58.10 reconn/s: 0.00\n[ 60s ] thds: 20 tps: 338.10 qps: 7669.44 (r/w/o: 5508.03/1429.91/731.50) lat (ms,95%): 147.61 err/s: 55.30 reconn/s: 0.00\n[ 70s ] thds: 20 tps: 320.50 qps: 7305.44 (r/w/o: 5251.46/1358.39/695.59) lat (ms,95%): 153.02 err/s: 54.70 reconn/s: 0.00\n[ 80s ] thds: 20 tps: 361.80 qps: 8212.92 (r/w/o: 5901.02/1528.60/783.30) lat (ms,95%): 137.35 err/s: 59.60 reconn/s: 0.00\n[ 90s ] thds: 20 tps: 345.80 qps: 7903.20 (r/w/o: 5684.10/1467.20/751.90) lat (ms,95%): 139.85 err/s: 60.30 reconn/s: 0.00\n[ 100s ] thds: 20 tps: 351.00 qps: 8032.68 (r/w/o: 5779.16/1489.81/763.71) lat (ms,95%): 134.90 err/s: 61.70 reconn/s: 0.00\n[ 110s ] thds: 20 tps: 348.10 qps: 7964.29 (r/w/o: 5730.19/1476.70/757.40) lat (ms,95%): 137.35 err/s: 61.20 reconn/s: 0.00\n[ 120s ] thds: 20 tps: 343.40 qps: 7899.49 (r/w/o: 5690.09/1459.50/749.90) lat (ms,95%): 134.90 err/s: 63.10 reconn/s: 0.00\nSQL statistics:\n    queries performed:\n        read:                            654696\n        write:                           168754\n        other:                           86531\n        total:                           909981\n    transactions:                        39767  (331.22 per sec.)\n    queries:                             909981 (7579.29 per sec.)\n    ignored errors:                      6997   (58.28 per sec.)\n    reconnects:                          0      (0.00 per sec.)\n\nGeneral statistics:\n    total time:                          120.0600s\n    total number of events:              39767\n\nLatency (ms):\n         min:                                    4.99\n         avg:                                   60.36\n         max:                                  407.99\n         95th percentile:                      147.61\n         sum:                              2400503.17\n\nThreads fairness:\n    events (avg/stddev):           1988.3500/25.31\n    execution time (avg/stddev):   120.0252/0.02\n\n\n重要的参数指标为SQL statistics下的**queries(即QPS)**：达到了7579 per sec.也就是说当前MySQL在10张表100条数据的情况下面对20个线程的读写吞吐量达到了7579QPS。当然这只是在小数据小压力的情况下 ，并不能直接作为线上环境的参考。\n那么接下来让我们清理数据准备下一轮测试\n加大压力刚刚只是尝试一下sysbench的功能，这次我们开始使用较大的压力对MySQL进行读测试（一般对MySQL的读写测试应该分开单独的进行以更好的评估MySQL哪些方面需要提升）。\n\n测试操作：读\n线程数：100\n表大小：100w\n表数量：10\n测试时长：5分钟\n\n对应执行命令：\nsysbench oltp_read_only --mysql-host=127.0.0.1 --mysql-port=3306 --mysql-db=sysbench_test --mysql-user=root --mysql-password=[yourpassowrd] --table_size=1000000 --tables=10 --threads=100 --report-interval=10 --time=300 [command]\n\n测试结果：可以看到我们的MySQL在100个并发访问线程下对于10张表，每张表100w数据进行5分钟的读写测试。能够达到1w的读性能， 还是非常不错的。\n测试写性能现在让我们清理一下测试数据，最后测试一轮写性能\n\n测试操作：写\n线程数：100\n表大小：100w\n表数量：10\n测试时长：3分钟\n\n测试命令：\nsysbench oltp_write_only --mysql-host=127.0.0.1 --mysql-port=3306 --mysql-db=sysbench_test --mysql-user=root --mysql-password=[yourpassowrd] --table_size=1000000 --tables=10 --threads=100 --report-interval=10 --time=300 [command]\n\n\n测试结果：可以看到测试机器上的这个MySQL能够达到9k接近1w的写QPS。\n总结基本测试是我们了解MySQL所要掌握的必备技能之一，而sysbench是一款多线程的性能测试工具。 使用它我们就能够对MySQL的读写QPS有一个非常好的了解。\n","slug":"你真的了解你的MySQL吗-MySQL基准测试","date":"2022-11-12T13:31:33.000Z","categories_index":"","tags_index":"MySQL","author_index":"yaowenbin"},{"id":"09b0502743a194205f4a0a60c94f4ead","title":"MySQL深分页问题及解决方案","content":"在MySQL中的需要分页操作我们会使用limit关键字加上偏移量来实现。但实际上limit 10000， 10分页关键字的实际原理是，查询出10000 + 10行数据后，舍弃前面的10000行数据，然后取最后的10行数据。\n而要想优化这种分页查询，我们会尽可能的使用覆盖索引扫描 + 延迟关联来解决，具体的方案就是：\nSELECT * FROM `audit` INNER JOIN \n\t(SELECT id FROM `audit` ORDER BY `create_time` LIMIT 10000, 10) AS lim \nUSING(id);\n这样做的原理就是让执行器在二级索引中树找到10行数据，然后返回到聚集索引树取出所有记录后返回。\n如果是普通的LIMIT语句的话，那么其MySQL执行流程是这样的：\n\n会在二级索引树中去找出过滤出10010行数据。\n然后根据这10010行数据去聚集索引树中找到这10010行数据的取出（回表）。\n舍弃前10000万数据返回，取后10行数据返回（舍弃）。\n\n","slug":"MySQL深分页问题及解决方案","date":"2022-11-12T13:28:38.000Z","categories_index":"","tags_index":"MySQL","author_index":"yaowenbin"},{"id":"132bc548a30620e50cfa7c41af4da051","title":"OkHttp使用指南:封装一个好用的Http客户端","content":"OkHttp是一个高效的HTTP请求客户端对于Android和Java应用。其有着许多高级的功能，比如线程池、GZip压缩和响应缓存。 同时不仅能够发送同步的请求，也能够支持异步调用。\n&lt;dependency>\n    &lt;groupId>com.squareup.okhttp3&lt;/groupId>\n    &lt;artifactId>okhttp&lt;/artifactId>\n    &lt;version>4.9.1&lt;/version>\n&lt;/dependency>\n\n同步请求我们需要基于URL地址来构建一个Reqeust对象，然后调用newCall()来生成一个Call对象，然后调用execute()方法来同步发送请求并获取相应结果\n@Test\npublic void whenGetRequest_thenCorrect() throws IOException &#123;\n    Request request = new Request.Builder()\n      .url(BASE_URL + \"/date\")\n      .build();\n\n    Call call = client.newCall(request);\n    Response response = call.execute();\n\n    assertThat(response.code(), equalTo(200));\n&#125;\n\n\n异步请求@Test\npublic void whenAsynchronousGetRequest_thenCorrect() &#123;\n    Request request = new Request.Builder()\n      .url(BASE_URL + \"/date\")\n      .build();\n\n    Call call = client.newCall(request);\n    call.enqueue(new Callback() &#123;\n        public void onResponse(Call call, Response response) \n          throws IOException &#123;\n            // ...\n        &#125;\n        \n        public void onFailure(Call call, IOException e) &#123;\n            fail();\n        &#125;\n    &#125;);\n&#125;\n\n添加请求参数@Test\npublic void whenGetRequestWithQueryParameter_thenCorrect() \n  throws IOException &#123;\n    \n    HttpUrl.Builder urlBuilder \n      = HttpUrl.parse(BASE_URL + \"/users\").newBuilder();\n    urlBuilder.addQueryParameter(\"name\", \"zhangsan\");\n\n    String url = urlBuilder.build().toString();\n\n    Request request = new Request.Builder()\n      .url(url)\n      .build();\n    Call call = client.newCall(request);\n    Response response = call.execute();\n\n    assertThat(response.code(), equalTo(200));\n&#125;\n\n添加请求体表单请求通过FormBody我们能够构造出表单请求（application&#x2F;x-www-form-urlencoded），使用方式如下：\n@Test\npublic void whenPostRequestWithBody_thenCorrect() \n  throws IOException &#123;\n    RequestBody formBody = new FormBody.Builder()\n      .add(\"username\", \"test\")\n      .add(\"password\", \"test\")\n      .build();\n\n    Request request = new Request.Builder()\n      .url(BASE_URL + \"/users\")\n      .post(formBody)\n      .build();\n\n    Call call = client.newCall(request);\n    Response response = call.execute();\n    \n    assertThat(response.code(), equalTo(200));\n&#125;\n\n\nJSON请求为了适应目前RESTAfulAPI风格的JSON请求格式，OkHttp提供了String类型的json支持，我们可以通过一些json序列化库来帮助我们将一些实体类型转化为json后发送请求：\nprivate MediaType JSON_TYPE = MediaType.get(\"application/json; charset=utf-8\");\n\n@Test\npublic void whenPostRequestWithJSON_thenCorrect() \n  throws IOException &#123;\n    User user = new User().userId(1L).username(\"yaowenbin\");\n    String json = JSON.toJSONString(user);\n    RequestBody requestBody = RequestBody.create(JSON_TYPE, json);\n\n    Request request = new Request.Builder()\n      .url(BASE_URL + \"/users\")\n      .post(formBody)\n      .build();\n\n    Call call = client.newCall(request);\n    Response response = call.execute();\n    \n    assertThat(response.code(), equalTo(200));\n&#125;\n\n\nPUT&#x2F;DELETE请求PUT&#x2F;DELETE和POST请求也类似，只不过把Request调用的post方法转化成了对应的put()和delete()罢了\n@Test\npublic void whenPuttRequestWithBody_thenCorrect() \n        throws IOException &#123;\n    ...\n    Request request = new Request.Builder()\n        .put(requestBody)\n        .url(BASE_URL + \"/users\")\n        .build();\n\t...\n&#125;\n\n@Test\npublic void whenPuttRequestWithBody_thenCorrect() \n        throws IOException &#123;\n    ...\n    Request request = new Request.Builder()\n        .delete(requestBody)\n        .url(BASE_URL + \"/users\")\n        .build();\n\t...\n&#125;\n\n\n请求头单次请求头我们只需要在Request的Builder中使用addHeader即可在请求中添加请求头了：\n@Test\npublic void whenSetHeader_thenCorrect() throws IOException &#123;\n    Request request = new Request.Builder()\n      .url(SAMPLE_URL)\n      .addHeader(\"Content-Type\", \"application/json\")\n      .build();\n\n    Call call = client.newCall(request);\n    Response response = call.execute();\n    response.close();\n&#125;\n\n全局请求头如果我们希望每一个发送的请求都能够携带上全局的请求参数，比如说登录Token时，OkHttp使用了Interceptor来帮助我们达到这样的效果：\n@Test\npublic void whenSetDefaultHeader_thenCorrect() \n  throws IOException &#123;\n    \n    OkHttpClient client = new OkHttpClient.Builder()\n      .addInterceptor(\n        new DefaultContentTypeInterceptor(\"application/json\"))\n      .build();\n\n    Request request = new Request.Builder()\n      .url(SAMPLE_URL)\n      .build();\n\n    Call call = client.newCall(request);\n    Response response = call.execute();\n    response.close();\n&#125;\n\n封装一个更加易用API\n\n\n\n\n\n\n\n\n对于序列化工具你想要使用FastJSON2还是Jackson还是Gson都是没有问题的\nimport com.alibaba.fastjson.JSONObject;\nimport okhttp3.*;\n\nimport java.io.IOException;\n\n/** \n * @author : yaowenbin\n */\npublic class HttpUtil &#123;\n    static OkHttpClient HTTP_CLIENT = new OkHttpClient.Builder()\n            .build();\n\n    public static final MediaType JSON\n            = MediaType.get(\"application/json; charset=utf-8\");\n\n    public static String get(String url) &#123;\n        Request request = new Request.Builder()\n                .get()\n                .url(url)\n                .build();\n\n        return synchronizedCall(request);\n    &#125;\n\n    public static &lt;T> T get(String url, Class&lt;T> clz) &#123;\n        return parse(get(url), clz);\n    &#125;\n\n    public static String post(String url, String json) &#123;\n        RequestBody requestBody = RequestBody.create(JSON, json);\n        Request request = new Request.Builder()\n                .post(requestBody)\n                .url(url)\n                .build();\n\n        return synchronizedCall(request);\n    &#125;\n    public static &lt;T> T post(String url, String json, Class&lt;T> clz) &#123;\n        return parse(post(url, json), clz);\n    &#125;\n\n    public static String put(String url, String json) &#123;\n        RequestBody body = RequestBody.create(JSON, json);\n\n        Request request = new Request.Builder()\n                .url(url)\n                .put(body)\n                .build();\n\n        return synchronizedCall(request);\n    &#125;\n\n    public static String put(String url) &#123;\n        return put(url, \"\");\n    &#125;\n\n\n    public static &lt;T> T put(String url, String json, Class&lt;T> clz) &#123;\n        return parse(put(url, json), clz);\n    &#125;\n\n\n    public static String delete(String url, String json) &#123;\n        RequestBody body = RequestBody.create(JSON, json);\n\n        Request request = new Request.Builder()\n                .url(url)\n                .delete(body)\n                .build();\n        return synchronizedCall(request);\n    &#125;\n\n    public static String delete(String url) &#123;\n        return delete(url, \"\");\n    &#125;\n\n    public static &lt;T> T delete(String url, Class&lt;T> clz) &#123;\n        return parse(\n                delete(url, \"\"),\n                clz\n        );\n    &#125;\n\n    public static &lt;T> T delete(String url, String json, Class&lt;T> clz) &#123;\n        return parse(delete(url, json), clz);\n    &#125;\n\n    private static String synchronizedCall(Request request) &#123;\n        try ( Response response = HTTP_CLIENT.newCall(request).execute() )&#123;\n            return response.body().toString();\n        &#125; catch (IOException e) &#123;\n            throw new RuntimeException(e);\n        &#125;\n    &#125;\n\n    private static&lt;T> T synchronizedCall(Request request, Class&lt;T> clz) &#123;\n        return parse(synchronizedCall(request), clz);\n    &#125;\n\n    public static &lt;T> T parse(String json, Class&lt;T> clz) &#123;\n        return JSONObject.parseObject(json, clz);\n    &#125;\n&#125;\n\n","slug":"OkHttp使用指南-封装一个好用的Http客户端","date":"2022-11-17T14:45:41.000Z","categories_index":"","tags_index":"工具","author_index":"yaowenbin"},{"id":"c0da45d30bd1cfd1cf13bf28f1302d4d","title":"Redis BitMap学习:实现海量二值数据统计","content":"为什么要使用bitmapBitmap 的底层数据结构用的是 String 类型的 SDS 数据结构来保存位数组，把每个字节数组的 8 个 bit 位利用起来，每个 bit 位 表示一个元素的二值状态（不是 0 就是 1）。可以将 Bitmap 看成是一个 bit 为单位的数组，所以能够极大程度的节省空间。数组的每个单元只能存储 0 或者 1，数组的下标在 Bitmap 中叫做 offset 偏移量。\nbitmap如何使用bitmap在Redis2.2.0版本后就被引入了，所以我们可以直接使用。其常用操作指令有：SETBIT &lt;key&gt; &lt;offset&gt; &lt;[0,1]&gt;、GETBIT &lt;key&gt; &lt;offset&gt;、BITCOUNT &lt;key&gt;、BITPOS KEY、BITOP operation destkey key [key ...]。\n使用场景：用户打卡假设我们使用一个bitmap来统计所有的用户打卡状态，现在用户1001要打卡。那么通过SETBIT sign_status 1001 1来表示用户1001打卡；\n判断用户是否打卡通过GETBIT sign_status 1001来查看该用户是否打卡。\n查看用户每个月打卡情况如果要按照时间维度来进行统计的话，我们一般会把key设置成sign:&#123;uid&#125;:&#123;yyyyMM&#125;的格式。而对于每个key最多只会占用31个bit也就是4个字节， 非常的节省了。\n\n用户1001在2015年5月16日打卡\nSETBIT sign:1001:202105 16 1\n\n判断用户在2015年5月的打卡情况\nBITCOUNT sign:1001:202105\n\n查看用户首次打卡日期通过BITPOS，我们可以快速的得出一个BITMAP中第一个值为的offset\nBITPOS sign:1001:202105 1\n\n\n统计某个时间段的所有用户登陆次数如果在记录了一亿个用户连续7天打卡数据，如何统计出这7天连续打卡的用户总数呢？Redis提供了BITOP &lt;operation&gt; &lt;destinationKey&gt; &lt;key1&gt; [&lt;key2&gt; ...]这个指令，能够对多个BITMAP进行AND、OR、NOT、XOR的位计算。为了应付这种业务需求，我们会采用sign:&#123;yyyyMMdd&#125; &#123;uid&#125;的设计方式来实现需求。 而想要统计2022年11月01日到03日的用户连续打卡次数只需要通过指令：\nBITOP and sign:20221101to20221103 sign:20221101 sign:20221102 sign:20221103\n即可实现。\n小结当我们遇到的统计场景只需要统计数据的二值状态，比如用户是否存在、 ip 是否是黑名单、以及签到打卡统计等场景就可以考虑使用 Bitmap。只需要一个 bit 位就能表示 0 和 1。在统计海量数据的时候将大大减少内存占用。\n","slug":"Redis-BitMap学习-实现海量二值数据统计","date":"2022-11-15T01:08:44.000Z","categories_index":"","tags_index":"Reids","author_index":"yaowenbin"},{"id":"738b9e0126fd231ff00bb0e09eaaa743","title":"如何使用Swagger3与OpenAPI规范","content":"swagger 是一个 api 文档维护组织，后来成为了 Open API 标准的主要定义者，现在最新的版本为17年发布的 Swagger3（Open Api3）。 国内绝大部分人还在用过时的swagger2（17年停止维护并更名为swagger3） swagger2的包名为 io.swagger，而swagger3的包名为 io.swagger.core.v3。 \n17年就停更了东西，目前Swagger2依然占大多数市场，我也不知道为啥\n相比于Swagger2，Swagger3的配置非常的简洁。只需要引入一个Starer的包，并自己定义一个配置文件即可。\n实现步骤（项目中已经实现）：\n\n引入依赖\n&lt;!--Swagger3-->\n&lt;dependency>\n  &lt;groupId>io.springfox&lt;/groupId>\n  &lt;artifactId>springfox-boot-starter&lt;/artifactId>\n  &lt;version>3.0.0&lt;/version>\n&lt;/dependency>\n\n定义配置文件并使用spring.factories自动装配\nimport io.swagger.annotations.ApiOperation;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\nimport springfox.documentation.builders.ApiInfoBuilder;\nimport springfox.documentation.builders.PathSelectors;\nimport springfox.documentation.builders.RequestHandlerSelectors;\nimport springfox.documentation.service.ApiInfo;\nimport springfox.documentation.service.Contact;\nimport springfox.documentation.spi.DocumentationType;\nimport springfox.documentation.spring.web.plugins.Docket;\n\n@Configuration\npublic class Swagger3Config &#123;\n    @Bean\n    public Docket createRestApi() &#123;\n        return new Docket(DocumentationType.OAS_30)\n                .apiInfo(apiInfo())\n                .select()\n                .apis(RequestHandlerSelectors.withMethodAnnotation(ApiOperation.class))\n                .paths(PathSelectors.any())\n                .build();\n    &#125;\n    private ApiInfo apiInfo() &#123;\n        return new ApiInfoBuilder()\n                .title(\"Swagger3接口文档\")\n                .description(\"房易租-租房管理系统\")\n                .contact(new Contact(\"房易租\", \"http://www.baidu.com\",\"ywb992134@163.com\"))\n                .version(\"1.0\")\n                .build();\n    &#125;\n&#125;\n\norg.springframework.boot.autoconfigure.EnableAutoConfiguration = \\\n  com.fangyz.swagger.config.Swagger3Config\n\n\n在对应的Controller类上添加@Api和方法上添加@ApiOperation即可\n\n\n需要说明的是：Swagger会造成严重的代码侵入，因为每个Controller的接口都需要@ApiOperation，但是这也是最方便生成API文档的方法了。所以对于没有时间写API文档的小项目来说，Swagger还是占有非常重要的地位的。\n为什么Swagger3不需要在启动类上@OpenApi（Swagger3中的启动类注解）？因为在Swagger3依赖包中，我们可以找到一个spring.factories，，熟悉Spring Boot的同学都知道这个是一个Spring Boot 特有的SPI文件，能够自动的发现并注册Starter组件的配置。里面有这样的配置：\n# Auto Configure\norg.springframework.boot.autoconfigure.EnableAutoConfiguration=\\\nspringfox.boot.starter.autoconfigure.OpenApiAutoConfiguration\n其OpenApiAutoConfiguration具体的实现源码就是\n@Configuration\n@EnableConfigurationProperties(SpringfoxConfigurationProperties.class)\n// 下面这个是关键默认值就是true，也就是通过这个为我们完成了自动配置\n@ConditionalOnProperty(value = \"springfox.documentation.enabled\", havingValue = \"true\", matchIfMissing = true)\n@Import(&#123;\n    OpenApiDocumentationConfiguration.class,\n    SpringDataRestConfiguration.class,\n    BeanValidatorPluginsConfiguration.class,\n    Swagger2DocumentationConfiguration.class,\n    SwaggerUiWebFluxConfiguration.class,\n    SwaggerUiWebMvcConfiguration.class\n&#125;)\n@AutoConfigureAfter(&#123; WebMvcAutoConfiguration.class, JacksonAutoConfiguration.class,\n    HttpMessageConvertersAutoConfiguration.class, RepositoryRestMvcAutoConfiguration.class &#125;)\npublic class OpenApiAutoConfiguration &#123;\n \n&#125;\n在@EnableOpenApi中我们也可以发现其就是为我们导入了OpenApiAutoConfiguration这个类\n@Retention(RetentionPolicy.RUNTIME)\n@Target(&#123;ElementType.TYPE&#125;)\n@Documented\n@Import(&#123;OpenApiDocumentationConfiguration.class&#125;)\npublic @interface EnableOpenApi &#123;\n&#125;\nSwagger2中的那个Enable巴拉巴拉的也是这样，不过Swagger2并不会为我们自动配置，需要我们手动开启而已\n","slug":"如何使用Swagger3与OpenAPI规范","date":"2022-11-13T08:09:01.000Z","categories_index":"","tags_index":"OpenAPI","author_index":"yaowenbin"},{"id":"f09369e68b8b45b4d7396ee196bab5db","title":"MySQL索引到底什么场景会失效？","content":"MySQL在一些场景下会出现设置了索引也会失效的情况。\n1. 数据量过小当MySQL的数据量只有几行十几行的时候，这时候MySQL可能会认为全表扫描的效率甚至高于使用索引扫描，所以产生索引失效。\n数据量过小的情况无论是否使用索引速度都很快，所以无需优化。\n2. 当表过大在MySQL之前的版本中，如果一个索引的返回数据行范围超过了全表的30%，那么MySQL同样会认为全表扫描的效率可能更好，导致了不走索引。 而目前这个基于30的百分比变成了更加复杂的判断条件，会基于表的大小、数据行数和IO块大小来判断。\n这时候的解决方案有两个\n\n如果是MySQL误判的话，使用analy table来重新对表进行分析。\n如果是索引的返回数据量真的超过了MySQL的阈值，但我们依然想要走索引的话可以使用force index来强制某条SQL语句使用索引。\n\n错误结论：使用了or语句且左右是不相同的列MySQL推出了索引合并的概念，来提高or语句时左右是不同列时的查询。  当查询的where语句下包含了or，并且or连接两个不同的索引列的时候， 我们使用explain能够观察到。 其会使用一个名为index_merget的索引类型。\n\n\n\n\n\n\n\n\n\nMySQL索引合并的失效场景：多表连接、较为复杂的OR和AND查询和全文索引。\nselect * from `user` where id = 1 or name = 2;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nunion会对结果进行去重，导致额外的开销。如果能够确保查询结果不会有重复则使用union all性能更高。\n参考文章：MySQL8.0官方文档: 索引合并优化\n","slug":"MySQL索引到底什么场景会失效？","date":"2022-11-12T13:28:26.000Z","categories_index":"","tags_index":"MySQL","author_index":"yaowenbin"},{"id":"7f627cb8c8effb7ae31a975e2ea25f37","title":"为什么MySQL不使用Hash作为索引类型","content":"因为Hash这个数据结构的特征，导致每个key最后的哈希值是无序的。所以\n\n使用Hash作为索引类型的使用无法使用任何的范围查询。\n也无法使用像b+树那样满足like语句。\n优化器也无法将hash作用于order by操作符\n\n","slug":"为什么MySQL不使用Hash作为索引类型","date":"2022-11-12T13:26:39.000Z","categories_index":"","tags_index":"MySQL","author_index":"yaowenbin"},{"id":"4ddb3a53c2271811530493a47c276c44","title":"Java应届生面试复盘","content":"\n\n\n主要内容\n日期 &#x2F; 地点\n面试结果\n\n\n\nJava基础、并发编程、JVM、MQ、业务设计。\n2022-11-05 线上\n一般\n\n\n业务设计\n\n\n\n\n\n\n\n\n说说业务设计时你会考虑什么\n首先，我会先确认好业务需求，明确这个功能会接受什么样的输入，产生什么样的输出。然后跟上游沟通好是否需要进行额外的特殊处理，比如说异步、幂等性或者高并发。然后根据功能的大小将其为多个明确的子功能。然后以验收测试驱动开发的思想，我会先准备好子功能的测试用例（至少包含正常状态和异常输入状态）。对每个子功能进行详细的思路设计（在大脑中找出实现的方案，然后找出最短的可行路径），编写单元测试。实现代码。\nJava基础\n\n\n\n\n\n\n\n\nequals和&#x3D;&#x3D;的区别\n对于基本类型，&#x3D;&#x3D;判断两个值是否相等，euqals方法不存在于基本类型中；对于引用类型，&#x3D;&#x3D;判断两个引用是否指向同一个对象，equals用于比较两个对象是否等价；Object类中的equals方法默认比较的就是地址。\n\n\n\n\n\n\n\n\n\n为什么说重写equals后要重写hashcode方法\nJava之所以使用了equals和hashCode两个方法来判断两个对象是否相等，是出于可靠性和性能。\n\nequals用于比较两个对象是否绝对相等。\nhashCode用于快速判断两个对象是否相等，由于哈希碰撞可能会出现误差。\n\n我们对于这两个方法会有以下约定：\n\n两个对象的euqals相等，hashCode一定相等。\n两个对象的hashCode相等，euqals方法不一定相等。\n\n如果只重写了equals而没有重写hashCode方法，会导致new出了两个相同属性的对象，其调用equals方法之后是相等的，但是他们hashCode方法继承于Object类，是比较两个对象的内存地址，散列值不同。这就违背了equals的可靠性\n\n\n\n\n\n\n\n\n\nArrayList和LinkedList的区别\nArrayList是基于数组这个数据结构实现的，LinkedList是基于链表实现的。前者在随机访问和尾部操作时的效率高，后者在插入和删除时的效率高，但链表的内存占用比较大。而LinkedList的一个优势是其使用了Deque接口，这就意味着其可以作为栈、队列两个数据结构的具体实现。\n\n\n\n\n\n\n\n\n\n并发场景下如何保证列表的线程安全\nJDK中提供了三种线程安全的列表：Vector、syncronizedList和copyOnWriteList\n\nVector是从JDK1.0开始就提供了线程安全列表，但是其实现方式十分的粗暴。就是通过在对外提供的public的方法上添加上syncronized关键字来实现的，所以性能较低。\nCollections.synchronizedList(List list) 是 JDK1.2 版本后推出的Collections工具类中的API，能够返回指定参数列表的线程安全版本。 其实现方式是通过减少syncronized的同步粒度来优化性能。\ncopyOnWriteList 是从JDK1.5版本后由juc工具包的开发大佬Doug Lea编写的基于写时复制机制实现的线程安全列表。目的在于将列表的读性能发挥极致，在类的使用过程中读和写操作都互不干涉。在写的时候会从原数组中复制出一个新的数组，然后对新数组进行写操作后再将其覆盖回原数组。而此时如果发生了读操作就只能够读取到旧的数组，由于读写操作位于不同的数组上，所以不会发生线程安全问题。\n\n\n\n\n\n\n\n\n\n\nJava的错误机制\nThrowable是Java中所有异常和错误的父类。其中包含了printStackTrace()接口用于获取堆栈信息。\n而Throwable的有两个子类：Error和Exception。Error是程序无法处理的错误，一般是由于运行程序中出现了严重错误。一般表示JVM在运行时出现问题，比如NoClassDefFoundError、OutOfMemoryError和StackOverFlow Error。\nException是程序本身可以捕获并且处理的异常，又分为两个子类：RuntimeException和编译器异常（Java中没有明确的定义类）RuntimeException是程序运行中才会出现的异常，比如说数组下标越界、空指针异常等。这类异常一般是由于程序逻辑错误导致的。\n编译器异常从程序语法角度讲是必须处理的异常，如果不能处理编译就不能通过。 比如IOException、SQLException。所以这类异常也叫做受检异常， 受检异常虽然是异常状态，但是从某种程度上他是可以预知的，所以Java编译器会对其进行检查，如果出现这类异常要么使用try-catch捕获他，要么使用throws声明抛出它，否则将会编译不通过。\n\n\n\n\n\n\n\n\n\nJava包装类的缓存机制\nJDK会为Boolean、Byte、Short、Integer、Character包装类中的值进行缓存，Integer中默认缓存池范围是-128～127。如果在缓存池范围内的包装类对象调用valueOf方法的话就取出缓存池中的值，而不是创建一个新对象了。这样做就能够节约内存并且提高运行效率，因为对象已经创建好了就无需再重复创建。\nJava并发编程\n\n\n\n\n\n\n\n\n并发编程三要素\n原子性、可见行和有序性。\n\n\n\n\n\n\n\n\n\n线程池中的线程出现了异常会发生什么？\n当线程池中的线程出现异常时，会被Executors框架捕捉到，对于Runnable接口的类会被隐藏；对于Callable方法在调用其返回值Future对象的get方法时，Executors会将异常信息传递给调用者。\nJVM\n\n\n\n\n\n\n\n\n泛型的作用是什么？JVM是如何实现泛型特性的？\n泛型的作用是参数化数据类型，在泛型使用过程中，操作的数据类型被指定为一个参数。JDK1.5在版本之后推出的新特性，考虑到向前兼容这个特性，所以使用到的实现方案是擦除法，本质上就是一个语法糖，参数类型会在代码在编译后就被擦除，就像没有泛型一样。\nTODO：其他语言中泛型的解决方案。\n\n\n\n\n\n\n\n\n\n volatile关键字的作用\nvolatile关键字的作用是保持变量的可见性和有序性和原子性。\n\n有序性是指防止操作系统对指令进行重排，比较经典的例子就是单例模式的双重检查，因为实例化对象的步骤可以分为：分配内存空间、初始化对象和内存空间地址赋值给引用。 但是OS会对指令进行重排序，导致实际执行顺序不同，为了防止这个重排序，所以我们需要在使用volatile关键字修饰单例对象。\n可见性问题是指当一个线程修改了共享变量而另外一个线程看不到。 这个问题的主要原因是因为没个线程都拥有自己的高速缓存区 – 线程工作内存。 而volatile关键字能够解决这个问题：当变量发生变化时，会立即刷回主内存中，并及时通知其他线程的缓存更新。\n原子性，volatile关键字能够保证单次读&#x2F;写的原子性。 long和double两种数据类型是64位的，而对于32位的JVM来说，会将其分为高32位和低32位进行缓存。因此普通的long或double类型的读&#x2F;写操作不是原子的。\n（目前各个平台的商用虚拟机包括HotSpot都会将64位数据的读写操作作为原子操作来对待，因此不使用volatile进行修饰也不会出错的）\n\n\n\n\n\n\n\n\n\n\n\n\nJVM内存屏障是什么？ 如何实现？\n\n\n\n\n\n\n\n\n\nJVM如何保证volatile的特性的？\nJVM是通过内存屏障来保证volatile的特性的。内存屏障是一个CPU指令，会告诉编译器和CPU不管什么指令都不能和这条Memory Barrier指令重排序。\n对于可见性：从编译后的代码来看，对于volatile关键字变量的写操作前会添加上lock前缀，这个前缀会发生两件事：将当前处理器的缓存行数据写回系统内存。通知总线该变量被修改了。而其他处理器通过嗅探总线上传播的数据来检查自己缓存的值是不是过期了， 如果过期了就将数据设置为无效状态。通过这样的机制使得每个线程都能够获得该变量的最新值。\n对于有序性：编译器会在生成指令序列时在适当的位置插入内存屏障指令来禁止特定类型的处理器重排序。对于volatile变量\n\n在每一个读操作后插入LoadLoad和LoadStore屏障。 \n在每一个写操作前插入StoreStore屏障，写操作后插入StoreLoad屏障。\n\n\n\n\n\n\n\n\n\n\nJVM如何保证synchronized的同步的？\nsynchronized是Java中进行同步操作最基本的关键字，可以作用在方法和代码块中。对于被syncronized修饰的方法或者代码块，编译器会在前后加上monitorenter和monitorexit两个指令。monitor可以看作是一把锁，每个对象在同一时间只能和一个monitor关联，且每个monitor只能够被一个线程获得。对于monitor的实现和可重入锁类似。monitorenter是否有这个monitor的所有权，如果有则表示重入锁，则计数器加一。若果没有，会检查计数器的次数，如果为0表示这个锁还没有被其他线程获得，则立即获得这把锁，并计数器加+1。对于对于monitorexit指令，就是将monitor计数器减一，如果为0则表示释放锁。\n网络\n\n\n\n\n\n\n\n\n三次握手\n三次握手是TCP的连接建立过程，与HTTP协议无关，只不过因为HTTP协议是基于于TCP实现的应用层协议。握手之前，客户端和服务端都处于CLOSE状态，然后先是服务端主动监听某个端口，处于LISTEN状态。第一次握手时，客户端初始化一个随机序列号放入报文首部的「序列号」字段中，然后将报文的SYN标志位设置为1，发送给服务端，随后客户端进入SYN_SENT状态。第二次握手时，服务端接收到客户端的SYN报文后，初始化自己的序列号，填入「序列号」字段，然后将「确认答应」字段填入客户端的seq+1，然后把SYN和ACK标志位设置为1，发送给客户端。服务端进入SYN_RCVD状态。第三次握手时，客户端收到服务端的报文后，最后回复一个答应报文，将ACK标志位设置为1，然后将服务端报文中的seq+1填入TCP报文首部的「确认答应」字段中，发送给客户端，进入ESTABLISH状态。服务端收到后也会进入ESTABLISH状态。\n\n\n\n\n\n\n\n\n\n三次握手能够附带数据吗？\n由于第三次握手时能够确认客户端和服务端能够建立连接，所以处于性能考虑，第三次握手时的报文是可以附带数据的。 而前两次握手的主要目的在于「建立连接」所以不能携带报文。\n\n\n\n\n\n\n\n\n\n为什么要三次握手？ 两次不行吗\n因为要考虑到网络阻塞的问题，当第一次握手时，客户端发送的\n\n\n\n\n\n\n\n\n\n什么是跨域问题？\n跨域问题是源于浏览器的同源策略。 这是浏览器中一个重要的安全策略，防止XSS、CSFR等浏览器层面的攻击。而同源是指：相同的协议、相同的域名和相同的端口所以当访问不同协议、不同域名、不同端口的资源时会出现跨域问题。\n当我们的项目是前后端分离架构时，前后端可能会部署在不同的服务器上，这时就会导致跨域的问题。\n\n\n\n\n\n\n\n\n\n解决跨域问题的常见方案？\n\nNginx方向代理。使用Nginx作为一层跳板机，将指定同源域名后的资源（比如说&#x2F;api）映射到具体后端服务器地址，实现跨域功能。\n\nhttp &#123;\n  server &#123;\n    location ~/api &#123;\n      # 反向代理\n      proxy_pass xxxxx; #\n    &#125;\n\n\nWebpack Server代理，可以在webpack.config.js中配置一个devServer，然后将指定资源映射到具体后端服务器的地址。\n\nmodule.exports = &#123; \n  devServer: &#123; \n    port: 3000, \n    proxy: &#123; \n      \"/api\": &#123; \n        target: \"http://localhost:3001\" \n      &#125; \n    &#125; \n  &#125;, \n&#125;;\n\n\n\n\n\n\n\n\n\n\n我们使用到的IDE：Webstorm、IDEA等是如何直接运行vue项目的？\nIntelliJ公司的IDE会内置一个轻量级的NodeJs的服务器，当我们在Configurations中配置好前端项目的package.json后会自动将前端项目编译打包放入node服务器进行运行。\nMQ\n\n\n\n\n\n\n\n\n线程池同样能够做到异步的作用，那么为什么还要使用MQ？\n第一个核心作用是削峰，比如当上游处理简单请求，平均处理速度为1w&#x2F;s，而下游处理秒杀的复杂请求，平均处理速度为2k&#x2F;s。 为了防止下游系统不被压垮，我们就需要引入MQ的削峰功能。我们就可以通过MQ的MQ-clinet模式，将下游的处理方式从MQ推送变为Client拉取。 这样下游就能够根据自己的处理能力，每隔一段时间拉取若干信息。\n第二个核心功能是解藕，这个解藕不单单是指生产者和消费者的解藕。 当上下游服务系统是异构的话，就需要MQ来帮助我们实现通讯的功能。\n参考文章：\n《图解HTTP》 作者：小林Coding\nJava全栈知识体系：https://pdai.tech/md/java/basic/java-basic-lan-basic.html\n\n","slug":"Java应届生面试复盘","date":"2022-11-07T03:07:40.000Z","categories_index":"","tags_index":"面经","author_index":"yaowenbin"}]